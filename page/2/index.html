<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"szhowardhuang.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="嵌入式老兵博客">
<meta property="og:url" content="https://szhowardhuang.github.io/page/2/index.html">
<meta property="og:site_name" content="嵌入式老兵博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Howard Huang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://szhowardhuang.github.io/page/2/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/2/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>嵌入式老兵博客</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">嵌入式老兵博客</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Howard Huang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">32</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/04/03/AirLLM-on-4G/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/04/03/AirLLM-on-4G/" class="post-title-link" itemprop="url">如何在单个 4GB GPU 上运行 70B LLM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-04-03 11:26:28 / 修改时间：21:26:28" itemprop="dateCreated datePublished" datetime="2024-04-03T11:26:28+08:00">2024-04-03</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>您是否曾经梦想过使用最先进的大型语言模型 （LLM） 来完成您的自然语言处理 （NLP） 任务，但对高内存要求感到沮丧？如果是这样，您可能会对 AirLLM 感兴趣，这是一个优化推理内存使用的 Python 包，允许 70B LLM 在单个 4GB GPU 卡上运行推理。不需要量化、蒸馏、修剪或其他会导致模型性能下降的模型压缩技术。</p>
<h2 id="什么是-AirLLM"><a href="#什么是-AirLLM" class="headerlink" title="什么是 AirLLM"></a>什么是 AirLLM</h2><p>大型语言模型 （LLM） 的计算成本很高，并且需要大量内存来训练和运行。这样做的原因是 LLM 有大量的层——一个 70B 模型可以有 80 多个层。但是，在推理过程中，语言模型中的每一层都是独立的，并且仅依赖于前一层的输出。因此，没有必要将所有层都保留在 GPU 内存中。相反，我们可以在执行该层时仅从磁盘加载必要的层，执行所有计算，然后完全释放内存。这样，单层所需的 GPU 内存仅与该transformer 层的参数大小差不多，即完整型号的 1&#x2F;80，即 ~2GB。</p>
<p>AirLLM 背后的主要思想确实是将原始 LLM 拆分为更小的子模型，每个子模型包含一个或几个层，并在推理过程中按需加载它们。这样，在任何给定时间，只有必要的子模型保存在内存中，其余的都存储在磁盘上。它还应用块式量化来进一步压缩子模型，从而减少磁盘加载时间和内存使用量。</p>
<p>AirLLM 支持 Hugging Face open LLM 排行榜中的大多数顶级型号，例如 Platypus2、LLaMa2、Mistral、Mixtral、SOLAR、StellarBright 等。</p>
<h2 id="如何使用AirLLM"><a href="#如何使用AirLLM" class="headerlink" title="如何使用AirLLM"></a>如何使用AirLLM</h2><p>使用 AirLLM 非常简单直观。你只需要安装 airllm pip 包，然后使用 AutoModel 类从 Hugging Face 中心或本地路径加载你选择的 LLM。然后，您可以使用 generate 方法执行类似于常规 transformer 模型的推理。例如，以下代码片段演示如何使用 AirLLM 加载和使用 Platypus2–70B-instruct 模型，该模型可以回答自然语言问题并遵循说明。</p>
<p>pip install airllm</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">from airllm import AutoModel</span><br><span class="line"></span><br><span class="line">MAX_LENGTH = 128</span><br><span class="line"></span><br><span class="line"># load the model from the Hugging Face hub</span><br><span class="line">model = AutoModel.from_pretrained(&quot;garage-bAInd/Platypus2-70B-instruct&quot;)</span><br><span class="line"></span><br><span class="line"># or load the model from a local path</span><br><span class="line"># model = AutoModel.from_pretrained(&quot;/home/ubuntu/.cache/huggingface/hub/models--garage-bAInd--Platypus2-70B-instruct/snapshots/b585e74bcaae02e52665d9ac6d23f4d0dbc81a0f&quot;)</span><br><span class="line"></span><br><span class="line"># prepare the input text</span><br><span class="line">input_text = [</span><br><span class="line">    &#x27;What is the capital of United States?&#x27;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"># tokenize the input text</span><br><span class="line">input_tokens = model.tokenizer(input_text,</span><br><span class="line">    return_tensors=&quot;pt&quot;,</span><br><span class="line">    return_attention_mask=False,</span><br><span class="line">    truncation=True,</span><br><span class="line">    max_length=MAX_LENGTH,</span><br><span class="line">    padding=False)</span><br><span class="line"></span><br><span class="line"># generate the output text</span><br><span class="line">generation_output = model.generate(</span><br><span class="line">    input_tokens[&#x27;input_ids&#x27;].cuda(),</span><br><span class="line">    max_new_tokens=20,</span><br><span class="line">    use_cache=True,</span><br><span class="line">    return_dict_in_generate=True)</span><br><span class="line"></span><br><span class="line"># decode the output text</span><br><span class="line">output = model.tokenizer.decode(generation_output.sequences[0])</span><br><span class="line"></span><br><span class="line"># print the output text</span><br><span class="line">print(output)</span><br></pre></td></tr></table></figure>
<p>此代码片段的输出为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">What is the capital of United States?</span><br><span class="line">The capital of the United States is Washington, D.C.</span><br></pre></td></tr></table></figure>
<p>请注意，在第一次推理中，AirLLM 将逐层分解并保存原始 LLM，因此请确保您有足够的磁盘空间。之后，AirLLM 将按需加载子模型，并以更快的速度和更少的内存执行推理。</p>
<h2 id="使用-AirLLM-有什么好处？"><a href="#使用-AirLLM-有什么好处？" class="headerlink" title="使用 AirLLM 有什么好处？"></a>使用 AirLLM 有什么好处？</h2><p>通过使用 AirLLM，您具有以下优势：</p>
<ul>
<li>访问最先进的 LLM：您可以将最先进的 LLM 用于您的 NLP 任务，例如问答、文本生成、文本摘要、文本分类等。您可以从各种适合您的需求和偏好的模型中进行选择，例如特定于域的模型、多语言模型或指令优化模型。</li>
<li>低内存要求：无需担心内存不足错误或昂贵的云计算资源。您可以在单个 4GB GPU 卡上运行推理，甚至可以在 CPU 或 Mac 设备上运行推理。</li>
<li>简单直观的使用：您可以使用 AirLLM 作为常规 transformer 型号的直接替代品，只需最少的代码更改。</li>
</ul>
<h2 id="使用-AirLLM-的缺点是什么？"><a href="#使用-AirLLM-的缺点是什么？" class="headerlink" title="使用 AirLLM 的缺点是什么？"></a>使用 AirLLM 的缺点是什么？</h2><p>如前所述，AirLLM 在执行该层时仅从磁盘加载必要的层，然后完全释放内存。但是，从速度较慢的存储（如磁盘 I&#x2F;O）按顺序加载数据会增加推理过程的延迟。如果 SSD 的读数为 4GB&#x2F;s，而型号的读数为 80Gb，则您将等待 20 秒才能生成一个令牌，并且对于每个令牌，您需要完全通过。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://generativeai.pub/how-to-run-70b-llms-on-a-single-4gb-gpu-d1c61ed5258c">https://generativeai.pub/how-to-run-70b-llms-on-a-single-4gb-gpu-d1c61ed5258c</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/04/02/scratch-ai-agent-openai/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/04/02/scratch-ai-agent-openai/" class="post-title-link" itemprop="url">利用 OpenAI 工具：从零开始构建可靠的 AI 代理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-04-02 20:26:28" itemprop="dateCreated datePublished" datetime="2024-04-02T20:26:28+08:00">2024-04-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-04-03 21:26:28" itemprop="dateModified" datetime="2024-04-03T21:26:28+08:00">2024-04-03</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>18k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>32 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>思考人工智能的未来，我脑海中浮现出《钢铁侠》中的贾维斯和《命运》（游戏）中的拉斯普京。在这两个例子中，人工智能充当了一个通过语音控制的接口，提供高级抽象的复杂系统。例如，托尼·斯塔克使用它来管理他的研究、进行计算和运行模拟。甚至 R2D2 也可以响应语音命令，与陌生的计算机系统进行交互，提取数据或与建筑系统进行交互。</p>
<p>在这些场景中，人工智能使得用户可以与复杂系统进行交互，而无需对其有深入的理解。这可以类比今天大型公司中的企业资源计划（ERP）系统。在大型公司中很少有人完全了解和理解内部 ERP 系统的每个方面。不难想象，在不久的将来，人工智能可能几乎会在与 ERP 系统的每个交互中提供帮助。从最终用户管理客户数据或记录订单，到软件开发人员修复错误或实施新功能，这些交互可能很快就会由熟悉 ERP 系统的人工智能助手来完成。这样的人工智能助手将知道将客户数据输入哪个数据库，以及哪些流程和代码可能与错误相关。</p>
<p>为了实现这一目标，我们面临着几个挑战和创新。我们需要重新思考流程及其文档化。如今的企业资源计划（ERP）流程是为人类使用而设计的，不同用户有不同的角色，文档是给人类看的，输入掩码是为人类设计的，用户交互旨在直观且无误。对于人工智能（AI）交互，这些方面的设计将会有所不同。我们需要为 AI 交互设定特定的角色，并设计不同的流程，以实现直观且无误的 AI 交互。这在我们与提示信息的工作中已经显现出来。我们认为的明确任务往往并不那么简单明了。</p>
<h2 id="从概念到现实：构建-AI-代理的基础"><a href="#从概念到现实：构建-AI-代理的基础" class="headerlink" title="从概念到现实：构建 AI 代理的基础"></a>从概念到现实：构建 AI 代理的基础</h2><p>让我们回到代理的概念。代理是能够使用提供的工具执行任务并决策如何使用这些工具的人工智能助手，它们是最终可能实现这样一个系统的构建模块。它们是我们希望将其整合到复杂系统的各个方面的过程组件。但是它们很难可靠地部署（之前在internlm实践lagent就不成功）。在本文中，我将演示如何设计和优化一个能够可靠地与数据库进行交互的代理。</p>
<p>AI 的未来宏伟愿景令人鼓舞，但实现这一愿景需要采取切实可行的步骤。为了展示我们如何开始构建这种先进 AI 系统的基础，让我们专注于创建一个常见任务的原型代理：“费用跟踪”。这个原型将作为一个具体的例子，展示 AI 如何在高效管理财务交易方面提供帮助，展示 AI 在自动化例行任务方面的潜力，并突出设计与数据库无缝交互的 AI 系统所涉及的挑战和考虑因素。通过从一个具体而易于理解的用例开始，我们可以获得宝贵的见解，为未来更复杂的 AI 代理的开发提供指导。</p>
<h2 id="本文目的"><a href="#本文目的" class="headerlink" title="本文目的"></a>本文目的</h2><p>本文将为一系列文章奠定基础，旨在开发一个聊天机器人，它可以作为小型企业支持和执行业务流程的唯一交互点，或者作为你个人生活中组织一切需要跟踪的东西的聊天机器人。从数据、例行事务、文件到图片，我们希望能够简单地与我们的助手聊天，让它找出存储和检索数据的位置。</p>
<p>从 AI 未来的宏伟愿景过渡到实际应用，让我们聚焦于创建一个原型代理。这个代理将作为实现之前讨论的雄心勃勃的目标的基础步骤。我们将着手开发一个“费用跟踪”代理，这是一个简单但至关重要的任务，展示了 AI 如何能够有效地协助管理财务交易。</p>
<p>这个“费用跟踪”原型不仅展示了人工智能在自动化例行任务方面的潜力，还阐明了设计与数据库无缝交互的人工智能系统所涉及的挑战和考虑因素。通过专注于这个例子，我们可以探索代理设计、输入验证以及将人工智能与现有系统整合的复杂性，为未来更复杂的应用奠定坚实基础。</p>
<h2 id="实践：测试-OpenAI-工具调用"><a href="#实践：测试-OpenAI-工具调用" class="headerlink" title="实践：测试 OpenAI 工具调用"></a>实践：测试 OpenAI 工具调用</h2><p>为了让我们的原型代理程序运行起来并找出潜在的瓶颈，我们尝试测试 OpenAI 的工具调用功能。从一个基本的费用跟踪示例开始，我们正在构建一个模拟真实应用程序的基础部分。这个阶段涉及创建一个基本模型，并使用 langchain 库的convert_to_openai_tool函数将其转换为 OpenAI 工具模式。此外，我们制作一个 report_tool ，以便我们未来的代理程序可以与结果进行通信，或者突出显示缺失的信息或问题。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">from pydantic.v1 import BaseModel, validator  </span><br><span class="line">from datetime import datetime</span><br><span class="line">from langchain_core.utils.function_calling import convert_to_openai_tool</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">class Expense(BaseModel):    </span><br><span class="line">   description: str    </span><br><span class="line">   net_amount: float    </span><br><span class="line">   gross_amount: float    </span><br><span class="line">   tax_rate: float    </span><br><span class="line">   date: datetime</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Report(BaseModel):</span><br><span class="line">   report: str</span><br><span class="line"></span><br><span class="line">add_expense_tool = convert_to_openai_tool(Expense)</span><br><span class="line">report_tool = convert_to_openai_tool(Report)</span><br></pre></td></tr></table></figure>

<p>数据模型和工具设置好之后，下一步是使用 OpenAI 客户端 SDK 发起一个简单的工具调用。在这个初始测试中，我们故意提供不足的信息给模型，以查看它是否能正确地指示缺失的内容。这种方法不仅测试了代理的功能能力，还测试了它的交互和错误处理能力。</p>
<h3 id="调用-OpenAI-API"><a href="#调用-OpenAI-API" class="headerlink" title="调用 OpenAI API"></a>调用 OpenAI API</h3><p>现在，我们将使用 OpenAI 客户端 SDK 来发起一个简单的工具调用。在我们的第一个测试中，我们故意提供了不足的信息给模型，以查看它是否能够通知我们缺失的细节。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">from openai import OpenAI  </span><br><span class="line">from langchain_core.utils.function_calling import convert_to_openai_tool  </span><br><span class="line">  </span><br><span class="line">SYSTEM_MESSAGE = &quot;&quot;&quot;You are tasked with completing specific objectives and </span><br><span class="line">must report the outcomes. At your disposal, you have a variety of tools, </span><br><span class="line">each specialized in performing a distinct type of task.  </span><br><span class="line">  </span><br><span class="line">For successful task completion:  </span><br><span class="line">Thought: Consider the task at hand and determine which tool is best suited </span><br><span class="line">based on its capabilities and the nature of the work.  </span><br><span class="line">  </span><br><span class="line">Use the report_tool with an instruction detailing the results of your work.  </span><br><span class="line">If you encounter an issue and cannot complete the task:  </span><br><span class="line">  </span><br><span class="line">Use the report_tool to communicate the challenge or reason for the </span><br><span class="line">task&#x27;s incompletion.  </span><br><span class="line">You will receive feedback based on the outcomes of </span><br><span class="line">each tool&#x27;s task execution or explanations for any tasks that </span><br><span class="line">couldn&#x27;t be completed. This feedback loop is crucial for addressing </span><br><span class="line">and resolving any issues by strategically deploying the available tools.  </span><br><span class="line">&quot;&quot;&quot;  </span><br><span class="line">user_message = &quot;I have spend 5$ on a coffee today please track my expense. The tax rate is 0.2.&quot;</span><br><span class="line">  </span><br><span class="line">client = OpenAI()  </span><br><span class="line">model_name = &quot;gpt-3.5-turbo-0125&quot;  </span><br><span class="line">  </span><br><span class="line">messages = [  </span><br><span class="line">    &#123;&quot;role&quot;:&quot;system&quot;, &quot;content&quot;: SYSTEM_MESSAGE&#125;,  </span><br><span class="line">    &#123;&quot;role&quot;:&quot;user&quot;, &quot;content&quot;: user_message&#125;  </span><br><span class="line">]  </span><br><span class="line">  </span><br><span class="line">response = client.chat.completions.create(  </span><br><span class="line">            model=model_name,  </span><br><span class="line">            messages=messages,  </span><br><span class="line">            tools=[  </span><br><span class="line">                convert_to_openai_tool(Expense),  </span><br><span class="line">                convert_to_openai_tool(ReportTool)]  </span><br><span class="line">        )</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>接下来，我们需要一个新的函数来从响应中读取函数调用的参数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def parse_function_args(response):</span><br><span class="line">    message = response.choices[0].message</span><br><span class="line">    return json.loads(message.tool_calls[0].function.arguments)</span><br><span class="line"></span><br><span class="line">print(parse_function_args(response))</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;description&#x27;: &#x27;Coffee&#x27;,</span><br><span class="line"> &#x27;net_amount&#x27;: 5,</span><br><span class="line"> &#x27;gross_amount&#x27;: None,</span><br><span class="line"> &#x27;tax_rate&#x27;: 0.2,</span><br><span class="line"> &#x27;date&#x27;: &#x27;2023-10-06T12:00:00Z&#x27;&#125;</span><br></pre></td></tr></table></figure>
<p> 正如我们所观察到的，我们在执行过程中遇到了几个问题</p>
<ul>
<li><p>The gross_amount is not calculated.  总金额未计算。</p>
</li>
<li><p>The date is hallucinated.  日期是幻觉。</p>
</li>
</ul>
<p>考虑到这一点，让我们尝试解决这些问题并优化我们的代理工作流程。</p>
<h2 id="优化：工具处理流程"><a href="#优化：工具处理流程" class="headerlink" title="优化：工具处理流程"></a>优化：工具处理流程</h2><p>为了优化代理工作流程，我发现将工作流程置于提示工程之上至关重要。虽然微调提示以使代理学会完美使用提供的工具并避免错误可能很诱人，但更明智的做法是首先调整工具和流程。当发生典型错误时，最初的考虑应该是如何基于代码修复它。</p>
<h3 id="处理缺失信息"><a href="#处理缺失信息" class="headerlink" title="处理缺失信息"></a>处理缺失信息</h3><p>处理缺失信息对于创建强大可靠的代理程序来说是一个重要的课题。在前面的例子中，为代理程序提供像“get_current_date”这样的工具是特定场景下的一种解决方法。然而，我们必须假设在各种情境中都会出现缺失信息，并且不能仅仅依靠提示工程和添加更多工具来防止模型产生缺失信息的幻觉。</p>
<p>对于这种情况，一个简单的解决方法是修改工具模式，将所有参数视为可选。这种方法确保代理只提交它知道的参数，避免不必要的幻觉。</p>
<p>因此，让我们来看一下 OpenAI 工具模式：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">add_expense_tool = convert_to_openai_tool(Expense)</span><br><span class="line">print(add_expense_tool)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#x27;type&#x27;: &#x27;function&#x27;,</span><br><span class="line"> &#x27;function&#x27;: &#123;&#x27;name&#x27;: &#x27;Expense&#x27;,</span><br><span class="line">  &#x27;description&#x27;: &#x27;&#x27;,</span><br><span class="line">  &#x27;parameters&#x27;: &#123;&#x27;type&#x27;: &#x27;object&#x27;,</span><br><span class="line">   &#x27;properties&#x27;: &#123;&#x27;description&#x27;: &#123;&#x27;type&#x27;: &#x27;string&#x27;&#125;,</span><br><span class="line">    &#x27;net_amount&#x27;: &#123;&#x27;type&#x27;: &#x27;number&#x27;&#125;,</span><br><span class="line">    &#x27;gross_amount&#x27;: &#123;&#x27;type&#x27;: &#x27;number&#x27;&#125;,</span><br><span class="line">    &#x27;tax_rate&#x27;: &#123;&#x27;type&#x27;: &#x27;number&#x27;&#125;,</span><br><span class="line">    &#x27;date&#x27;: &#123;&#x27;type&#x27;: &#x27;string&#x27;, &#x27;format&#x27;: &#x27;date-time&#x27;&#125;&#125;,</span><br><span class="line">   &#x27;required&#x27;: [&#x27;description&#x27;,</span><br><span class="line">    &#x27;net_amount&#x27;,</span><br><span class="line">    &#x27;gross_amount&#x27;,</span><br><span class="line">    &#x27;tax_rate&#x27;,</span><br><span class="line">    &#x27;date&#x27;]&#125;&#125;&#125;</span><br></pre></td></tr></table></figure>
<p>正如我们所看到的，我们有一个特殊的键required，我们需要将其移除。以下是您可以通过移除该required键来调整add_expense_tool模式以使参数变为可选的方法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">del add_expense_tool[&quot;function&quot;][&quot;parameters&quot;][&quot;required&quot;]</span><br></pre></td></tr></table></figure>

<h3 id="设计工具类"><a href="#设计工具类" class="headerlink" title="设计工具类"></a>设计工具类</h3><p>接下来，我们可以设计一个工具类，最初检查输入参数是否缺失。我们创建这个Tool类有两个方法：.run()，.validate_input()，和一个openai_tool_schema属性，通过删除必需的参数来操作工具模式。此外，我们定义了ToolResult BaseModel，其中包含content字段和success字段，用于每次工具运行的输出对象。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">from pydantic import BaseModel</span><br><span class="line">from typing import Type, Callable, Dict, Any, List</span><br><span class="line"></span><br><span class="line">class ToolResult(BaseModel):  </span><br><span class="line">    content: str  </span><br><span class="line">    success: bool  </span><br><span class="line">  </span><br><span class="line">class Tool(BaseModel):  </span><br><span class="line">    name: str  </span><br><span class="line">    model: Type[BaseModel]  </span><br><span class="line">    function: Callable  </span><br><span class="line">    validate_missing: bool = False  </span><br><span class="line">  </span><br><span class="line">    class Config:  </span><br><span class="line">        arbitrary_types_allowed = True  </span><br><span class="line">  </span><br><span class="line">    def run(self, **kwargs) -&gt; ToolResult:</span><br><span class="line">        if self.validate_missing:</span><br><span class="line">            missing_values = self.validate_input(**kwargs)  </span><br><span class="line">            if missing_values:  </span><br><span class="line">                content = f&quot;Missing values: &#123;&#x27;, &#x27;.join(missing_values)&#125;&quot;  </span><br><span class="line">                return ToolResult(content=content, success=False)  </span><br><span class="line">        result = self.function(**kwargs)  </span><br><span class="line">        return ToolResult(content=str(result), success=True)  </span><br><span class="line">      </span><br><span class="line">    def validate_input(self, **kwargs) -&gt; List[str]:  </span><br><span class="line">        missing_values = []  </span><br><span class="line">        for key in self.model.__fields__.keys():  </span><br><span class="line">            if key not in kwargs:  </span><br><span class="line">                missing_values.append(key)  </span><br><span class="line">        return missing_values</span><br><span class="line">    @property</span><br><span class="line">    def openai_tool_schema(self) -&gt; Dict[str, Any]:</span><br><span class="line">        schema = convert_to_openai_tool(self.model)</span><br><span class="line">        if &quot;required&quot; in schema[&quot;function&quot;][&quot;parameters&quot;]:</span><br><span class="line">            del schema[&quot;function&quot;][&quot;parameters&quot;][&quot;required&quot;]</span><br><span class="line">        return schem</span><br></pre></td></tr></table></figure>
<p>该Tool类是 AI 代理工作流程中的关键组成部分，作为一个蓝图来创建和管理各种工具，这些工具由代理来执行特定任务。它被设计用于处理输入验证，执行工具的功能，并以标准化格式返回结果。</p>
<h3 id="Tool-类的关键组成部分："><a href="#Tool-类的关键组成部分：" class="headerlink" title="Tool 类的关键组成部分："></a>Tool 类的关键组成部分：</h3><ul>
<li>name ：工具的名称。</li>
<li>model ：定义工具输入模式的 Pydantic BaseModel。</li>
<li>function ：工具执行的可调用函数。</li>
<li>validate_missing ：一个布尔标志，指示是否validate missing的输入值（默认为 False ）。</li>
</ul>
<h3 id="这个Tool类有两个主要方法："><a href="#这个Tool类有两个主要方法：" class="headerlink" title="这个Tool类有两个主要方法："></a>这个Tool类有两个主要方法：</h3><ul>
<li><p>run(self, **kwargs) -&gt; ToolResult: 该方法负责使用提供的输入参数执行tool的功能。首先检查validate_missing 是否设置为True。如果是，则调用validate_input()方法检查缺少的输入值。如果找到任何缺少的值，则返回一个带有错误消息的ToolResult对象，并设置success 为False。如果所有必需的输入值都存在，则继续使用提供的参数执行tool的function ，并返回一个带有结果的对象ToolResult ，并设置success 为True。</p>
</li>
<li><p>validate_input(self, **kwargs) -&gt; List[str]: 该方法将传递给tool的输入参数与在model中定义的预期输入模式进行比较。它遍历model中定义的字段，并检查每个字段是否存在于输入参数中。如果有任何字段缺失，它将字段名称追加到缺失值列表中。最后，它返回缺失值列表。</p>
</li>
</ul>
<p>该Tool类还有一个名为openai_tool_schema的属性，它返回该工具的 OpenAI 工具模式。它使用convert_to_openai_tool()函数将model转换为 OpenAI 工具模式格式。此外，它从模式中删除”required”键，使所有输入参数都是可选的。这使得代理可以仅提供可用信息，而无需虚构缺失的值。</p>
<p>通过封装工具的功能、输入验证和模式生成，该Tool类为在 AI 代理的工作流中创建和管理工具提供了一个干净且可重用的接口。它抽象了处理缺失值的复杂性，并确保代理能够优雅地处理不完整的信息，同时根据可用的输入执行适当的工具。</p>
<h3 id="测试缺失信息的处理流程"><a href="#测试缺失信息的处理流程" class="headerlink" title="测试缺失信息的处理流程"></a>测试缺失信息的处理流程</h3><p>接下来，我们将扩展我们的 OpenAI API 调用。我们希望客户端能够利用我们的工具，并且我们的响应对象能够直接触发 tool.run()。为此，我们需要在我们新创建的 Tool 类中初始化我们的工具。我们定义了两个虚拟函数，它们返回一个成功消息字符串。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def add_expense_func(**kwargs):  </span><br><span class="line">    return f&quot;Added expense: &#123;kwargs&#125; to the database.&quot;</span><br><span class="line"></span><br><span class="line">add_expense_tool = Tool(  </span><br><span class="line">    name=&quot;add_expense_tool&quot;,  </span><br><span class="line">    model=Expense,  </span><br><span class="line">    function=add_expense_func  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line">def report_func(report: str = None):  </span><br><span class="line">    return f&quot;Reported: &#123;report&#125;&quot;  </span><br><span class="line">  </span><br><span class="line">report_tool = Tool(  </span><br><span class="line">    name=&quot;report_tool&quot;,  </span><br><span class="line">    model=ReportTool,  </span><br><span class="line">    function=report_func  </span><br><span class="line">)  </span><br><span class="line">  </span><br><span class="line">tools = [add_expense_tool, report_tool]</span><br></pre></td></tr></table></figure>
<p>接下来，我们定义了一个辅助函数，它以客户端的响应作为输入，并帮助与我们的工具进行交互。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">def get_tool_from_response(response, tools=tools):  </span><br><span class="line">    tool_name = response.choices[0].message.tool_calls[0].function.name  </span><br><span class="line">    for t in tools:  </span><br><span class="line">        if t.name == tool_name:  </span><br><span class="line">            return t  </span><br><span class="line">    raise ValueError(f&quot;Tool &#123;tool_name&#125; not found in tools list.&quot;)</span><br><span class="line"></span><br><span class="line">def parse_function_args(response):  </span><br><span class="line">    message = response.choices[0].message  </span><br><span class="line">    return json.loads(message.tool_calls[0].function.arguments)</span><br><span class="line"></span><br><span class="line">def run_tool_from_response(response, tools=tools):  </span><br><span class="line">    tool = get_tool_from_response(response, tools)  </span><br><span class="line">    tool_kwargs = parse_function_args(response)  </span><br><span class="line">    return tool.run(**tool_kwargs)</span><br></pre></td></tr></table></figure>

<p>现在，我们可以使用我们的新工具执行客户端并使用该run_tool_from_response 函数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">response = client.chat.completions.create(  </span><br><span class="line">            model=model_name,  </span><br><span class="line">            messages=messages,  </span><br><span class="line">            tools=[tool.openai_tool_schema for tool in tools]  </span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">tool_result = run_tool_from_response(response, tools=tools)</span><br><span class="line">print(tool_result)</span><br></pre></td></tr></table></figure>

<p>运行上述代码后，您应该会看到以下输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">content=&#x27;Missing values: gross_amount, date&#x27; success=False</span><br></pre></td></tr></table></figure>

<p>完美，我们现在看到我们的工具显示存在缺失值。多亏了我们将所有参数都设为可选的技巧，我们现在避免了虚构的参数。</p>
<h2 id="构建代理工作流程"><a href="#构建代理工作流程" class="headerlink" title="构建代理工作流程"></a>构建代理工作流程</h2><p>我们目前的流程还不能代表一个真正的代理。到目前为止，我们只执行了一个 API 工具调用。为了将其转化为代理工作流程，我们需要引入一个迭代过程，将工具执行的结果反馈给客户端。基本流程应该是这样的：<br><img src="/../asset_scratchaiagentopenai/01.webp"></p>
<p>让我们开始创建一个新的 OpenAIAgent 类：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">class StepResult(BaseModel):  </span><br><span class="line">    event: str   </span><br><span class="line">    content: str  </span><br><span class="line">    success: bool</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class OpenAIAgent:  </span><br><span class="line">      </span><br><span class="line">    def __init__(  </span><br><span class="line">            self,   </span><br><span class="line">            tools: list[Tool],   </span><br><span class="line">            client: OpenAI,   </span><br><span class="line">            system_message: str = SYSTEM_MESSAGE,   </span><br><span class="line">            model_name: str = &quot;gpt-3.5-turbo-0125&quot;,  </span><br><span class="line">            max_steps: int = 5,  </span><br><span class="line">            verbose: bool = True  </span><br><span class="line">    ):  </span><br><span class="line">        self.tools = tools  </span><br><span class="line">        self.client = client  </span><br><span class="line">        self.model_name = model_name  </span><br><span class="line">        self.system_message = system_message  </span><br><span class="line">        self.step_history = []  </span><br><span class="line">        self.max_steps = max_steps  </span><br><span class="line">        self.verbose = verbose  </span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">    def to_console(self, tag: str, message: str, color: str = &quot;green&quot;):  </span><br><span class="line">        if self.verbose:  </span><br><span class="line">            color_prefix = Fore.__dict__[color.upper()]  </span><br><span class="line">            print(color_prefix + f&quot;&#123;tag&#125;: &#123;message&#125;&#123;Style.RESET_ALL&#125;&quot;)</span><br></pre></td></tr></table></figure>

<p>像我们的ToolResult对象一样，我们将一个StepResult  定义为每个代理步骤的对象。然后，我们定义了 OpenAIAgent 类的__init__ 方法和一个 to_console() 方法，用于将我们的中间步骤和工具调用打印到控制台，使用 colorama 进行彩色打印。接下来，我们定义了代理的核心，run() 和run_step() 方法。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">class OpenAIAgent:</span><br><span class="line"></span><br><span class="line">    # ... __init__...</span><br><span class="line">    </span><br><span class="line">    # ... to_console ...</span><br><span class="line">    </span><br><span class="line">    def run(self, user_input: str):  </span><br><span class="line">        </span><br><span class="line">        openai_tools = [tool.openai_tool_schema for tool in self.tools]    </span><br><span class="line">        self.step_history = [    </span><br><span class="line">            &#123;&quot;role&quot;:&quot;system&quot;, &quot;content&quot;:self.system_message&#125;,    </span><br><span class="line">            &#123;&quot;role&quot;:&quot;user&quot;, &quot;content&quot;:user_input&#125;    </span><br><span class="line">        ]    </span><br><span class="line">          </span><br><span class="line">        step_result = None    </span><br><span class="line">        i = 0</span><br><span class="line">        </span><br><span class="line">        self.to_console(&quot;START&quot;, f&quot;Starting Agent with Input: &#123;user_input&#125;&quot;)</span><br><span class="line">          </span><br><span class="line">        while i &lt; self.max_steps:  </span><br><span class="line">            step_result = self.run_step(self.step_history, openai_tools)    </span><br><span class="line">            </span><br><span class="line">            if step_result.event == &quot;finish&quot;:    </span><br><span class="line">                break  </span><br><span class="line">            elif step_result.event == &quot;error&quot;:  </span><br><span class="line">                self.to_console(step_result.event, step_result.content, &quot;red&quot;)  </span><br><span class="line">            else:  </span><br><span class="line">                self.to_console(step_result.event, step_result.content, &quot;yellow&quot;)  </span><br><span class="line">            i += 1   </span><br><span class="line">              </span><br><span class="line">        self.to_console(&quot;Final Result&quot;, step_result.content, &quot;green&quot;)  </span><br><span class="line">        return step_result.content</span><br></pre></td></tr></table></figure>

<p>在该run() 方法中，我们首先通过使用预定义的 system_message 和 user_input 来初始化step_history，它们将作为我们的消息存储器。然后我们开始我们的 while 循环，在每次迭代中调用 run_step  函数，它将返回一个 StepResult 对象。我们判断代理是否完成了任务或是否发生了错误，并将其传递给控制台。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">class OpenAIAgent:</span><br><span class="line"></span><br><span class="line">    # ... __init__...</span><br><span class="line">    </span><br><span class="line">    # ... to_console ...</span><br><span class="line">    # ... run ...</span><br><span class="line">    def run_step(self, messages: list[dict], tools):  </span><br><span class="line">          </span><br><span class="line">        # plan the next step  </span><br><span class="line">        response = self.client.chat.completions.create(  </span><br><span class="line">            model=self.model_name,  </span><br><span class="line">            messages=messages,  </span><br><span class="line">            tools=tools  </span><br><span class="line">        )  </span><br><span class="line">          </span><br><span class="line">        # add message to history  </span><br><span class="line">        self.step_history.append(response.choices[0].message)  </span><br><span class="line">        </span><br><span class="line">        # check if tool call is present  </span><br><span class="line">        if not response.choices[0].message.tool_calls:  </span><br><span class="line">            return StepResult(</span><br><span class="line">                event=&quot;Error&quot;,</span><br><span class="line">                content=&quot;No tool calls were returned.&quot;, </span><br><span class="line">                success=False</span><br><span class="line">                )  </span><br><span class="line">          </span><br><span class="line">        tool_name = response.choices[0].message.tool_calls[0].function.name  </span><br><span class="line">        tool_kwargs = parse_function_args(response)  </span><br><span class="line">          </span><br><span class="line">        # execute the tool call  </span><br><span class="line">        self.to_console(</span><br><span class="line">        &quot;Tool Call&quot;, f&quot;Name: &#123;tool_name&#125;\nArgs: &#123;tool_kwargs&#125;&quot;, &quot;magenta&quot;</span><br><span class="line">        )  </span><br><span class="line">        tool_result = run_tool_from_response(response, tools=self.tools)  </span><br><span class="line">        tool_result_msg = self.tool_call_message(response, tool_result)  </span><br><span class="line">        self.step_history.append(tool_result_msg)  </span><br><span class="line">          </span><br><span class="line">        if tool_result.success:  </span><br><span class="line">            step_result = StepResult(  </span><br><span class="line">                event=&quot;tool_result&quot;,   </span><br><span class="line">                content=tool_result.content,   </span><br><span class="line">                success=True)  </span><br><span class="line">        else:  </span><br><span class="line">            step_result = StepResult(  </span><br><span class="line">                event=&quot;error&quot;,   </span><br><span class="line">                content=tool_result.content,   </span><br><span class="line">                success=False  </span><br><span class="line">            )   </span><br><span class="line">          </span><br><span class="line">        return step_result  </span><br><span class="line">          </span><br><span class="line">      </span><br><span class="line">    def tool_call_message(self, response, tool_result: ToolResult):  </span><br><span class="line">        tool_call = response.choices[0].message.tool_calls[0]  </span><br><span class="line">        return &#123;  </span><br><span class="line">            &quot;tool_call_id&quot;: tool_call.id,  </span><br><span class="line">            &quot;role&quot;: &quot;tool&quot;,  </span><br><span class="line">            &quot;name&quot;: tool_call.function.name,  </span><br><span class="line">            &quot;content&quot;: tool_result.content,  </span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>

<p>现在我们已经为每个步骤定义了逻辑。我们首先通过之前测试过的客户端 API 调用工具来获取一个响应对象。我们将响应消息对象附加到我们的 step_history 中。然后我们验证是否在响应对象中包含了一个工具调用，否则我们在 StepResult 中返回一个错误。然后我们将工具调用记录到控制台，并使用之前定义的run_tool_from_response() 方法运行所选的工具。我们还需要将工具结果附加到我们的消息历史中。OpenAI 已经为此定义了一个特定的格式，以便模型知道哪个工具调用与哪个输出相关联，通过将 tool_call_id 传递给我们的消息字典。这是通过我们的方法 tool_call_message() 完成的，该方法接受响应对象和工具结果作为输入参数。在每个步骤结束时，我们将工具结果分配给一个 StepResult 对象，该对象还指示步骤是否成功，并将其返回给我们的循环 run() 中。</p>
<h2 id="运行代理"><a href="#运行代理" class="headerlink" title="运行代理"></a>运行代理</h2><p>现在我们可以使用之前的示例来测试我们的代理，直接将其配备为一个get_current_date_tool 。在这里，我们可以将之前定义的validate_missing 属性设置为False ，因为该工具不需要任何输入参数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class DateTool(BaseModel):  </span><br><span class="line">    x: str = None  </span><br><span class="line"></span><br><span class="line">get_date_tool = Tool(  </span><br><span class="line">    name=&quot;get_current_date&quot;,  </span><br><span class="line">    model=DateTool,  </span><br><span class="line">    function=lambda: datetime.now().strftime(&quot;%Y-%m-%d&quot;),  </span><br><span class="line">    validate_missing=False  </span><br><span class="line">)  </span><br><span class="line">      </span><br><span class="line">tools = [  </span><br><span class="line">    add_expense_tool,   </span><br><span class="line">    report_tool,  </span><br><span class="line">    get_date_tool  </span><br><span class="line">]  </span><br><span class="line">  </span><br><span class="line">agent = OpenAIAgent(tools, client)</span><br><span class="line">agent.run(&quot;I have spent 5$ on a coffee today please track my expense. The tax rate is 0.2.&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">START: Starting Agent with Input: </span><br><span class="line">&quot;I have spend 5$ on a coffee today please track my expense. The tax rate is 0.2.&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Tool Call: get_current_date</span><br><span class="line">Args: &#123;&#125;</span><br><span class="line">tool_result: 2024-03-15</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Tool Call: add_expense_tool</span><br><span class="line">Args: &#123;&#x27;description&#x27;: &#x27;Coffee expense&#x27;, &#x27;net_amount&#x27;: 5, &#x27;tax_rate&#x27;: 0.2, &#x27;date&#x27;: &#x27;2024-03-15&#x27;&#125;</span><br><span class="line">error: Missing values: gross_amount</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Tool Call: add_expense_tool</span><br><span class="line">Args: &#123;&#x27;description&#x27;: &#x27;Coffee expense&#x27;, &#x27;net_amount&#x27;: 5, &#x27;tax_rate&#x27;: 0.2, &#x27;date&#x27;: &#x27;2024-03-15&#x27;, &#x27;gross_amount&#x27;: 6&#125;</span><br><span class="line">tool_result: Added expense: &#123;&#x27;description&#x27;: &#x27;Coffee expense&#x27;, &#x27;net_amount&#x27;: 5, &#x27;tax_rate&#x27;: 0.2, &#x27;date&#x27;: &#x27;2024-03-15&#x27;, &#x27;gross_amount&#x27;: 6&#125; to the database.</span><br><span class="line">Error: No tool calls were returned.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Tool Call: Name: report_tool</span><br><span class="line">Args: &#123;&#x27;report&#x27;: &#x27;Expense successfully tracked for coffee purchase.&#x27;&#125;</span><br><span class="line">tool_result: Reported: Expense successfully tracked for coffee purchase.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Final Result: Reported: Expense successfully tracked for coffee purchase.</span><br></pre></td></tr></table></figure>

<p>在成功执行我们的原型代理之后，值得强调的是代理根据计划有效地利用了指定的工具。最初，它调用了get_current_date_tool，为费用记录建立了基础时间戳。随后，在尝试通过add_expense_tool 进行费用记录时，我们智能设计的工具类发现了一个缺失的gross_amount——这是准确财务跟踪所必需的关键信息。令人印象深刻的是，代理通过使用提供的tax_rate自动解决了这个gross_amount 计算问题。</p>
<p>调试发现AI Agent不会成功，不成功log如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">START: Starting Agent with Input: I have spend 5$ on a coffee today please track my expense. The tax rate is 0.2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Tool Call: Name: get_current_date</span><br><span class="line">Args: &#123;&#125;</span><br><span class="line">tool_result: 2024-04-03</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Tool Call: Name: add_expense_tool</span><br><span class="line">Args: &#123;&#x27;description&#x27;: &#x27;Coffee&#x27;, &#x27;net_amount&#x27;: 5, &#x27;tax_rate&#x27;: 0.2, &#x27;date&#x27;: &#x27;2024-04-03&#x27;&#125;</span><br><span class="line">error: Missing values: gross_amount</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Tool Call: Name: add_expense_tool</span><br><span class="line">Args: &#123;&#x27;description&#x27;: &#x27;Coffee&#x27;, &#x27;net_amount&#x27;: 5, &#x27;tax_rate&#x27;: 0.2, &#x27;date&#x27;: &#x27;2024-04-03&#x27;, &#x27;gross_amount&#x27;: 6&#125;</span><br><span class="line">tool_result: Added expense: &#123;&#x27;description&#x27;: &#x27;Coffee&#x27;, &#x27;net_amount&#x27;: 5, &#x27;tax_rate&#x27;: 0.2, &#x27;date&#x27;: &#x27;2024-04-03&#x27;, &#x27;gross_amount&#x27;: 6&#125; to the database.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Tool Call: Name: report_tool</span><br><span class="line">Args: &#123;&#x27;report&#x27;: &#x27;Expense of $5 for coffee today has been successfully tracked with a tax rate of 0.2.&#x27;&#125;</span><br><span class="line">tool_result: Reported: Expense of $5 for coffee today has been successfully tracked with a tax rate of 0.2.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Error: No tool calls were returned.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Final Result: No tool calls were returned.</span><br></pre></td></tr></table></figure>
<p>这是因为openai的模型判断工具调用完成就会停止工具调用，返回response.choices[0].message.tool_calls 为空，需要改一下源代码。<br>如下：在agent.py中添加check if task is completed如下代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># add message to history</span><br><span class="line">self.step_history.append(response.choices[0].message)</span><br><span class="line"># check if task is completed</span><br><span class="line">if response.choices[0].finish_reason == &quot;stop&quot;:</span><br><span class="line">    step_result = StepResult(event=&quot;finish&quot;, content=&quot;Task completed.&quot;, success=True)</span><br><span class="line">    return step_result</span><br></pre></td></tr></table></figure>


<p>在我们的测试中，需要提到的是，输入费用的性质——无论是花在咖啡上的 5 美元是净额还是毛额——并没有明确指定。在这个阶段，为了使代理能够成功完成任务，这种明确性并不是必需的。然而，这为我们改进代理的理解和交互能力提供了有价值的见解：将这些详细信息纳入我们的初始系统提示中，可以显著提高代理在处理费用记录方面的准确性和效率。这个调整将确保从一开始就更全面地掌握财务数据。</p>
<h2 id="要点"><a href="#要点" class="headerlink" title="要点"></a>要点</h2><ul>
<li>迭代开发：该项目强调了迭代开发周期的关键性，通过反馈促进持续改进。在人工智能领域，这种方法至关重要，因为变化是常态，需要一种适应性强、响应迅速的开发策略。</li>
<li>处理不确定性：我们的过程突显了优雅地管理模糊和错误的重要性。创新，如可选参数和严格的输入验证，已被证明对提高代理的可靠性和用户体验至关重要。</li>
<li>特定任务的定制代理工作流程：这项工作的一个关键洞察是定制代理工作流程以适应特定的用例的重要性。除了组装一套工具之外，工具交互和响应的战略设计至关重要。这种定制确保代理有效地应对特定挑战，从而实现更加专注和高效的问题解决方法。</li>
</ul>
<h2 id="展望未来"><a href="#展望未来" class="headerlink" title="展望未来"></a>展望未来</h2><p>接下来的文章将着重扩展我们原型的功能，并将其与现实世界的系统集成。在下一篇文章中，我们将深入探讨设计一个强大的项目结构，使我们的代理能够与 SQL 数据库无缝交互。通过利用本文中开发的代理，我们将展示 AI 如何高效地管理和操作存储在数据库中的数据，为自动化数据相关任务打开了无限可能性。</p>
<p>在此基础上，本系列的第三篇文章将介绍高级查询功能，使我们的代理能够处理更复杂的数据检索和操作任务。我们还将探讨路由代理的概念，它将充当一个中央枢纽，用于管理多个子代理，每个子代理负责与特定的数据库表进行交互。这种分层结构将允许用户用自然语言提出请求，然后路由代理将解释并将其指示给适当的子代理执行。</p>
<p>为了进一步提升我们基于人工智能的系统的实用性和安全性，我们将引入基于角色的访问控制系统。这将确保用户根据其分配的角色拥有适当的权限来访问和修改数据。通过实施这一功能，我们可以展示人工智能代理如何在现实场景中部署，同时保持数据的完整性和安全性。</p>
<p>通过这些即将推出的增强功能，我们旨在展示 AI 代理在简化数据管理流程和为用户提供更直观高效的与数据库交互方式方面的真正潜力。通过结合自然语言处理、数据库管理和基于角色的访问控制的能力，我们将为开发能够彻底改变企业和个人处理数据方式的复杂 AI 助手奠定基础。</p>
<h2 id="源代码"><a href="#源代码" class="headerlink" title="源代码"></a>源代码</h2><p>所涵盖项目的全部源代码都可以在 GitHub 上找到。您可以在 <a target="_blank" rel="noopener" href="https://github.com/elokus/AgentDemo">https://github.com/elokus/AgentDemo</a> 访问它。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/leverage-openai-tool-calling-building-a-reliable-ai-agent-from-scratch-4e21fcd15b62">https://towardsdatascience.com/leverage-openai-tool-calling-building-a-reliable-ai-agent-from-scratch-4e21fcd15b62</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/03/31/graph-rag/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/03/31/graph-rag/" class="post-title-link" itemprop="url">从传统的 RAG 到图形 RAG</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-03-31 23:26:28" itemprop="dateCreated datePublished" datetime="2024-03-31T23:26:28+08:00">2024-03-31</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-05-15 14:59:01" itemprop="dateModified" datetime="2024-05-15T14:59:01+08:00">2024-05-15</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>16 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>When Large Language Models Meet Knowledge Graphs<br>当大语言模型遇上知识图谱</p>
<h2 id="图-RAG：RAG-x-知识图谱"><a href="#图-RAG：RAG-x-知识图谱" class="headerlink" title="图 RAG：RAG x 知识图谱"></a>图 RAG：RAG x 知识图谱</h2><p>图形 RAG 基于 RAG 的概念，利用知识图谱（KGs）。</p>
<p>这种创新的方法是由 NebulaGraph 首创的概念，通过整合图数据库改变了LLMs解释和响应查询的方式。</p>
<p>图形 RAG 通过将知识图谱中的结构化数据整合到LLM的处理中，为模型的响应提供更细致和明智的基础。</p>
<p>KG 是现实世界实体及其之间关系的结构化表示，由节点和边组成。节点代表人、地点、物体或概念等实体。</p>
<p>反过来，这些节点代表着这些实体之间的关系或连接。</p>
<p><img src="/../asset_graphrag/01.png"></p>
<p>这种结构极大地提高了LLMs生成明智回应的能力，使模型能够精确访问与上下文相关的数据。</p>
<p>Graph RAG 的创新在于将图数据库与LLMs集成，以丰富模型在生成响应之前的上下文。</p>
<p>一些受欢迎的图数据库产品包括 Ontotext、NebulaGraph 和 Neo4J。</p>
<h2 id="图形-RAG-演示"><a href="#图形-RAG-演示" class="headerlink" title="图形 RAG 演示"></a>图形 RAG 演示</h2><p>对于这个演示，我们将使用 <a target="_blank" rel="noopener" href="https://www.developer.tech.gov.sg/products/all-products/">Govtech 的开发者门户网站</a>上的产品信息作为我们的知识库。</p>
<h3 id="1-设置"><a href="#1-设置" class="headerlink" title="1. 设置"></a>1. 设置</h3><p>使用 Neo4j 桌面版启动一个本地的 Neo4j 实例,<br><img src="/../asset_graphrag/04.png"></p>
<p>使用 LangChain 在本地连接到 Neo4j 数据库。好消息是，LangChain 有一个可直接使用的<a target="_blank" rel="noopener" href="https://python.langchain.com/docs/templates/neo4j-advanced-rag">模板</a>，方便快速设置。</p>
<h3 id="2-提取"><a href="#2-提取" class="headerlink" title="2. 提取"></a>2. 提取</h3><p>使用快速工程和LLM来提取信息、节点及其连接。以下是一个提示的示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># Instructions for Creating Knowledge Graphs</span><br><span class="line">## Overview</span><br><span class="line">You are engineered for organising data into knowledge graphs.</span><br><span class="line">- **Nodes**: Represent entities and ideas.</span><br><span class="line">- The objective is to ensure the knowledge graph is straightforward and intelligible for broad use.</span><br><span class="line"></span><br><span class="line">## Node Labeling</span><br><span class="line">- **Uniformity**: Stick to simple labels for nodes. For instance, label any entity that is an organisation as &quot;company&quot;, rather than using terms like &quot;Facebook&quot; or &quot;Amazon&quot;.</span><br><span class="line">- **Identifiers for Nodes**: Opt for textual or comprehensible identifiers over numerical ones.</span><br><span class="line">  - **Permissible Node Labels**: If there are specific allowed node labels, list them here.</span><br><span class="line">  - **Permissible Relationship Types**: If there are specific allowed relationship types, list them here.</span><br><span class="line"></span><br><span class="line">## Managing Numerical Data and Dates</span><br><span class="line">- Integrate numerical information directly as attributes of nodes.</span><br><span class="line">- **Integrated Dates/Numbers**: Refrain from creating distinct nodes for dates or numbers, attaching them instead as attributes.</span><br><span class="line">- **Format for Properties**: Use a key-value pairing format.</span><br><span class="line">- **Avoiding Quotation Marks**: Do not use escaped quotes within property values.</span><br><span class="line">- **Key Naming**: Adopt camelCase for naming keys, such as `dateTime`.</span><br><span class="line"></span><br><span class="line">## Uniformity</span><br><span class="line">- **Entity Uniformity**: Ensure consistent identification for entities across various mentions or references.</span><br><span class="line">  </span><br><span class="line">## Adherence to Guidelines</span><br><span class="line">Strict adherence to these instructions is mandatory. Non-adherence will result in termination.</span><br></pre></td></tr></table></figure>

<h3 id="3-图构建"><a href="#3-图构建" class="headerlink" title="3. 图构建"></a>3. 图构建</h3><ul>
<li><p>使用 CSVLoader 和文档分割来处理我们的文档</p>
</li>
<li><p>将提取的信息映射到图节点和关系</p>
</li>
<li><p>通过我们的提取管道处理文档，并将信息存储在 Neo4j 中</p>
</li>
</ul>
<p><img src="/../asset_graphrag/02.png"><br>该过程耗时近一个小时，最终得到了提取的节点标签的最终列表</p>
<ul>
<li>不幸的是，不是所有的节点标签都对我们的上下文有用或符合我们的需求。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;identity&quot;: 1040,</span><br><span class="line">  &quot;labels&quot;: [</span><br><span class="line">    &quot;Feedbackstatus&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;properties&quot;: &#123;</span><br><span class="line">    &quot;id&quot;: &quot;Feedback-Success&quot;,</span><br><span class="line">    &quot;message&quot;: &quot;Sent. Thank you for the feedback!&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;elementId&quot;: &quot;4:81cd2613-0f18-49c1-8134-761643e88b7a:1040&quot;</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">  &quot;identity&quot;: 1582,</span><br><span class="line">  &quot;labels&quot;: [</span><br><span class="line">    &quot;Feedbackstatus&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;properties&quot;: &#123;</span><br><span class="line">    &quot;id&quot;: &quot;Feedbacksuccess&quot;,</span><br><span class="line">    &quot;status&quot;: &quot;Sent. Thank you for the feedback!&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;elementId&quot;: &quot;4:81cd2613-0f18-49c1-8134-761643e88b7a:1582&quot;</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">  &quot;identity&quot;: 1405,</span><br><span class="line">  &quot;labels&quot;: [</span><br><span class="line">    &quot;Header&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;properties&quot;: &#123;</span><br><span class="line">    &quot;id&quot;: &quot;Modalcardhead&quot;,</span><br><span class="line">    &quot;class&quot;: &quot;sgds-modal-card-head&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;elementId&quot;: &quot;4:81cd2613-0f18-49c1-8134-761643e88b7a:1405&quot;</span><br><span class="line">&#125;,</span><br><span class="line">&#123;</span><br><span class="line">  &quot;identity&quot;: 1112,</span><br><span class="line">  &quot;labels&quot;: [</span><br><span class="line">    &quot;Feedbackindicator&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;properties&quot;: &#123;</span><br><span class="line">    &quot;id&quot;: &quot;Feedbacksuccess&quot;,</span><br><span class="line">    &quot;title&quot;: &quot;check&quot;,</span><br><span class="line">    &quot;message&quot;: &quot;Sent. Thank you for the feedback!&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;elementId&quot;: &quot;4:81cd2613-0f18-49c1-8134-761643e88b7a:1112&quot;</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<h3 id="4-评估和改进"><a href="#4-评估和改进" class="headerlink" title="4. 评估和改进"></a>4. 评估和改进</h3><p>我们将指定LLM应该提取哪些节点标签来完善我们的方法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">permissible_nodes_to_extract = [</span><br><span class="line">    &quot;Aisubfield&quot;,</span><br><span class="line">    &quot;Application&quot;,</span><br><span class="line">    &quot;Cloudservice&quot;,</span><br><span class="line">    &quot;Concept&quot;,</span><br><span class="line">    &quot;Digitalsolution&quot;,</span><br><span class="line">    &quot;Division&quot;,</span><br><span class="line">    &quot;Entity&quot;,</span><br><span class="line">    &quot;Feature&quot;,</span><br><span class="line">    &quot;Fundinginitiative&quot;,</span><br><span class="line">    &quot;Initiative&quot;,</span><br><span class="line">    &quot;Link&quot;,</span><br><span class="line">    &quot;Location&quot;,</span><br><span class="line">    &quot;Organization&quot;,</span><br><span class="line">    &quot;Person&quot;,</span><br><span class="line">    &quot;Platform&quot;,</span><br><span class="line">    &quot;Policy&quot;,</span><br><span class="line">    &quot;Program&quot;</span><br><span class="line">    &quot;Resource&quot;,</span><br><span class="line">    &quot;Role&quot;,</span><br><span class="line">    &quot;Schema&quot;,</span><br><span class="line">    &quot;Service&quot;,</span><br><span class="line">    &quot;Standard&quot;,</span><br><span class="line">    &quot;Technology&quot;,</span><br><span class="line">    &quot;Technologyplatform&quot;,</span><br><span class="line">    &quot;Technologystack&quot;,</span><br><span class="line">    &quot;Webframework&quot;,</span><br><span class="line">    &quot;Webresource&quot;,</span><br><span class="line">    &quot;Website&quot;</span><br><span class="line">]</span><br></pre></td></tr></table></figure>


<p>使用 Neo4j 浏览器探索我们新构建的知识图谱</p>
<p><img src="/../asset_graphrag/03.png"></p>
<p>例子如下：</p>
<p>Q: “I need to remove sensitive information from some of my documents. What products does Govtech offer that has these capabilities?”</p>
<p>Base RAG: “Govtech offers the products Cloak and FileSG, which have capabilities to help remove sensitive information from documents.”</p>
<p>Graph RAG: “GovTech offers a product called Cloak, which is a central privacy toolkit for policy-compliant data anonymization. This product helps public officers apply data anonymization techniques to datasets and review re-identification risks in compliance with guidelines.”</p>
<p>与传统的 RAG 方法相比，Graph RAG 在这个例子中的优势是显而易见的。</p>
<p>该回应不仅展示了准确性，还通过上下文和关系的丰富提供了一种在标准的（RAG）方法中缺失的深度水平。</p>
<p>Graph RAG 的秘密在于其分析 “用户的查询” 的能力，从图数据库中找出相关数据，并将这种上下文洞察力融入到LLM的回应中。</p>
<p>这种方法利用了传统方法可能忽视的一系列相互关联的信息，从而得到对查询更加细致入微的理解。</p>
<h2 id="使用-Neo4j-和-Langchain-从-Bhagavad-Gita-Treatise-PDF-开发基本知识图谱"><a href="#使用-Neo4j-和-Langchain-从-Bhagavad-Gita-Treatise-PDF-开发基本知识图谱" class="headerlink" title="使用 Neo4j 和 Langchain 从 Bhagavad Gita Treatise PDF 开发基本知识图谱"></a>使用 Neo4j 和 Langchain 从 Bhagavad Gita Treatise PDF 开发基本知识图谱</h2><p>让我们逐步演示一下如何使用著名的《薄伽梵歌》来创建基本知识图谱。这篇由斯里·斯瓦米·悉瓦南达 （Sri Swami Sivananda） 撰写的文本充满了丰富的信息，我们可以用知识图谱来组织这些信息。我们将使用 Neo4j（帮助我们管理和构建图形）和 Langchain（帮助我们处理文本）。</p>
<p>首先，使用 <a target="_blank" rel="noopener" href="https://neo4j.com/cloud/aura-free/">Neo4j</a> 创建一个免费帐户。在此示例中，我们将使用free版本，它允许创建一个实例。</p>
<p><img src="/../asset_graphrag/05.png"></p>
<p>凭据文件将包含以下详细信息，您需要在后续代码中使用这些详细信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">NEO4J_URI=value</span><br><span class="line">NEO4J_USERNAME=neo4j</span><br><span class="line">NEO4J_PASSWORD=创建Neo4j账号时给的密码</span><br><span class="line">AURA_INSTANCEID=value</span><br><span class="line">AURA_INSTANCENAME=Instance01</span><br></pre></td></tr></table></figure>
<p>现在让我们创建知识图谱。</p>
<h3 id="安装库"><a href="#安装库" class="headerlink" title="安装库"></a>安装库</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from dotenv import load_dotenv</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line"># Common data processing</span><br><span class="line">import textwrap</span><br><span class="line"></span><br><span class="line"># Langchain</span><br><span class="line">from langchain_community.graphs import Neo4jGraph</span><br><span class="line">from langchain_community.vectorstores import Neo4jVector</span><br><span class="line">from langchain.text_splitter import RecursiveCharacterTextSplitter</span><br><span class="line">from langchain.chains import RetrievalQAWithSourcesChain</span><br><span class="line">from langchain.llms import OpenAI</span><br><span class="line">from langchain.embeddings import OpenAIEmbeddings</span><br><span class="line">from langchain.document_loaders import PyPDFLoader</span><br></pre></td></tr></table></figure>

<h3 id="从-PDF-中提取文本："><a href="#从-PDF-中提取文本：" class="headerlink" title="从 PDF 中提取文本："></a>从 PDF 中提取文本：</h3><p>加载 PDF 文件并将其页面拆分为可管理的文本块。我们利用 langchain 库中的 PyPDFLoader 模块来完成这项任务。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Load PDF file</span><br><span class="line">loader = PyPDFLoader(&quot;path/to/your/pdf/file.pdf&quot;)</span><br><span class="line">pages = loader.load_and_split()</span><br></pre></td></tr></table></figure>
<h3 id="将文本拆分为块："><a href="#将文本拆分为块：" class="headerlink" title="将文本拆分为块："></a>将文本拆分为块：</h3><p>接下来，我们将提取的文本拆分为更小的块，以便于进一步处理。langchain 中的 RecursiveCharacterTextSplitter 类用于此目的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Split pages into chunks</span><br><span class="line">text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)</span><br><span class="line">chunks = text_splitter.split_documents(pages)</span><br></pre></td></tr></table></figure>

<h3 id="创建一个向量存储，在-Neo4j-中嵌入和存储："><a href="#创建一个向量存储，在-Neo4j-中嵌入和存储：" class="headerlink" title="创建一个向量存储，在 Neo4j 中嵌入和存储："></a>创建一个向量存储，在 Neo4j 中嵌入和存储：</h3><p>我们创建一个 Neo4jVector 对象来存储文本块的嵌入到 Neo4j 图形数据库中。这使我们能够在以后有效地检索和操作嵌入。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># Warning control</span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&quot;ignore&quot;)</span><br><span class="line"></span><br><span class="line"># Load from environment from the credentials file</span><br><span class="line">load_dotenv(&#x27;.env&#x27;, override=True)</span><br><span class="line">NEO4J_URI = os.getenv(&#x27;NEO4J_URI&#x27;) </span><br><span class="line"></span><br><span class="line">NEO4J_USERNAME = os.getenv(&#x27;NEO4J_USERNAME&#x27;)</span><br><span class="line"></span><br><span class="line">NEO4J_PASSWORD = os.getenv(&#x27;NEO4J_PASSWORD&#x27;)</span><br><span class="line"></span><br><span class="line">NEO4J_DATABASE = os.getenv(&#x27;NEO4J_DATABASE&#x27;) or &#x27;neo4j&#x27;</span><br><span class="line">NEO4J_DATABASE = &#x27;neo4j&#x27;</span><br><span class="line"># Global constants</span><br><span class="line">VECTOR_INDEX_NAME = &#x27;pdf_chunks&#x27;</span><br><span class="line">VECTOR_NODE_LABEL = &#x27;Chunk&#x27;</span><br><span class="line">VECTOR_SOURCE_PROPERTY = &#x27;text&#x27;</span><br><span class="line">VECTOR_EMBEDDING_PROPERTY = &#x27;textEmbedding&#x27;</span><br><span class="line"></span><br><span class="line">kg = Neo4jGraph(</span><br><span class="line">    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Create Neo4j vector store</span><br><span class="line">neo4j_vector_store = Neo4jVector.from_documents(</span><br><span class="line">    embedding=OpenAIEmbeddings(),</span><br><span class="line">    documents=chunks,</span><br><span class="line">    url=NEO4J_URI,</span><br><span class="line">    username=NEO4J_USERNAME,</span><br><span class="line">    password=NEO4J_PASSWORD,</span><br><span class="line">    index_name=VECTOR_INDEX_NAME,</span><br><span class="line">    text_node_property=VECTOR_SOURCE_PROPERTY,</span><br><span class="line">    embedding_node_property=VECTOR_EMBEDDING_PROPERTY,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="构建关系："><a href="#构建关系：" class="headerlink" title="构建关系："></a>构建关系：</h3><p>我们在图形中的块之间建立关系，指示它们的顺序以及它们与父 PDF 文档的关联。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"># Create a PDF node</span><br><span class="line">cypher = &quot;&quot;&quot;</span><br><span class="line">MERGE (p:PDF &#123;name: $pdfName&#125;)</span><br><span class="line">RETURN p</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">kg.query(cypher, params=&#123;&#x27;pdfName&#x27;: &quot;path/to/your/pdf/file.pdf&quot;&#125;)</span><br><span class="line"></span><br><span class="line"># Connect chunks to their parent PDF with a PART_OF relationship</span><br><span class="line">cypher = &quot;&quot;&quot;</span><br><span class="line">MATCH (c:Chunk), (p:PDF)</span><br><span class="line">WHERE p.name = $pdfName</span><br><span class="line">MERGE (c)-[newRelationship:PART_OF]-&gt;(p)</span><br><span class="line">RETURN count(newRelationship)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">kg.query(cypher, params=&#123;&#x27;pdfName&#x27;: &quot;path/to/your/pdf/file.pdf&quot;&#125;)</span><br><span class="line"></span><br><span class="line"># Create a NEXT relationship between subsequent chunks</span><br><span class="line">cypher = &quot;&quot;&quot;</span><br><span class="line">MATCH (c1:Chunk), (c2:Chunk)</span><br><span class="line">WHERE c1.chunkSeqId = c2.chunkSeqId - 1</span><br><span class="line">MERGE (c1)-[r:NEXT]-&gt;(c2)</span><br><span class="line">RETURN count(r)</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">kg.query(cypher)</span><br></pre></td></tr></table></figure>

<h3 id="问答："><a href="#问答：" class="headerlink" title="问答："></a>问答：</h3><p>最后，我们可以利用构建的知识图谱来执行问答任务。我们从矢量存储中创建一个检索器和一个聊天机器人问答链，以根据 PDF 文档的内容回答问题。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># Create a retriever from the vector store</span><br><span class="line">retriever = neo4j_vector_store.as_retriever()</span><br><span class="line"></span><br><span class="line"># Create a chatbot Question &amp; Answer chain from the retriever</span><br><span class="line">chain = RetrievalQAWithSourcesChain.from_chain_type(</span><br><span class="line">    OpenAI(temperature=0), </span><br><span class="line">    chain_type=&quot;stuff&quot;,</span><br><span class="line">    retriever=retriever</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># Ask a question</span><br><span class="line">question = &quot;What is the main topic of this PDF document?&quot;</span><br><span class="line">answer = chain(</span><br><span class="line">    &#123;&quot;question&quot;: question&#125;,</span><br><span class="line">    return_only_outputs=True,</span><br><span class="line">)</span><br><span class="line">print(textwrap.fill(answer[&quot;answer&quot;]))</span><br></pre></td></tr></table></figure>

<p>以下是一些查询，用于检查neo4j的数据</p>
<p>节点数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Returns the node count</span><br><span class="line">kg.query(&quot;&quot;&quot;</span><br><span class="line">         MATCH (n)</span><br><span class="line">         RETURN count(n) as nodeCount</span><br><span class="line">         &quot;&quot;&quot;)</span><br></pre></td></tr></table></figure>

<p>打印架构</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kg.refresh_schema()</span><br><span class="line">print(kg.schema)</span><br></pre></td></tr></table></figure>

<p>显示索引</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kg.query(&quot;SHOW INDEXES&quot;)</span><br></pre></td></tr></table></figure>

<p>示例输出</p>
<p>Neo4j 仪表板<br><img src="/../asset_graphrag/06.png"></p>
<p>Q&amp;A 输出</p>
<p><img src="/../asset_graphrag/07.png"></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>从传统的检索增强生成（RAG）转向图形 RAG，代表了我们与大型语言模型（LLMs）交互方式的重要进步</p>
<p>这个转变解决了一个根本性的挑战：如何提高LLMs在复杂查询中提供上下文准确答案的能力。</p>
<p>比较这两种方法时，图形 RAG 在处理上下文复杂查询方面的优势变得明显。</p>
<p>Conventional RAG techniques often misses the mark on contextually complex questions.<br>传统的 RAG 技术经常在上下文复杂的问题上失准。</p>
<p>相比之下，图形 RAG 利用更复杂的数据网络，提供能够捕捉查询细微差别更深入理解的响应。</p>
<p>然而，图形 RAG 的有效性并非一刀切的解决方案。</p>
<p>这仍然高度依赖于底层知识图谱的质量、深度和广度。</p>
<p>在知识图谱受限或偏向特定领域的情况下，Graph RAG 的性能可能不会超过传统的 RAG 方法。</p>
<p>话虽如此，这种转变有望带来更好地模拟人类思维和发现的人工智能系统。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/03/26/coding-for-next/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/03/26/coding-for-next/" class="post-title-link" itemprop="url">编码是一个值得学习的技能吗？</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-03-26 22:42:45 / 修改时间：22:57:25" itemprop="dateCreated datePublished" datetime="2024-03-26T22:42:45+08:00">2024-03-26</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>许多在 50 年前出现的技术都遵循了两种轨迹之一：要么随着时代的发展而进化，要么消失在人们的视野中。一个例子是 1938 年推出的第一台可编程机械计算机。由于内存限制和庞大的重量，很难想象今天我们的家庭或工作场所会容纳这样的设备。</p>
<p>事实上，有许多科技先驱对计算机的未来及其与人类的互动进行了推测。这种做法并不新鲜 - 它有着悠久的历史，持续至今，并预计将持续到未来。塑造未来的前景总是吸引我们，引发支持者和反对者之间的无休止辩论。</p>
<p>以下是 NVIDIA 首席执行官 Jensen Huang 在题为“谁将塑造人工智能的未来？”的活动上的发言</p>
<p>“It’s going to sound completely opposite of what people feel. You probably recall over the course of the last 10–15 years, almost everybody who sits on a stage like this would tell you it is vital that your children learn computer science … everybody should learn how to program. In fact, it’s almost exactly the opposite. It is our job to create computing technology such that nobody has to program and that the programming language is human. Everybody in the world is now a programmer. This is the miracle of artificial intelligence”</p>
<p>我不完全同意 Jensen（至少最初不同意），因为学习编程本身就是一种揭示基本解决方案的方式，直到我们能够开发专注于解决实际业务场景的代码行。我的意思是，编程就像学习任何其他学科或学科一样，培养了我们在决策中的判断力。仅仅通过指示 ChatGPT 来构建整个移动应用程序是具有挑战性的。它可能能够做到，但是在不知道它应该遵循的逻辑顺序的情况下理解整个代码将会很困难。此外，该过程可能存在偏见，并可能在途中遇到错误。</p>
<p>就像其他科学、技术或学科一样，编程将不可避免地受到人工智能的影响，但仍然需要开发人员继续构建更多和更优秀的模型或软件，比如 NVIDIA 销售的那些。</p>
<p>对于 90%的人来说，由于人工智能工程师和提示工程师的团队在所有这些代码行后面工作，提示将变得越来越用户友好，最终优化我们与之交互的聊天界面，无论我们使用 ChatGPT、Gemini、Claude 3、Copilot等等。重点是，虽然主要角色（人工智能）似乎似乎做了一切，但实际上在我们看到的屏幕后面，有人类开发人员通过代码继续投入时间和知识。</p>
<p>Jensen的言论指出了一个基本观念：从学习过程到实施（可能只需要一个提示），人工智能在编程中的角色正处于转变之中。山姆·奥特曼在多次采访中强调，编程在未来仍将保持其重要性，但形式将与我们今天习惯的不同。这种变化主要是由于人工智能在这个领域中作为进步的催化剂的作用。</p>
<p>另一方面，有人声称，像 Stability AI 的首席执行官兼联合创始人 Emad Mostaque 一样的声音认为，在大约 5 年内，我们所熟知的程序员可能不再存在。</p>
<p>对于那些渴望涉足编程的人来说，这可能是一个令人畏惧和不确定的情况。然而，现实情况是，未来大部分的代码开发可能会由人工智能而不是人类编写的代码驱动。这让我产生了以下的问题：</p>
<pre><code>编程语言的目标之一不就是以更易于理解和使用的方式发展，从而使更多的人能够通过编码来创造解决方案吗？
</code></pre>
<p><img src="/../asset_coding/01.webp"></p>
<p>回顾历史，特别是 20 世纪 50 年代，我们可以看到编程语言复杂且需要广泛的专业知识才能掌握。然而，随着时间的推移，这种进入门槛逐渐变得更加容易接触，使得编码变得更加包容和吸引人。</p>
<p>如今我们通过人工智能助手获取代码。因此，许多非程序员或初学者可能正在以与过去完全不同的方式创建他们的第一行代码。然而，就功能而言，他们有潜力达到相同的解决方案或目标。</p>
<p>我们所熟知的编程在未来 3 到 5 年内不太可能消失，但是我们中的一些人可能会想知道 20 年后会发生什么。我们还会继续编码吗？对此，我会质疑到那时我们是否还会使用计算机。</p>
<h2 id="接下来的抽象层"><a href="#接下来的抽象层" class="headerlink" title="接下来的抽象层"></a>接下来的抽象层</h2><p>在标准计算机中，我们通常会遇到软件层和硬件层。这两个层之间的通信是通过所谓的机器语言来实现的，通常以二进制系统表示，这对人类来说可能很复杂。通过对体系结构的详细研究，可以明显看出，抽象和复杂性的元素与每个抽象层有不同程度的关联。随着时间的推移，这些层积累起来，给我们带来了现代计算机，大大提高了人与机器之间的通信便利性，使我们能够使用更简单的指令执行各种任务。</p>
<p><img src="/../asset_coding/02.webp"></p>
<p>AI 助手的概念将是一个额外的层次，它将被添加到图表的顶部。这代表了人类更容易与计算机进行交流的机会。目前，多模态在捕捉我们想要提供的尽可能多的信息方面发挥着重要作用（音频、图像、文本），这些信息被 AI 用作启动生成接近我们期望的回应的提示。</p>
<p>在这个意义上，现在我们正在经历一波代码 AI 助手的浪潮，比如 GitHub Copilot，CodiumAI，AWS Code Whisperer，Tabnine，它们无疑正在改变人们编写代码的方式。</p>
<p>然而，尽管人工智能似乎会让程序员黯然失色，但现实是许多当前的模型仍然容易出错。此外，对于聊天机器人的回应缺乏控制。一个例子是最近谷歌 Gemini 引起轰动的事件，它拒绝生成白人的图像，并过分强调包容性。</p>
<p>如果您经常使用人工智能来生成代码，您可能已经注意到，由于其复杂性或项目所需的代码量，一次性生成高质量代码自然是一项挑战。</p>
<p>现在，事实是人工智能是一个很棒的编码助手，它将帮助我们更快地编写和调试代码，但如果我们期望它能够自己从零开始构建软件，它的能力仍然有限。不可避免地，它将在未来这样做，也许比我们预期的要早，但这是否意味着我们应该停止学习编程呢？</p>
<p>当然不是。</p>
<p>AI 艺术并没有阻止这个领域的爱好者进行绘画、绘图或创作。同样地，AI 不会使程序员过时，而是代表编码中的接下来的一层抽象，使人与机器之间的交互更加顺畅。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://medium.com/artificial-corner/nvidia-ceo-advises-against-learning-to-code-is-coding-still-a-worthwhile-skill-to-learn-704f091a8078">https://medium.com/artificial-corner/nvidia-ceo-advises-against-learning-to-code-is-coding-still-a-worthwhile-skill-to-learn-704f091a8078</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/03/26/sso/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/03/26/sso/" class="post-title-link" itemprop="url">单点登录（SSO）工作原理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-03-26 22:00:30 / 修改时间：22:56:17" itemprop="dateCreated datePublished" datetime="2024-03-26T22:00:30+08:00">2024-03-26</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>17 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Single Sign-On (SSO) is an authentication process that allows users to access multiple applications with a single login. This is accomplished using a central authentication server that stores the user’s credentials and verifies them for each application.</p>
<p>单点登录（SSO）是一种身份验证过程，允许用户使用单个登录访问多个应用程序。这是通过使用一个中央身份验证服务器来实现的，该服务器存储用户的凭据并为每个应用程序进行验证。</p>
<p>The idea of SSO is not new in the Cloud era. The on-premises identity solutions that enabled businesses to safely link their PCs, networks, and servers in the middle to late 1990s are the source of SSO technology. Around this time, companies started using specialized systems to handle user IDs, such as Lightweight Directory Access Protocol (LDAP) and Microsoft Active Directory (AD). After that, they used on-premises SSO or Web Access Management (WAM) products to protect access.</p>
<p>SSO 的概念在云时代并不新鲜。在 20 世纪 90 年代中后期，使企业能够安全地将其个人电脑、网络和服务器连接起来的本地身份解决方案是 SSO 技术的来源。在此期间，公司开始使用专门的系统来处理用户 ID，例如轻量级目录访问协议（LDAP）和 Microsoft Active Directory（AD）。之后，他们使用本地 SSO 或 Web 访问管理（WAM）产品来保护访问。</p>
<h2 id="SSO-组成"><a href="#SSO-组成" class="headerlink" title="SSO 组成"></a>SSO 组成</h2><ul>
<li><p>Identity Provider (IdP): This is the central authentication server. It’s where you enter your credentials and get verified. Think of it as a high-security building entrance.<br>身份提供者（IdP）：这是中央认证服务器。这是您输入凭据并进行验证的地方。将其视为高安全性建筑入口。</p>
</li>
<li><p>Service Provider (SP): These individual applications rely on SSO for user login. Your work email, project management tool, and CRM platform can all be SPs. Imagine these as individual offices within the secure building.<br>服务提供商（SP）：这些个别应用程序依赖 SSO 进行用户登录。您的工作电子邮件、项目管理工具和 CRM 平台都可以是 SP。将其想象为安全建筑内的个别办公室。</p>
</li>
<li><p>SSO Server: This is the bridge between the IdP and SPs. It handles the communication and securely transmits authentication tokens between them. Think of it as a secure hallway connecting the entrance to the various offices.<br>SSO 服务器：这是 IdP 和 SP 之间的桥梁。它处理通信并安全地传输身份验证令牌。将其视为连接入口和各个办公室的安全走廊。</p>
</li>
</ul>
<h2 id="SSO-工作流程"><a href="#SSO-工作流程" class="headerlink" title="SSO 工作流程"></a>SSO 工作流程</h2><p>Google and other services are excellent examples of how SSO works. Let’s take the example of trying to access Trello using your Google account. You don’t need to create a new user account on Trello and remember a new set of usernames&#x2F;passwords.<br>谷歌和其他服务是单点登录(SSO)运作的绝佳例子。以使用谷歌账户访问 Trello 为例。您无需在 Trello 上创建新的用户账户并记住一组新的用户名&#x2F;密码。</p>
<p>For example, when you try to log in to Trello with your Google account, it redirects you to the central service hosted on accounts.google.com. Here, you will see a sign-in form to input your credentials. If the authentication process is successful, Google redirects you to Trello, where you can gain access where you are automatically signed in.<br>例如，当您尝试使用 Google 账户登录 Trello 时，它会将您重定向到托管在 accounts.google.com 上的中央服务。在这里，您将看到一个登录表单，输入您的凭据。如果身份验证过程成功，Google 会将您重定向到 Trello，您将自动登录并获得访问权限。</p>
<p>如果您想使用 Google 账户访问 Trello，以下是所需的步骤：</p>
<ul>
<li><p>User requests access: Use the Trello login web page and select Google account as a login method.<br>用户请求访问：使用 Trello 登录网页，并选择 Google 账户作为登录方式。</p>
</li>
<li><p>Redirection to IdP: Trello redirects the user to the Google login page.<br>重定向到 IdP：Trello 将用户重定向到 Google 登录页面。</p>
</li>
<li><p>Login page served: The user is served with the Google login page.<br>登录页面已提供：用户被提供了谷歌登录页面。</p>
</li>
<li><p>Credentials entered: The user enters their Google credentials.<br>输入的凭据：用户输入他们的谷歌凭据。</p>
</li>
<li><p>SSO Server verification: Google sends authentication info to the SSO Authorization server<br>SSO 服务器验证：谷歌将认证信息发送给 SSO 授权服务器</p>
</li>
<li><p>Authentication at IdP: The Authorization server returns the auth token (SAML) if the credentials are valid.<br>在 IdP 上进行身份验证：如果凭证有效，授权服务器将返回授权令牌（SAML）。</p>
</li>
<li><p>Access granted: Google sends the auth token to the Trello<br>授权已授予：Google 将认证令牌发送给 Trello</p>
</li>
<li><p>Validate token: In the last step, Trello sends the token to the Google Authorization server to validate it<br>验证令牌：在最后一步中，Trello 将令牌发送给 Google 授权服务器以进行验证</p>
</li>
<li><p>Token valid: If the token is valid, Trello will allow access to the user and store the session for future interactions<br>令牌有效：如果令牌有效，Trello 将允许用户访问并存储会话以供将来交互使用</p>
</li>
</ul>
<p><img src="/../asset_sso/01.webp"></p>
<h2 id="单点登录的好处"><a href="#单点登录的好处" class="headerlink" title="单点登录的好处"></a>单点登录的好处</h2><p>单点登录有多个好处，即：</p>
<ul>
<li><p>Improved user experience: Users do not need to remember multiple usernames and passwords.<br>改进的用户体验：用户不需要记住多个用户名和密码。</p>
</li>
<li><p>Increased security: Users are less likely to reuse passwords across applications.<br>增加安全性：用户更不太可能在不同应用程序中重复使用密码。</p>
</li>
<li><p>Simplified user access auditing: Ensuring the appropriate individuals have access to resources and sensitive data can be challenging. SSO solutions can configure user access permissions according to their role, department, and seniority level.<br>简化用户访问审计：确保适当的人员可以访问资源和敏感数据可能具有挑战性。SSO 解决方案可以根据用户的角色、部门和资历级别配置用户访问权限。</p>
</li>
</ul>
<h2 id="单点登录的类型"><a href="#单点登录的类型" class="headerlink" title="单点登录的类型"></a>单点登录的类型</h2><p>与 SSO 一起工作，您应该了解不同的标准和协议。一些常见的协议类型包括：</p>
<ul>
<li><p>SAML: This is the most common type of SSO. It uses the SAML protocol to exchange authentication information between the SSO server and applications.<br>SAML：这是最常见的单点登录类型。它使用 SAML 协议在单点登录服务器和应用程序之间交换身份验证信息。</p>
</li>
<li><p>Open Authorization (OAuth) 2.0): It provides delegated access to server resources on behalf of a resource owner. It specifies how tokens are transferred, allowing a user’s identity to be authenticated by an IDP and the credentials to be used to access APIs.<br>开放授权（OAuth）2.0：它代表资源所有者提供对服务器资源的委托访问。它规定了令牌的传输方式，允许用户的身份由 IDP 进行验证，并使用凭据访问 API。</p>
</li>
<li><p>Open ID Connect (OIDC) is a newer type of SSO based on OAuth 2.0. It is a more straightforward protocol than SAML and more accessible to integrate with web applications.<br>Open ID Connect（OIDC）是基于 OAuth 2.0 的一种较新的单点登录（SSO）类型。它是比 SAML 更直接的协议，更容易与 Web 应用程序集成。</p>
</li>
</ul>
<p>要了解更多关于 OAuth 2.0 的信息，请查看以下<a target="_blank" rel="noopener" href="https://newsletter.techworld-with-milan.com/i/138606726/how-does-oauth-work">链接</a>。请注意，您也可以在 <a target="_blank" rel="noopener" href="https://learning.postman.com/docs/sending-requests/authorization/oauth-20/">Postman 中使用 OAuth 2.0</a>。</p>
<p><img src="/../asset_sso/02.webp"></p>
<p>一些其他的 SSO 类型，如 Kerberos 和智能卡认证，使用较少。</p>
<ul>
<li><p>Kerberos allows users to obtain service tickets from the KDC using their credentials. These tickets are then presented to applications for access, eliminating the need for repeated logins. However, Kerberos relies on shared secrets between the KDC and all participants, making it less suitable for internet-facing SSO due to security concerns like compromised servers exposing credentials.<br>Kerberos 允许用户使用他们的凭据从 KDC 获取服务票据。然后将这些票据提供给应用程序以进行访问，从而消除了重复登录的需要。然而，Kerberos 依赖于 KDC 和所有参与者之间的共享密钥，这使得它不太适合面向互联网的 SSO，因为存在安全问题，比如受损的服务器暴露凭据。</p>
</li>
<li><p>A smart card that holds an identity works with the SSO system (like a lock) to grant access to applications (doors) without needing separate logins for each. It adds a physical element to the authentication process, making it more resistant to unauthorized access. Yet, the user must physically carry it.<br>一张持有身份信息的智能卡与 SSO 系统（类似于锁）配合使用，可以在不需要单独登录的情况下，为应用程序（门）提供访问权限。它为认证过程增加了物理元素，使其更加抵抗未经授权的访问。然而，用户必须亲自携带它。</p>
</li>
</ul>
<h2 id="如何选择适当的-SSO-协议"><a href="#如何选择适当的-SSO-协议" class="headerlink" title="如何选择适当的 SSO 协议"></a>如何选择适当的 SSO 协议</h2><p>在选择适当的协议时，应考虑以下因素：</p>
<ul>
<li><p>Enterprise vs. consumer applications: SAML is often preferred for enterprise applications due to its extensive support and integration capabilities with enterprise identity providers and complex authentication scenarios. OAuth 2.0 and OIDC are more suited for consumer-facing applications, offering flexibility and compatibility with mobile and web applications.<br>企业与消费者应用：由于 SAML 在与企业身份提供者和复杂认证场景的支持和集成能力方面非常强大，因此在企业应用中通常更受青睐。而 OAuth 2.0 和 OIDC 更适用于面向消费者的应用，提供了与移动和 Web 应用的灵活性和兼容性。</p>
</li>
<li><p>Authorization vs. authentication: If your primary need is authentication (verifying user identity), SAML or OIDC are your go-to options. OIDC, built on top of OAuth 2.0, provides an additional identity layer over OAuth’s authorization capabilities. Use OAuth 2.0 when your application needs to request access to user resources without exposing user passwords.<br>授权与认证：如果您的主要需求是认证（验证用户身份），SAML 或 OIDC 是您的首选选项。OIDC 是建立在 OAuth 2.0 之上的，为 OAuth 的授权能力提供了额外的身份层。当您的应用程序需要请求访问用户资源而不暴露用户密码时，请使用 OAuth 2.0。</p>
</li>
<li><p>Evaluate application and platform compatibility: Check the SSO protocols’ compatibility with your existing infrastructure and the applications you plan to integrate. Some legacy or enterprise systems might support SAML more broadly, while modern applications often favor OAuth 2.0 and OIDC because they are API-friendly.<br>评估应用程序和平台的兼容性：检查 SSO 协议与您现有基础设施和计划集成的应用程序的兼容性。一些传统或企业系统可能更广泛地支持 SAML，而现代应用程序通常更喜欢 OAuth 2.0 和 OIDC，因为它们对 API 友好。</p>
</li>
<li><p>Consider the user experience: OIDC and OAuth 2.0’s modern, token-based approach can offer a smoother and more integrated user experience, especially for web and mobile applications.<br>考虑用户体验：OIDC 和 OAuth 2.0 的现代、基于令牌的方法可以为 Web 和移动应用提供更流畅、更集成的用户体验。</p>
</li>
<li><p>Future-proofing: Consider the future direction of your application ecosystem. Are you moving towards cloud-based services, APIs, and mobile apps? OAuth 2.0 and OIDC may offer more flexibility and are generally considered more forward-looking in cloud and mobile services.<br>未来保护：考虑您的应用生态系统的未来方向。您是否正在转向基于云的服务、API 和移动应用？OAuth 2.0 和 OIDC 可能提供更多的灵活性，并且在云和移动服务中通常被认为更具前瞻性。</p>
</li>
<li><p>Compliance and regulatory requirements: Ensure the chosen protocol meets any specific regulatory requirements relevant to your industry, such as GDPR, HIPAA, or others that may dictate specific security or privacy standards.<br>合规和监管要求：确保所选择的协议符合与您所在行业相关的任何特定监管要求，例如 GDPR、HIPAA 或其他可能规定特定安全或隐私标准的要求。</p>
</li>
</ul>
<h2 id="SSO-实施"><a href="#SSO-实施" class="headerlink" title="SSO 实施"></a>SSO 实施</h2><p>市场上有许多产品可用于 SSO</p>
<ul>
<li><p>Microsoft Entra ID (formerly known as a Microsoft Active Directory). Ideal for organizations already invested in the Microsoft ecosystem, it offers seamless integration with Office 365, Dynamics CRM, and other Microsoft services. It’s known for its robust security features and comprehensive management capabilities.<br>Microsoft Entra ID（以前称为 Microsoft Active Directory）。适用于已经投资于 Microsoft 生态系统的组织，它与 Office 365、Dynamics CRM 和其他 Microsoft 服务无缝集成。它以其强大的安全功能和全面的管理能力而闻名。</p>
</li>
<li><p>Okta is a popular cloud-based SSO solution known for its ease of use, scalability, and wide range of application integrations. It’s a strong option for organizations seeking a comprehensive identity and access management (IAM) platform.<br>Okta 是一种流行的基于云的单点登录（SSO）解决方案，以其易用性、可扩展性和广泛的应用集成而闻名。对于寻求全面身份和访问管理（IAM）平台的组织来说，它是一个强大的选择。</p>
</li>
<li><p>Ping Identity. Known for its flexibility, Ping Identity caters to enterprises with complex security requirements. It offers strong mobile and API security options, making it suitable for organizations needing high levels of customization and security.<br>Ping Identity。以其灵活性而闻名，适用于具有复杂安全需求的企业。它提供强大的移动和 API 安全选项，适用于需要高度定制和安全性的组织。</p>
</li>
<li><p>OneLogin. With a focus on simplicity and integration, OneLogin offers a straightforward SSO solution that works well for small to medium-sized businesses. It provides real-time threat detection and AI-powered authentication for enhanced security.<br>OneLogin。专注于简单性和集成性，OneLogin 提供了一个适用于中小型企业的直接 SSO 解决方案。它提供实时威胁检测和 AI 驱动的身份验证，以增强安全性。</p>
</li>
<li><p>Auth0 is favored for its developer-friendly approach. It provides powerful customization options, making it a go-to for organizations that must tailor their authentication flows. It supports a wide range of programming languages and frameworks.<br>Auth0 因其开发者友好的方法而备受青睐。它提供强大的定制选项，使其成为那些必须定制其身份验证流程的组织的首选。它支持广泛的编程语言和框架。</p>
</li>
</ul>
<p><img src="/../asset_sso/03.webp"></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://medium.com/@techworldwithmilan/how-does-single-sign-on-sso-work-31ffa1afcc63">https://medium.com/@techworldwithmilan/how-does-single-sign-on-sso-work-31ffa1afcc63</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/03/20/electron-vue-chatgpt/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/03/20/electron-vue-chatgpt/" class="post-title-link" itemprop="url">用chatgpt零基础开发electron+vue桌面应用</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-03-20 20:32:57" itemprop="dateCreated datePublished" datetime="2024-03-20T20:32:57+08:00">2024-03-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-03-30 21:25:18" itemprop="dateModified" datetime="2024-03-30T21:25:18+08:00">2024-03-30</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>最近在想了解前端vue，但是不想学习vue，所以就让chatgpt来写代码。实际例子是找一个python实现的工具来porting，因为涉及到OS，所以使用chatgpt来开发electron+vue桌面应用。</p>
<h2 id="先做一个vue的UI"><a href="#先做一个vue的UI" class="headerlink" title="先做一个vue的UI"></a>先做一个vue的UI</h2><p>把vue的架子搭建好，然后把UI图发给chatgpt。<br><img src="/../asset_electronvuechatgpt/01.png"></p>
<p>把图片传给它，然后chatgpt生成代码<br><img src="/../asset_electronvuechatgpt/02.png"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">&lt;template&gt;</span><br><span class="line">  &lt;div id=&quot;menu-bar&quot;&gt;</span><br><span class="line">    &lt;ul&gt;</span><br><span class="line">      &lt;li&gt;&lt;a href=&quot;#file&quot;&gt;File&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">      &lt;li&gt;&lt;a href=&quot;#operate&quot;&gt;Operate&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">      &lt;li&gt;&lt;a href=&quot;#config&quot;&gt;Config&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">    &lt;/ul&gt;</span><br><span class="line">  &lt;/div&gt;</span><br><span class="line">&lt;/template&gt;</span><br><span class="line"></span><br><span class="line">&lt;script&gt;</span><br><span class="line">export default &#123;</span><br><span class="line">  name: &#x27;MenuBar&#x27;</span><br><span class="line">  // You can add your JavaScript here</span><br><span class="line">&#125;</span><br><span class="line">&lt;/script&gt;</span><br><span class="line"></span><br><span class="line">&lt;style scoped&gt;</span><br><span class="line">#menu-bar &#123;</span><br><span class="line">  background-color: #8B0000; /* Dark red color */</span><br><span class="line">  color: white;</span><br><span class="line">  padding: 10px 0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#menu-bar ul &#123;</span><br><span class="line">  list-style-type: none;</span><br><span class="line">  margin: 0;</span><br><span class="line">  padding: 0;</span><br><span class="line">  overflow: hidden;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#menu-bar li &#123;</span><br><span class="line">  float: left;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#menu-bar li a &#123;</span><br><span class="line">  display: block;</span><br><span class="line">  color: white;</span><br><span class="line">  text-align: center;</span><br><span class="line">  padding: 14px 16px;</span><br><span class="line">  text-decoration: none;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/* Change the link color to #9d2b2b on hover */</span><br><span class="line">#menu-bar li a:hover &#123;</span><br><span class="line">  background-color: #9d2b2b;</span><br><span class="line">&#125;</span><br><span class="line">&lt;/style&gt;</span><br></pre></td></tr></table></figure>
<p>发现issue，需要修改一下。问chatgpt</p>
<p><img src="/../asset_electronvuechatgpt/03.png"></p>
<p>不停的询问，调试代码，把chatgpt当作一个编程助手，效率还是不错。UI界面做完后，就该做功能了。</p>
<h2 id="添加功能"><a href="#添加功能" class="headerlink" title="添加功能"></a>添加功能</h2><p>把python功能代码发给chatgpt，然后问chatgpt怎么办，根据他的说法不停的问，同时告诉他我想用的electron框架，chatgpt就会生成代码，然后不停的问，调试。整体工作量大概3~4天就完成整个工具，我记得当初做python工具用了2周。</p>
<h2 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h2><p>虽然chatgpt帮忙很多，但是不可避免要调试。 render进程用浏览器developer tools调试，不特别讲。<br>调试electron主进程需要讲一下，</p>
<p>在一个electron 应用中有且只有一个主进程。调试主进程的方式也用浏览器来进行debugger</p>
<ul>
<li><p>使用 –inspect&#x3D;[port] 来设置一个运行端口，比如在vscode package.json中配置如下脚本：</p>
<p>“electron:serve”: “vue-cli-service electron:serve –inspect&#x3D;5858”</p>
</li>
<li><p>在终端输入 npm run electron:serve</p>
</li>
<li><p>浏览器输入： chrome:&#x2F;&#x2F;inspect，配置Discover network targets的端口为上述端口(5858)，点击下方inspect</p>
</li>
<li><p>点击inspect之后就会弹出一个调试主进程的浏览器窗口，就可以在这个窗口中去debugger当前electron应用的主进程了。如下：进入Sources标签，ctrl+p输入main.js或者其它js，输入断点可以debug</p>
</li>
</ul>
<h2 id="体会"><a href="#体会" class="headerlink" title="体会"></a>体会</h2><p>大模型确实改变了软件行业，编程语言已经不成为开发软件的门槛，软件工程师具备软件思想，熟悉编程语言的基本逻辑，基本语法甚至不是太熟悉也可以开发软件。 而且效率还是很高的. 主要节省时间在于不需要 </p>
<ul>
<li>上网大量搜索并挑选答案, </li>
<li>代码组织</li>
</ul>
<p>人在这个过程中主要起到：</p>
<ul>
<li>构想功能,提出具体步骤, </li>
<li>根据代码来调试, </li>
<li>根据调试问题提出新问题给chatgpt解决.</li>
</ul>
<p>整体上来讲chatgpt起到编程手册和代码库的作用.大模型作为软件工程师的助手，大大降低了开发难度，提高效率。以后不使用大模型的软件工程师，被使用大模型的软件工程师淘汰是肯定的。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/03/12/ollama/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/03/12/ollama/" class="post-title-link" itemprop="url">ollama使用方法</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-03-12 11:49:50 / 修改时间：15:41:44" itemprop="dateCreated datePublished" datetime="2024-03-12T11:49:50+08:00">2024-03-12</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>591</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Ollama 终于可以在 Windows 上运行了</p>
<h2 id="下载-Ollma-安装文件"><a href="#下载-Ollma-安装文件" class="headerlink" title="下载 Ollma 安装文件"></a>下载 Ollma 安装文件</h2><p>访问 <a target="_blank" rel="noopener" href="https://ollama.com/download%EF%BC%8C%E9%80%89%E6%8B%A9">https://ollama.com/download，选择</a> Windows，单击 “Download for Windows (Preview)” 进行下载。</p>
<h2 id="安装-Ollama"><a href="#安装-Ollama" class="headerlink" title="安装 Ollama"></a>安装 Ollama</h2><p>双击下载的 “OllamaSetup.exe”，直接安装就可以了。安装完以后，ollama app后台进程会自动运行。</p>
<h2 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h2><p>Ollama 下载的模型模型保存在 C 盘，如果想更改默认路径的话，可以通过设置 OLLAMA_MODELS 进行修改。</p>
<p>OLLAMA_MODELS：F:\OllamaCache</p>
<p>Ollama 默认提供 OpenAI 的兼容 API，默认端口是 11434，默认只可以通过 localhost 进行访问，如果想公开访问的话，可以通过设置 OLLAMA_HOST 进行修改。</p>
<p>OLLAMA_HOST：0.0.0.0</p>
<p>修改环境变量后，需要去任务管理器杀掉ollama的各个进程或者重启电脑，然后重新打开cmd，set看下环境变量，再重新启动ollama</p>
<h2 id="使用-Ollama"><a href="#使用-Ollama" class="headerlink" title="使用 Ollama"></a>使用 Ollama</h2><p>访问 <a target="_blank" rel="noopener" href="https://ollama.com/library">https://ollama.com/library</a></p>
<p>搜索你要使用的模型，主流的模型，比如 llama2、qwen1.5、mixtral 等，Ollama都支持。</p>
<p>下面以允许 qwen 为例，我们要运行 7b 的模型，</p>
<p>ollama App在后台已经跑起来的情况下，打开cmd，输入</p>
<p><code>ollama run qwen:7b</code></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/03/11/make-million-llm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/03/11/make-million-llm/" class="post-title-link" itemprop="url">使用Python从头开始构建一个百万参数的LLM</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-03-11 20:32:51" itemprop="dateCreated datePublished" datetime="2024-03-11T20:32:51+08:00">2024-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-03-12 09:27:37" itemprop="dateModified" datetime="2024-03-12T09:27:37+08:00">2024-03-12</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>42k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1:17</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>构建LLaMA架构的逐步指南<br>Fareed Khan</p>
<p>制作自己的大型语言模型（LLM）是一件很酷的事情，许多像谷歌、推特和Facebook这样的大公司都在做。他们发布了不同版本的这些模型，如70亿、130亿或700亿。甚至较小的社区也在做自己的模型。你可能已经阅读过创建自己的LLM的博客或观看了相关视频，但它们通常更多地讨论理论，而不是实际的步骤和代码。</p>
<p>在这篇博客中，我将尝试制作仅有230万个参数的LLM，有趣的是我们不需要一台高级的GPU。我们将遵循LLaMA 1 Paper的方法来进行。我们会保持简单，并使用一个基本的数据集，这样你就可以看到创建自己的百万参数LLM有多容易。</p>
<h2 id="先决条件"><a href="#先决条件" class="headerlink" title="先决条件"></a>先决条件</h2><p>需要您对面向对象编程（OOP）和神经网络（NN）有基本的了解。熟悉PyTorch在编码方面也会有帮助。</p>
<h2 id="理解LLaMA的Transformer架构"><a href="#理解LLaMA的Transformer架构" class="headerlink" title="理解LLaMA的Transformer架构"></a>理解LLaMA的Transformer架构</h2><p>在深入研究使用LLaMA方法创建我们自己的LLM之前，了解LLaMA的架构是至关重要的。下面是普通Transformer和LLaMA之间的比较图。<br><img src="/../asset_makemillionllm/01.webp"></p>
<p>Transformers和Llama架构（由Umar Jamil提出的Llama架构）之间的区别</p>
<p>如果您对vanilla transformer架构不熟悉，可以阅读这篇<a target="_blank" rel="noopener" href="https://medium.com/@fareedkhandev/understanding-transformers-a-step-by-step-math-example-part-1-a7809015150a">博客</a>了解基本信息。</p>
<p>让我们更详细地了解LLaMA的基本概念</p>
<h2 id="预归一化使用RMSNorm："><a href="#预归一化使用RMSNorm：" class="headerlink" title="预归一化使用RMSNorm："></a>预归一化使用RMSNorm：</h2><p>In the LLaMA approach, a technique called RMSNorm is employed for normalizing the input of each transformer sub-layer. This method is inspired by GPT-3 and is designed to optimize the computational cost associated with Layer Normalization. RMSNorm provides similar performance to LayerNorm but reduces the running time significantly (by 7%∼64%).</p>
<p>在LLaMA方法中，采用了一种称为RMSNorm的技术来对每个transformer子层的输入进行归一化。这种方法受到了GPT-3的启发，并旨在优化与层归一化相关的计算成本。RMSNorm提供了与LayerNorm类似的性能，但显著减少了运行时间（约7%∼64%）。</p>
<p><img src="/../asset_makemillionllm/02.webp"><br>均方根层归一化论文（<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.07467%EF%BC%89">https://arxiv.org/abs/1910.07467）</a></p>
<p>It achieves this by emphasizing re-scaling invariance and regulating the summed inputs based on the root mean square (RMS) statistic. The primary motivation is to simplify LayerNorm by removing the mean statistic. Interested readers can explore the detailed implementation of RMSNorm here.<br>通过强调重新缩放不变性并根据均方根（RMS）统计量调节总输入，它实现了这一点。主要动机是通过去除均值统计量来简化LayerNorm。有兴趣的读者可以在<a target="_blank" rel="noopener" href="https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py">这里</a>探索RMSNorm的详细实现。</p>
<h2 id="SwiGLU激活函数"><a href="#SwiGLU激活函数" class="headerlink" title="SwiGLU激活函数"></a>SwiGLU激活函数</h2><p>LLaMA introduces the SwiGLU activation function, drawing inspiration from PaLM. To understand SwiGLU, it’s essential to first grasp the Swish activation function. SwiGLU extends Swish and involves a custom layer with a dense network to split and multiply input activations.<br>LLaMA引入了SwiGLU激活函数，灵感来自于PaLM。要理解SwiGLU，首先必须掌握Swish激活函数。SwiGLU扩展了Swish，并涉及一个自定义层，其中包含一个密集网络来分割和乘以输入激活。</p>
<p><img src="/../asset_makemillionllm/03.webp"><br>SwiGLU: GLU Variants Improve Transformer (<a target="_blank" rel="noopener" href="https://kikaben.com/swiglu-2020/">https://kikaben.com/swiglu-2020/</a>)</p>
<p>The aim is to enhance the expressive power of the model by introducing a more sophisticated activation function. Further details on SwiGLU can be found in the associated paper.<br>目标是通过引入更复杂的激活函数来增强模型的表达能力。有关SwiGLU的更多详细信息，请参阅相关论文。</p>
<h2 id="旋转嵌入（RoPE）"><a href="#旋转嵌入（RoPE）" class="headerlink" title="旋转嵌入（RoPE）"></a>旋转嵌入（RoPE）</h2><p>Rotary Embeddings, or RoPE, is a type of position embedding used in LLaMA. It encodes absolute positional information using a rotation matrix and naturally includes explicit relative position dependency in self-attention formulations. RoPE offers advantages such as scalability to various sequence lengths and decaying inter-token dependency with increasing relative distances.<br>旋转嵌入，或称RoPE，是LLaMA中使用的一种位置嵌入类型。它使用旋转矩阵对绝对位置信息进行编码，并在自注意力公式中自然地包含了显式的相对位置依赖关系。RoPE具有可扩展到不同序列长度的优势，并且随着相对距离的增加，它能够减弱令牌之间的依赖关系。</p>
<p>This is achieved by encoding relative positions through multiplication with a rotation matrix, resulting in decayed relative distances — a desirable feature for natural language encoding. Those interested in the mathematical details can refer to the RoPE paper.<br>这是通过使用旋转矩阵对相对位置进行乘法编码来实现的，从而导致相对距离的衰减 - 这是自然语言编码的一个理想特征。对于对数学细节感兴趣的人可以参考RoPE论文。</p>
<p>In addition to these concepts, the LLaMA paper introduces other significant approaches, including the use of the AdamW optimizer with specific parameters, efficient implementations such as the causal multi-head attention operator available in the xformers library, and manually implemented backward functions for transformer layers to optimize computation during backward passes.<br>除了这些概念之外，LLaMA论文还介绍了其他重要的方法，包括使用具体参数的AdamW优化器，高效实现，如xformers库中提供的因果多头注意力操作符，以及手动实现的反向函数，用于在反向传递过程中优化计算。</p>
<h2 id="基本设置"><a href="#基本设置" class="headerlink" title="基本设置"></a>基本设置</h2><p>我们将在整个项目中使用一系列的Python库，所以先导入它们：</p>
<pre><code># PyTorch for implementing LLM (No GPU)
import torch

# Neural network modules and functions from PyTorch
from torch import nn
from torch.nn import functional as F

# NumPy for numerical operations
import numpy as np

# Matplotlib for plotting Loss etc.
from matplotlib import pyplot as plt

# Time module for tracking execution time
import time

# Pandas for data manipulation and analysis
import pandas as pd

# urllib for handling URL requests (Downloading Dataset)
import urllib.request
</code></pre>
<p>此外，创建一个存储模型参数的配置对象。</p>
<pre><code># Configuration object for model parameters
MASTER_CONFIG = &#123;
    # Adding parameters later
&#125;
</code></pre>
<p>这种方法保持了灵活性，允许在将来需要时添加更多的参数。</p>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>在原始的LLaMA论文中，使用了多样的开源数据集来训练和评估模型。</p>
<p><img src="/../asset_makemillionllm/04.webp"><br><a target="_blank" rel="noopener" href="https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/">https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/</a></p>
<p>不幸的是，对于较小的项目来说，利用大量的数据集可能是不切实际的。因此，对于我们的实施，我们将采取一种更为适度的方法，创建一个大幅缩小的LLaMA版本。</p>
<p>Given the constraints of not having access to vast amounts of data, we will focus on training a simplified version of LLaMA using the TinyShakespeare dataset. This open source dataset, available here, contains approximately 40,000 lines of text from various Shakespearean works. This choice is influenced by the Makemore series by Karpathy, which provides valuable insights into training language models.<br>鉴于无法获取大量数据的限制，我们将专注于使用TinyShakespeare数据集训练LLaMA的简化版本。这个开源数据集可以在这里找到，其中包含了大约40,000行来自不同莎士比亚作品的文本。这个选择受到了<a target="_blank" rel="noopener" href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">Karpathy的Makemore系列</a>的影响，该系列提供了训练语言模型的宝贵见解。</p>
<p>While LLaMA was trained on an extensive dataset comprising 1.4 trillion tokens, our dataset, TinyShakespeare, containing around 1 million characters.<br>虽然LLaMA是在包含1.4万亿个标记的大型数据集上进行训练的，但我们的数据集TinyShakespeare只包含约100万个字符。</p>
<p>首先，让我们通过下载来获取我们的数据集</p>
<pre><code># The URL of the raw text file on GitHub
url = &quot;https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt&quot;

# The file name for local storage
file_name = &quot;tinyshakespeare.txt&quot;

# Execute the download
urllib.request.urlretrieve(url, file_name)
</code></pre>
<p>这个Python脚本从指定的URL获取tinyshakespeare数据集，并将其保存在本地，文件名为“tinyshakespeare.txt”。</p>
<p>Next, let’s determine the vocabulary size, which represents the unique number of characters in our dataset. Here’s the code snippet:<br>接下来，让我们确定词汇量，它代表了我们数据集中唯一字符的数量。以下是代码片段：</p>
<pre><code># Read the content of the dataset
lines = open(&quot;tinyshakespeare.txt&quot;, &#39;r&#39;).read()

# Create a sorted list of unique characters in the dataset
vocab = sorted(list(set(lines)))

# Display the first 10 characters in the vocabulary list
print(&#39;Printing the first 10 characters of the vocab list:&#39;, vocab[:10])

# Output the total number of characters in our dataset (Vocabulary Size)
print(&#39;Total number of characters in our dataset (Vocabulary Size):&#39;, len(vocab))
</code></pre>
<p><img src="/../asset_makemillionllm/05.webp"></p>
<p>Now, we’re creating mappings between integers to characters (itos) and characters to integers (stoi). Here’s the code:<br>现在，我们正在创建整数到字符的映射（itos）和字符到整数的映射（stoi）。以下是代码：</p>
<pre><code># Mapping integers to characters (itos)
itos = &#123;i: ch for i, ch in enumerate(vocab)&#125;

# Mapping characters to integers (stoi)
stoi = &#123;ch: i for i, ch in enumerate(vocab)&#125;
</code></pre>
<p><img src="/../asset_makemillionllm/06.webp"></p>
<p>In the original LLaMA paper, the SentencePiece byte-pair encoding tokenizer from Google was used. However, for simplicity, we’ll opt for a basic character-level tokenizer. Let’s create encode and decode functions that we’ll later apply to our dataset:<br>在原始的LLaMA论文中，使用了Google的SentencePiece字节对编码分词器。然而，为了简单起见，我们将选择一个基本的字符级分词器。让我们创建编码和解码函数，稍后将应用于我们的数据集：</p>
<pre><code># Encode function: Converts a string to a list of integers using the mapping stoi
def encode(s):
    return [stoi[ch] for ch in s]

# Decode function: Converts a list of integers back to a string using the mapping itos
def decode(l):
    return &#39;&#39;.join([itos[i] for i in l])

# Example: Encode the string &quot;hello&quot; and then decode the result
decode(encode(&quot;morning&quot;))
</code></pre>
<p>The final line will output morning confirms the proper functionality of the encode and decode functions.<br>最后一行将输出 morning ，确认编码和解码函数的正确功能。</p>
<p>We are now converting our dataset into a torch tensor, specifying its data type for further operations using PyTorch:<br>我们现在将我们的数据集转换为一个torch张量，使用PyTorch指定其数据类型以进行进一步的操作</p>
<pre><code># Convert the dataset into a torch tensor with specified data type (dtype)
dataset = torch.tensor(encode(lines), dtype=torch.int8)

# Display the shape of the resulting tensor
print(dataset.shape)
</code></pre>
<p>The output istorch.Size([1115394]) indicates that our dataset contains approximately one million tokens. It’s worth noting that this is significantly smaller than the LLaMA dataset, which consists of 1.4 trillion tokens.<br>输出 torch.Size([1115394]) 表示我们的数据集包含大约一百万个标记。值得注意的是，这比LLaMA数据集要小得多，后者包含了1.4万亿个标记。</p>
<p>We’ll create a function responsible for splitting our dataset into training, validation, or test sets. In machine learning or deep learning projects, such splits are crucial for developing and evaluating models, and the same principle applies here in replicating a Large Language Model (LLM) approach:<br>我们将创建一个负责将数据集分割为训练、验证或测试集的函数。在机器学习或深度学习项目中，这种分割对于开发和评估模型至关重要，同样的原则也适用于复制大型语言模型（LLM）的方法。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># Function to get batches for training, validation, or testing</span><br><span class="line">def get_batches(data, split, batch_size, context_window, config=MASTER_CONFIG):</span><br><span class="line">    # Split the dataset into training, validation, and test sets</span><br><span class="line">    train = data[:int(.8 * len(data))]</span><br><span class="line">    val = data[int(.8 * len(data)): int(.9 * len(data))]</span><br><span class="line">    test = data[int(.9 * len(data)):]</span><br><span class="line"></span><br><span class="line">    # Determine which split to use</span><br><span class="line">    batch_data = train</span><br><span class="line">    if split == &#x27;val&#x27;:</span><br><span class="line">        batch_data = val</span><br><span class="line">    if split == &#x27;test&#x27;:</span><br><span class="line">        batch_data = test</span><br><span class="line"></span><br><span class="line">    # Pick random starting points within the data</span><br><span class="line">    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))</span><br><span class="line"></span><br><span class="line">    # Create input sequences (x) and corresponding target sequences (y)</span><br><span class="line">    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()</span><br><span class="line">    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()</span><br><span class="line"></span><br><span class="line">    return x, y</span><br></pre></td></tr></table></figure>

<p>Now that our splitting function is defined, let’s establish two parameters crucial for this process:<br>现在我们已经定义了分割函数，让我们确定两个对这个过程至关重要的参数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Update the MASTER_CONFIG with batch_size and context_window parameters</span><br><span class="line">MASTER_CONFIG.update(&#123;</span><br><span class="line">    &#x27;batch_size&#x27;: 8,          # Number of batches to be processed at each random split</span><br><span class="line">    &#x27;context_window&#x27;: 16      # Number of characters in each input (x) and target (y) sequence of each batch</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<p>batch_size determines how many batches are processed at each random split, while context_window specifies the number of characters in each input (x) and target (y) sequence of each batch.<br>batch_size 确定每个随机拆分中处理的批次数量，而 context_window 指定每个批次中输入（ x ）和目标（ y ）序列中的字符数量。</p>
<p>Let’s print a random sample from the train split of batch 8 and context window 16 from our dataset:<br>让我们从我们的数据集中的批次8和上下文窗口16的训练集中打印一个随机样本</p>
<pre><code># Obtain batches for training using the specified batch size and context window
xs, ys = get_batches(dataset, &#39;train&#39;, MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;])

# Decode the sequences to obtain the corresponding text representations
decoded_samples = [(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))]

# Print the random sample
print(decoded_samples)
</code></pre>
<p><img src="/../asset_makemillionllm/07.webp"></p>
<h2 id="评估策略"><a href="#评估策略" class="headerlink" title="评估策略"></a>评估策略</h2><p>Now, we are set to create a function dedicated to evaluating our self-created LLaMA architecture. The reason for doing this before defining the actual model approach is to enable continuous evaluation during the training process.<br>现在，我们准备创建一个专门用于评估我们自己创建的LLaMA架构的函数。在定义实际的模型方法之前这样做的原因是为了在训练过程中实现持续评估。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">@torch.no_grad()  # Don&#x27;t compute gradients for this function</span><br><span class="line">def evaluate_loss(model, config=MASTER_CONFIG):</span><br><span class="line">    # Placeholder for the evaluation results</span><br><span class="line">    out = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    # Set the model to evaluation mode</span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    # Iterate through training and validation splits</span><br><span class="line">    for split in [&quot;train&quot;, &quot;val&quot;]:</span><br><span class="line">        # Placeholder for individual losses</span><br><span class="line">        losses = []</span><br><span class="line"></span><br><span class="line">        # Generate 10 batches for evaluation</span><br><span class="line">        for _ in range(10):</span><br><span class="line">            # Get input sequences (xb) and target sequences (yb)</span><br><span class="line">            xb, yb = get_batches(dataset, split, config[&#x27;batch_size&#x27;], config[&#x27;context_window&#x27;])</span><br><span class="line">            </span><br><span class="line">            # Perform model inference and calculate the loss</span><br><span class="line">            _, loss = model(xb, yb)</span><br><span class="line">            </span><br><span class="line">            # Append the loss to the list</span><br><span class="line">            losses.append(loss.item())</span><br><span class="line"></span><br><span class="line">        # Calculate the mean loss for the split and store it in the output dictionary</span><br><span class="line">        out[split] = np.mean(losses)</span><br><span class="line">    </span><br><span class="line">    # Set the model back to training mode</span><br><span class="line">    model.train()</span><br><span class="line">    </span><br><span class="line">    return out</span><br></pre></td></tr></table></figure>

<p>We have used the loss as a metric to assess the performance of the model during training iterations. Our function iterates through the training and validation splits, computes the mean loss over 10 batches for each split, and finally returns the results. The model is then set back to training mode with model.train().<br>我们使用损失作为度量标准来评估模型在训练迭代过程中的性能。我们的函数通过训练和验证集进行迭代，计算每个集合中10个批次的平均损失，并最终返回结果。然后，模型被设置回训练模式，使用 model.train() 。</p>
<h2 id="建立一个基本的神经网络模型"><a href="#建立一个基本的神经网络模型" class="headerlink" title="建立一个基本的神经网络模型"></a>建立一个基本的神经网络模型</h2><p>We’re building a basic neural network that we’ll improve later using LLaMA techniques.<br>我们正在构建一个基本的神经网络，稍后我们将使用LLaMA技术进行改进。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># Definition of a basic neural network class</span><br><span class="line">class SimpleBrokenModel(nn.Module):</span><br><span class="line">    def __init__(self, config=MASTER_CONFIG):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">        # Embedding layer to convert character indices to vectors (vocab size: 65)</span><br><span class="line">        self.embedding = nn.Embedding(config[&#x27;vocab_size&#x27;], config[&#x27;d_model&#x27;])</span><br><span class="line"></span><br><span class="line">        # Linear layers for modeling relationships between features</span><br><span class="line">        # (to be updated with SwiGLU activation function as in LLaMA)</span><br><span class="line">        self.linear = nn.Sequential(</span><br><span class="line">            nn.Linear(config[&#x27;d_model&#x27;], config[&#x27;d_model&#x27;]),</span><br><span class="line">            nn.ReLU(),  # Currently using ReLU, will be replaced with SwiGLU as in LLaMA</span><br><span class="line">            nn.Linear(config[&#x27;d_model&#x27;], config[&#x27;vocab_size&#x27;]),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        # Print the total number of model parameters</span><br><span class="line">        print(&quot;Model parameters:&quot;, sum([m.numel() for m in self.parameters()]))</span><br></pre></td></tr></table></figure>

<p>In the current architecture, the embedding layer has a vocabulary size of 65, representing the characters in our dataset. As this serves as our base model, we are using ReLU as the activation function in the linear layers; however, this will later be replaced with SwiGLU, as used in LLaMA.<br>在当前的架构中，嵌入层的词汇量为65，代表我们数据集中的字符。由于这是我们的基础模型，我们在线性层中使用ReLU作为激活函数；然而，这将在后面被替换为SwiGLU，就像LLaMA中使用的那样。</p>
<p>To create a forward pass for our base model, we must define a forward function within our NN model.<br>为了为我们的基础模型创建一个前向传递，我们必须在我们的神经网络模型中定义一个前向函数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># Definition of a basic neural network class</span><br><span class="line">class SimpleBrokenModel(nn.Module):</span><br><span class="line">    def __init__(self, config=MASTER_CONFIG):</span><br><span class="line"></span><br><span class="line">        # Rest of the code        </span><br><span class="line">        ... </span><br><span class="line"></span><br><span class="line">        # Forward pass function for the base model</span><br><span class="line">        def forward(self, idx, targets=None):</span><br><span class="line">            # Embedding layer converts character indices to vectors</span><br><span class="line">            x = self.embedding(idx)</span><br><span class="line">            </span><br><span class="line">            # Linear layers for modeling relationships between features</span><br><span class="line">            a = self.linear(x)</span><br><span class="line">            </span><br><span class="line">            # Apply softmax activation to obtain probability distribution</span><br><span class="line">            logits = F.softmax(a, dim=-1)</span><br><span class="line"></span><br><span class="line">            # If targets are provided, calculate and return the cross-entropy loss</span><br><span class="line">            if targets is not None:</span><br><span class="line">                # Reshape logits and targets for cross-entropy calculation</span><br><span class="line">                loss = F.cross_entropy(logits.view(-1, self.config[&#x27;vocab_size&#x27;]), targets.view(-1))</span><br><span class="line">                return logits, loss</span><br><span class="line"></span><br><span class="line">            # If targets are not provided, return the logits</span><br><span class="line">            else:</span><br><span class="line">                return logits</span><br><span class="line"></span><br><span class="line">        # Print the total number of model parameters</span><br><span class="line">        print(&quot;Model parameters:&quot;, sum([m.numel() for m in self.parameters()]))</span><br></pre></td></tr></table></figure>

<p>This forward pass function takes character indices (idx) as input, applies the embedding layer, passes the result through linear layers, applies a softmax activation to obtain a probability distribution (logits). If targets are provided, it calculates the cross-entropy loss and returns both logits and loss. If targets are not provided, it returns only the logits.<br>这个前向传播函数以字符索引（ idx ）作为输入，应用嵌入层，通过线性层传递结果，应用softmax激活函数以获得概率分布（ logits ）。如果提供了目标值，它计算交叉熵损失并返回logits和损失。如果没有提供目标值，它只返回logits。</p>
<p>To instantiate this model, we can directly invoke the class and print the total number of parameters in our Simple Neural Network Model. We’ve set the dimension of our linear layers to 128, specifying this value in our config object:<br>要实例化这个模型，我们可以直接调用类并打印出简单神经网络模型中的参数总数。我们将线性层的维度设置为128，在我们的配置对象中指定了这个值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># Update MASTER_CONFIG with the dimension of linear layers (128)</span><br><span class="line">MASTER_CONFIG.update(&#123;</span><br><span class="line">    &#x27;d_model&#x27;: 128,</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"># Instantiate the SimpleBrokenModel using the updated MASTER_CONFIG</span><br><span class="line">model = SimpleBrokenModel(MASTER_CONFIG)</span><br><span class="line"></span><br><span class="line"># Print the total number of parameters in the model</span><br><span class="line">print(&quot;Total number of parameters in the Simple Neural Network Model:&quot;, sum([m.numel() for m in model.parameters()]))</span><br></pre></td></tr></table></figure>
<p><img src="/../asset_makemillionllm/08.webp"></p>
<p>Our Simple Neural Network Model comprises approximately 33,000 parameters.<br>我们的简单神经网络模型包含大约33,000个参数。</p>
<p>Similarly, to compute logits and loss, we only need to feed our split dataset into our model:<br>同样，为了计算logits和损失，我们只需要将我们的分割数据集输入到我们的模型中：</p>
<pre><code># Obtain batches for training using the specified batch size and context window
xs, ys = get_batches(dataset, &#39;train&#39;, MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;])

# Calculate logits and loss using the model
logits, loss = model(xs, ys)
</code></pre>
<p>To train our base model and note its performance, we need to specify some parameters. We are training for a total of 1000 epochs. Increasing the batch size to 32 from 8, and set the log_interval to 10, indicating that the code will print or log information about the training progress every 10 batches. For optimization, we’ll use the Adam optimizer.<br>为了训练我们的基础模型并记录其性能，我们需要指定一些参数。我们总共训练1000个epochs。将批量大小从8增加到32，并将log_interval设置为10，表示代码将每10个批次打印或记录有关训练进度的信息。对于优化，我们将使用Adam优化器。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># Update MASTER_CONFIG with training parameters</span><br><span class="line">MASTER_CONFIG.update(&#123;</span><br><span class="line">    &#x27;epochs&#x27;: 1000,          # Number of training epochs</span><br><span class="line">    &#x27;log_interval&#x27;: 10,      # Log information every 10 batches during training</span><br><span class="line">    &#x27;batch_size&#x27;: 32,        # Increase batch size to 32</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"># Instantiate the SimpleBrokenModel with updated configuration</span><br><span class="line">model = SimpleBrokenModel(MASTER_CONFIG)</span><br><span class="line"></span><br><span class="line"># Define the Adam optimizer for model parameters</span><br><span class="line">optimizer = torch.optim.Adam(</span><br><span class="line">    model.parameters(),      # Pass the model parameters to the optimizer</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>Let’s execute the training process and capture the loss from our base model, including the total number of parameters. Additionally, each line is commented for clarity:<br>让我们执行训练过程并捕获基本模型的损失，包括参数的总数。此外，每一行都有注释以便清晰理解。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"># Function to perform training</span><br><span class="line">def train(model, optimizer, scheduler=None, config=MASTER_CONFIG, print_logs=False):</span><br><span class="line">    # Placeholder for storing losses</span><br><span class="line">    losses = []</span><br><span class="line">    </span><br><span class="line">    # Start tracking time</span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    # Iterate through epochs</span><br><span class="line">    for epoch in range(config[&#x27;epochs&#x27;]):</span><br><span class="line">        # Zero out gradients</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        # Obtain batches for training</span><br><span class="line">        xs, ys = get_batches(dataset, &#x27;train&#x27;, config[&#x27;batch_size&#x27;], config[&#x27;context_window&#x27;])</span><br><span class="line"></span><br><span class="line">        # Forward pass through the model to calculate logits and loss</span><br><span class="line">        logits, loss = model(xs, targets=ys)</span><br><span class="line"></span><br><span class="line">        # Backward pass and optimization step</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        # If a learning rate scheduler is provided, adjust the learning rate</span><br><span class="line">        if scheduler:</span><br><span class="line">            scheduler.step()</span><br><span class="line"></span><br><span class="line">        # Log progress every specified interval</span><br><span class="line">        if epoch % config[&#x27;log_interval&#x27;] == 0:</span><br><span class="line">            # Calculate batch time</span><br><span class="line">            batch_time = time.time() - start_time</span><br><span class="line">            </span><br><span class="line">            # Evaluate loss on validation set</span><br><span class="line">            x = evaluate_loss(model)</span><br><span class="line">            </span><br><span class="line">            # Store the validation loss</span><br><span class="line">            losses += [x]</span><br><span class="line">            </span><br><span class="line">            # Print progress logs if specified</span><br><span class="line">            if print_logs:</span><br><span class="line">                print(f&quot;Epoch &#123;epoch&#125; | val loss &#123;x[&#x27;val&#x27;]:.3f&#125; | Time &#123;batch_time:.3f&#125; | ETA in seconds &#123;batch_time * (config[&#x27;epochs&#x27;] - epoch)/config[&#x27;log_interval&#x27;] :.3f&#125;&quot;)</span><br><span class="line">                </span><br><span class="line">            # Reset the timer</span><br><span class="line">            start_time = time.time()</span><br><span class="line"></span><br><span class="line">            # Print learning rate if a scheduler is provided</span><br><span class="line">            if scheduler:</span><br><span class="line">                print(&quot;lr: &quot;, scheduler.get_lr())</span><br><span class="line"></span><br><span class="line">    # Print the final validation loss</span><br><span class="line">    print(&quot;Validation loss: &quot;, losses[-1][&#x27;val&#x27;])</span><br><span class="line">    </span><br><span class="line">    # Plot the training and validation loss curves</span><br><span class="line">    return pd.DataFrame(losses).plot()</span><br><span class="line"></span><br><span class="line"># Execute the training process</span><br><span class="line">train(model, optimizer)</span><br></pre></td></tr></table></figure>

<p><img src="/../asset_makemillionllm/09.webp"></p>
<p>The initial cross-entropy loss before training stands at 4.17, and after 1000 epochs, it reduces to 3.93. In this context, cross-entropy reflects the likelihood of selecting the incorrect word.<br>训练前的初始交叉熵损失为4.17，经过1000个时期后，降至3.93。在这个背景下，交叉熵反映了选择错误单词的可能性。</p>
<p>Our model incorporates a softmax layer on the logits, which transforms a vector of numbers into a probability distribution. Let’s use the built-in F.cross_entropy function, we need to directly pass in the unnormalized logits. Consequently, we will modify our model accordingly.<br>我们的模型在logits上加入了一个softmax层，将一个数字向量转化为概率分布。让我们使用内置的F.cross_entropy函数，我们需要直接传入未归一化的logits。因此，我们将相应地修改我们的模型。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># Modified SimpleModel class without softmax layer</span><br><span class="line">class SimpleModel(nn.Module):</span><br><span class="line">    def __init__(self, config):</span><br><span class="line">       </span><br><span class="line">       # Rest of the code</span><br><span class="line">       ...</span><br><span class="line"></span><br><span class="line">    def forward(self, idx, targets=None):</span><br><span class="line">        # Embedding layer converts character indices to vectors</span><br><span class="line">        x = self.embedding(idx)</span><br><span class="line">        </span><br><span class="line">        # Linear layers for modeling relationships between features</span><br><span class="line">        logits = self.linear(x)</span><br><span class="line"></span><br><span class="line">        # If targets are provided, calculate and return the cross-entropy loss</span><br><span class="line">        if targets is not None:</span><br><span class="line"></span><br><span class="line">            # Rest of the code</span><br><span class="line">            ...</span><br></pre></td></tr></table></figure>

<p>Let’s recreate the updated SimpleModel and train it for 1000 epochs to observe any changes:<br>让我们重新创建更新的SimpleModel，并进行1000个周期的训练，以观察是否有任何变化</p>
<pre><code># Create the updated SimpleModel
model = SimpleModel(MASTER_CONFIG)

# Obtain batches for training
xs, ys = get_batches(dataset, &#39;train&#39;, MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;])

# Calculate logits and loss using the model
logits, loss = model(xs, ys)

# Define the Adam optimizer for model parameters
optimizer = torch.optim.Adam(model.parameters())

# Train the model for 100 epochs
train(model, optimizer)
</code></pre>
<p><img src="/../asset_makemillionllm/10.webp"></p>
<p>After reducing the loss to 2.51, let’s explore how our language model with approximately 33,000 parameters generates text during inferencing. We’ll create a ‘generate’ function, which we’ll later use when replicating LLaMA:<br>将损失降低到2.51后，让我们探索一下我们的语言模型在推理过程中如何生成文本。我们将创建一个“generate”函数，在复制LLaMA时将使用该函数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Generate function for text generation using the trained model</span><br><span class="line">def generate(model, config=MASTER_CONFIG, max_new_tokens=30):</span><br><span class="line">    idx = torch.zeros(5, 1).long()</span><br><span class="line">    for _ in range(max_new_tokens):</span><br><span class="line">        # Call the model</span><br><span class="line">        logits = model(idx[:, -config[&#x27;context_window&#x27;]:])</span><br><span class="line">        last_time_step_logits = logits[</span><br><span class="line">            :, -1, :</span><br><span class="line">        ]  # all the batches (1), last time step, all the logits</span><br><span class="line">        p = F.softmax(last_time_step_logits, dim=-1)  # softmax to get probabilities</span><br><span class="line">        idx_next = torch.multinomial(</span><br><span class="line">            p, num_samples=1</span><br><span class="line">        )  # sample from the distribution to get the next token</span><br><span class="line">        idx = torch.cat([idx, idx_next], dim=-1)  # append to the sequence</span><br><span class="line">    return [decode(x) for x in idx.tolist()]</span><br><span class="line"></span><br><span class="line"># Generate text using the trained model</span><br><span class="line">generate(model)</span><br></pre></td></tr></table></figure>

<p><img src="/../asset_makemillionllm/11.webp"></p>
<p>The generated text doesn’t look great with our basic model of around 33K parameters. However, now that we’ve laid the groundwork with this simple model, we’ll move on to constructing the LLaMA architecture in the next section.<br>生成的文本在我们的基本模型中（大约33K个参数）看起来不太好。然而，既然我们已经用这个简单模型打下了基础，我们将在下一节中构建LLaMA架构。</p>
<h2 id="复制LLaMA架构"><a href="#复制LLaMA架构" class="headerlink" title="复制LLaMA架构"></a>复制LLaMA架构</h2><p>In the earlier part of the blog, we covered essential concepts, and now, we’ll integrate these concepts into our base model. LLaMA introduces three architectural modifications to the original Transformer:<br>在博客的前半部分，我们介绍了基本概念，现在我们将把这些概念融入到我们的基础模型中。LLaMA对原始Transformer进行了三个架构修改。</p>
<ul>
<li>RMSNorm用于预归一化</li>
<li>旋转嵌入</li>
<li>SwiGLU激活函数</li>
</ul>
<p>我们将逐个将这些修改合并到我们的基础模型中，进行迭代和构建。</p>
<h2 id="RMSNorm用于预归一化："><a href="#RMSNorm用于预归一化：" class="headerlink" title="RMSNorm用于预归一化："></a>RMSNorm用于预归一化：</h2><p>We are defining an RMSNorm function with the following functionalities:<br>我们正在定义一个具有以下功能的RMSNorm函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">class RMSNorm(nn.Module):</span><br><span class="line">    def __init__(self, layer_shape, eps=1e-8, bias=False):</span><br><span class="line">        super(RMSNorm, self).__init__()</span><br><span class="line"></span><br><span class="line">        # Registering a learnable parameter &#x27;scale&#x27; as a parameter of the module</span><br><span class="line">        self.register_parameter(&quot;scale&quot;, nn.Parameter(torch.ones(layer_shape)))</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Assumes shape is (batch, seq_len, d_model)</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # Calculating the Frobenius norm, RMS = 1/sqrt(N) * Frobenius norm</span><br><span class="line">        ff_rms = torch.linalg.norm(x, dim=(1,2)) * x[0].numel() ** -.5</span><br><span class="line"></span><br><span class="line">        # Normalizing the input tensor &#x27;x&#x27; with respect to RMS</span><br><span class="line">        raw = x / ff_rms.unsqueeze(-1).unsqueeze(-1)</span><br><span class="line"></span><br><span class="line">        # Scaling the normalized tensor using the learnable parameter &#x27;scale&#x27;</span><br><span class="line">        return self.scale[:x.shape[1], :].unsqueeze(0) * raw</span><br></pre></td></tr></table></figure>

<p>we define the RMSNorm class. During initialization, it registers a scale parameter. In the forward pass, it calculates the Frobenius norm of the input tensor and then normalizes the tensor. Finally, the tensor is scaled by the registered scale parameter. This function is designed for use in LLaMA to replace the LayerNorm operation.<br>我们定义了RMSNorm类。在初始化过程中，它注册了一个比例参数。在前向传播过程中，它计算输入张量的Frobenius范数，然后对张量进行归一化。最后，张量被注册的比例参数进行缩放。这个函数被设计用于在LLaMA中替代LayerNorm操作。</p>
<p>Now it’s time to incorporate the first implementation concept of LLaMA, which is RMSNorm, into our simple NN model. Here’s the updated code:<br>现在是将LLaMA的第一个实现概念RMSNorm融入我们简单的神经网络模型的时候了。以下是更新后的代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"># Define the SimpleModel_RMS with RMSNorm</span><br><span class="line">class SimpleModel_RMS(nn.Module):</span><br><span class="line">    def __init__(self, config):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">        # Embedding layer to convert character indices to vectors</span><br><span class="line">        self.embedding = nn.Embedding(config[&#x27;vocab_size&#x27;], config[&#x27;d_model&#x27;])</span><br><span class="line"></span><br><span class="line">        # RMSNorm layer for pre-normalization</span><br><span class="line">        self.rms = RMSNorm((config[&#x27;context_window&#x27;], config[&#x27;d_model&#x27;]))</span><br><span class="line"></span><br><span class="line">        # Linear layers for modeling relationships between features</span><br><span class="line">        self.linear = nn.Sequential(</span><br><span class="line">            # Rest of the code</span><br><span class="line">            ...</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        # Print the total number of model parameters</span><br><span class="line">        print(&quot;Model parameters:&quot;, sum([m.numel() for m in self.parameters()]))</span><br><span class="line"></span><br><span class="line">    def forward(self, idx, targets=None):</span><br><span class="line">        # Embedding layer converts character indices to vectors</span><br><span class="line">        x = self.embedding(idx)</span><br><span class="line"></span><br><span class="line">        # RMSNorm pre-normalization</span><br><span class="line">        x = self.rms(x)</span><br><span class="line"></span><br><span class="line">        # Linear layers for modeling relationships between features</span><br><span class="line">        logits = self.linear(x)</span><br><span class="line"></span><br><span class="line">        if targets is not None:</span><br><span class="line"></span><br><span class="line">            # Rest of the code</span><br><span class="line">            ...</span><br></pre></td></tr></table></figure>

<p>Let’s execute the modified NN model with RMSNorm and observe the updated number of parameters in the model, along with the loss:<br>让我们使用带有RMSNorm的修改后的NN模型，并观察模型中更新的参数数量以及损失</p>
<pre><code># Create an instance of SimpleModel_RMS
model = SimpleModel_RMS(MASTER_CONFIG)

# Obtain batches for training
xs, ys = get_batches(dataset, &#39;train&#39;, MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;])

# Calculate logits and loss using the model
logits, loss = model(xs, ys)

# Define the Adam optimizer for model parameters
optimizer = torch.optim.Adam(model.parameters())

# Train the model
train(model, optimizer)
</code></pre>
<p><img src="/../asset_makemillionllm/12.webp"></p>
<p>The validation loss experiences a small decrease, and the parameters of our updated LLM now total approximately 55,000.<br>验证损失经历了小幅下降，我们更新的LLM的参数总数约为55,000。</p>
<h2 id="旋转嵌入："><a href="#旋转嵌入：" class="headerlink" title="旋转嵌入："></a>旋转嵌入：</h2><p>Next, we will implement rotary positional embeddings. In RoPE, the authors suggest embedding the position of a token in a sequence by rotating the embedding, applying a different rotation at each position. Let’s create a function that mimics the actual paper implementation of RoPE:<br>接下来，我们将实现旋转位置嵌入。在RoPE中，作者建议通过旋转嵌入来嵌入序列中的令牌位置，每个位置应用不同的旋转。让我们创建一个模拟RoPE实际论文实现的函数。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def get_rotary_matrix(context_window, embedding_dim):</span><br><span class="line">    # Initialize a tensor for the rotary matrix with zeros</span><br><span class="line">    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)</span><br><span class="line">    </span><br><span class="line">    # Loop through each position in the context window</span><br><span class="line">    for position in range(context_window):</span><br><span class="line">        # Loop through each dimension in the embedding</span><br><span class="line">        for i in range(embedding_dim // 2):</span><br><span class="line">            # Calculate the rotation angle (theta) based on the position and embedding dimension</span><br><span class="line">            theta = 10000. ** (-2. * (i - 1) / embedding_dim)</span><br><span class="line">            # Calculate the rotated matrix elements using sine and cosine functions</span><br><span class="line">            m_theta = position * theta</span><br><span class="line">            R[position, 2 * i, 2 * i] = np.cos(m_theta)</span><br><span class="line">            R[position, 2 * i, 2 * i + 1] = -np.sin(m_theta)</span><br><span class="line">            R[position, 2 * i + 1, 2 * i] = np.sin(m_theta)</span><br><span class="line">            R[position, 2 * i + 1, 2 * i + 1] = np.cos(m_theta)</span><br><span class="line">    return R</span><br></pre></td></tr></table></figure>

<p>we generate a rotary matrix based on the specified context window and embedding dimension, following the proposed RoPE implementation.<br>我们根据指定的上下文窗口和嵌入维度生成一个旋转矩阵，遵循提出的RoPE实现。</p>
<p>As you may be familiar with the architecture of transformers, which involves attention heads, we similarly need to create attention heads when replicating LLaMA. To start, let’s first create a single masked attention head using the get_rotary_matrix function we previously developed for rotary embeddings. Additionally, each line is commented for clarity:<br>正如您可能熟悉的transformer架构，其中涉及到注意力头，我们在复制LLaMA时同样需要创建注意力头。首先，让我们使用之前为旋转嵌入开发的 get_rotary_matrix 函数来创建一个单独的掩码注意力头。此外，为了清晰起见，每行都有注释。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">class RoPEAttentionHead(nn.Module):</span><br><span class="line">    def __init__(self, config):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.config = config</span><br><span class="line">        # Linear transformation for query</span><br><span class="line">        self.w_q = nn.Linear(config[&#x27;d_model&#x27;], config[&#x27;d_model&#x27;], bias=False)</span><br><span class="line">        # Linear transformation for key</span><br><span class="line">        self.w_k = nn.Linear(config[&#x27;d_model&#x27;], config[&#x27;d_model&#x27;], bias=False)</span><br><span class="line">        # Linear transformation for value</span><br><span class="line">        self.w_v = nn.Linear(config[&#x27;d_model&#x27;], config[&#x27;d_model&#x27;], bias=False)</span><br><span class="line">        # Obtain rotary matrix for positional embeddings</span><br><span class="line">        self.R = get_rotary_matrix(config[&#x27;context_window&#x27;], config[&#x27;d_model&#x27;])</span><br><span class="line"></span><br><span class="line">    def get_rotary_matrix(context_window, embedding_dim):</span><br><span class="line">        # Generate rotational matrix for RoPE</span><br><span class="line">        R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)</span><br><span class="line">        for position in range(context_window):</span><br><span class="line">            for i in range(embedding_dim//2):</span><br><span class="line">                </span><br><span class="line">                # Rest of the code</span><br><span class="line">                ...</span><br><span class="line"></span><br><span class="line">        return R</span><br><span class="line"></span><br><span class="line">    def forward(self, x, return_attn_weights=False):</span><br><span class="line">        # x: input tensor of shape (batch, sequence length, dimension)</span><br><span class="line"></span><br><span class="line">        b, m, d = x.shape  # batch size, sequence length, dimension</span><br><span class="line"></span><br><span class="line">        # Linear transformations for Q, K, and V</span><br><span class="line">        q = self.w_q(x)</span><br><span class="line">        k = self.w_k(x)</span><br><span class="line">        v = self.w_v(x)</span><br><span class="line"></span><br><span class="line">        # Rotate Q and K using the RoPE matrix</span><br><span class="line">        q_rotated = (torch.bmm(q.transpose(0, 1), self.R[:m])).transpose(0, 1)</span><br><span class="line">        k_rotated = (torch.bmm(k.transpose(0, 1), self.R[:m])).transpose(0, 1)</span><br><span class="line"></span><br><span class="line">        # Perform scaled dot-product attention</span><br><span class="line">        activations = F.scaled_dot_product_attention(</span><br><span class="line">            q_rotated, k_rotated, v, dropout_p=0.1, is_causal=True</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        if return_attn_weights:</span><br><span class="line">            # Create a causal attention mask</span><br><span class="line">            attn_mask = torch.tril(torch.ones((m, m)), diagonal=0)</span><br><span class="line">            # Calculate attention weights and add causal mask</span><br><span class="line">            attn_weights = torch.bmm(q_rotated, k_rotated.transpose(1, 2)) / np.sqrt(d) + attn_mask</span><br><span class="line">            attn_weights = F.softmax(attn_weights, dim=-1)</span><br><span class="line">            return activations, attn_weights</span><br><span class="line"></span><br><span class="line">        return activations</span><br></pre></td></tr></table></figure>

<p>Now that we have a single masked attention head that returns attention weights, the next step is to create a multi-Head attention mechanism.<br>现在我们有一个返回注意力权重的单个掩码注意力头，下一步是创建一个多头注意力机制。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">class RoPEMaskedMultiheadAttention(nn.Module):</span><br><span class="line">    def __init__(self, config):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.config = config</span><br><span class="line">        # Create a list of RoPEMaskedAttentionHead instances as attention heads</span><br><span class="line">        self.heads = nn.ModuleList([</span><br><span class="line">            RoPEMaskedAttentionHead(config) for _ in range(config[&#x27;n_heads&#x27;])</span><br><span class="line">        ])</span><br><span class="line">        self.linear = nn.Linear(config[&#x27;n_heads&#x27;] * config[&#x27;d_model&#x27;], config[&#x27;d_model&#x27;])  # Linear layer after concatenating heads</span><br><span class="line">        self.dropout = nn.Dropout(.1)  # Dropout layer</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # x: input tensor of shape (batch, sequence length, dimension)</span><br><span class="line"></span><br><span class="line">        # Process each attention head and concatenate the results</span><br><span class="line">        heads = [h(x) for h in self.heads]</span><br><span class="line">        x = torch.cat(heads, dim=-1)</span><br><span class="line">        </span><br><span class="line">        # Apply linear transformation to the concatenated output</span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        </span><br><span class="line">        # Apply dropout</span><br><span class="line">        x = self.dropout(x)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>

<p>The original paper used 32 heads for their smaller 7b LLM variation, but due to constraints, we’ll use 8 heads for our approach.<br>原始论文在其较小的7b LLM 变体中使用了32个头，但由于限制，我们的方法将使用8个头。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Update the master configuration with the number of attention heads</span><br><span class="line">MASTER_CONFIG.update(&#123;</span><br><span class="line">    &#x27;n_heads&#x27;: 8,</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>Now that we’ve implemented Rotational Embedding and Multi-head Attention, let’s re-write our RMSNorm neural network model with the updated code. We’ll test its performance, compute the loss, and check the number of parameters. We’ll refer to this updated model as “RopeModel”<br>现在我们已经实施了旋转嵌入和多头注意力，让我们用更新后的代码重新编写我们的RMSNorm神经网络模型。我们将测试其性能，计算损失，并检查参数的数量。我们将称这个更新的模型为“RopeModel”。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">class RopeModel(nn.Module):</span><br><span class="line">    def __init__(self, config):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">        # Embedding layer for input tokens</span><br><span class="line">        self.embedding = nn.Embedding(config[&#x27;vocab_size&#x27;], config[&#x27;d_model&#x27;])</span><br><span class="line">        </span><br><span class="line">        # RMSNorm layer for pre-normalization</span><br><span class="line">        self.rms = RMSNorm((config[&#x27;context_window&#x27;], config[&#x27;d_model&#x27;]))</span><br><span class="line">        </span><br><span class="line">        # RoPEMaskedMultiheadAttention layer</span><br><span class="line">        self.rope_attention = RoPEMaskedMultiheadAttention(config)</span><br><span class="line"></span><br><span class="line">        # Linear layer followed by ReLU activation</span><br><span class="line">        self.linear = nn.Sequential(</span><br><span class="line">            nn.Linear(config[&#x27;d_model&#x27;], config[&#x27;d_model&#x27;]),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        # Final linear layer for prediction</span><br><span class="line">        self.last_linear = nn.Linear(config[&#x27;d_model&#x27;], config[&#x27;vocab_size&#x27;])</span><br><span class="line"></span><br><span class="line">        print(&quot;model params:&quot;, sum([m.numel() for m in self.parameters()]))</span><br><span class="line"></span><br><span class="line">    def forward(self, idx, targets=None):</span><br><span class="line">        # idx: input indices</span><br><span class="line">        x = self.embedding(idx)</span><br><span class="line"></span><br><span class="line">        # One block of attention</span><br><span class="line">        x = self.rms(x)  # RMS pre-normalization</span><br><span class="line">        x = x + self.rope_attention(x)</span><br><span class="line"></span><br><span class="line">        x = self.rms(x)  # RMS pre-normalization</span><br><span class="line">        x = x + self.linear(x)</span><br><span class="line"></span><br><span class="line">        logits = self.last_linear(x)</span><br><span class="line"></span><br><span class="line">        if targets is not None:</span><br><span class="line">            loss = F.cross_entropy(logits.view(-1, self.config[&#x27;vocab_size&#x27;]), targets.view(-1))</span><br><span class="line">            return logits, loss</span><br><span class="line"></span><br><span class="line">        else:</span><br><span class="line">            return logits</span><br></pre></td></tr></table></figure>

<p>Let’s execute the modified NN model with RMSNorm, Rotational Embeddings and Masked Multi Head Attentions to observe the updated number of parameters in the model, along with the loss:<br>让我们使用带有RMSNorm、旋转嵌入和掩码多头注意力的修改后的NN模型来执行，观察模型中更新后的参数数量以及损失</p>
<pre><code># Create an instance of RopeModel (RMSNorm, RoPE, Multi-Head)
model = RopeModel(MASTER_CONFIG)

# Obtain batches for training
xs, ys = get_batches(dataset, &#39;train&#39;, MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;])

# Calculate logits and loss using the model
logits, loss = model(xs, ys)

# Define the Adam optimizer for model parameters
optimizer = torch.optim.Adam(model.parameters())

# Train the model
train(model, optimizer)
</code></pre>
<p><img src="/../asset_makemillionllm/13.webp"></p>
<p>The validation loss experiences a small decrease again, and the parameters of our updated LLM now total approximately 55,000.<br>验证损失再次略微下降，我们更新的LLM的参数总数约为55,000。</p>
<p>Let’s train the model for more epochs to see if the loss of our recreated LLaMA LLM continues to decrease or not.<br>让我们训练模型更多个周期，看看我们重新创建的 LLaMA LLM 的损失是否继续减少。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Updating training configuration with more epochs and a logging interval</span><br><span class="line">MASTER_CONFIG.update(&#123;</span><br><span class="line">    &quot;epochs&quot;: 5000,</span><br><span class="line">    &quot;log_interval&quot;: 10,</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"># Training the model with the updated configuration</span><br><span class="line">train(model, optimizer)</span><br></pre></td></tr></table></figure>
<p><img src="/../asset_makemillionllm/14.webp"></p>
<p>The validation loss continues to decrease, suggesting that training for more epochs could lead to further loss reduction, though not significantly.<br>验证损失继续下降，表明训练更多轮次可能会进一步减少损失，尽管不显著。</p>
<h2 id="SwiGLU激活函数-1"><a href="#SwiGLU激活函数-1" class="headerlink" title="SwiGLU激活函数"></a>SwiGLU激活函数</h2><p>如前所述，LLaMA的创建者使用SwiGLU而不是ReLU，因此我们将在我们的代码中实现SwiGLU方程。</p>
<p><img src="/../asset_makemillionllm/15.webp"><br><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2002.05202v1.pdf">https://arxiv.org/pdf/2002.05202v1.pdf</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class SwiGLU(nn.Module):</span><br><span class="line">    &quot;&quot;&quot; Paper Link -&gt; https://arxiv.org/pdf/2002.05202v1.pdf &quot;&quot;&quot;</span><br><span class="line">    def __init__(self, size):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.config = config  # Configuration information</span><br><span class="line">        self.linear_gate = nn.Linear(size, size)  # Linear transformation for the gating mechanism</span><br><span class="line">        self.linear = nn.Linear(size, size)  # Linear transformation for the main branch</span><br><span class="line">        self.beta = torch.randn(1, requires_grad=True)  # Random initialization of the beta parameter</span><br><span class="line"></span><br><span class="line">        # Using nn.Parameter for beta to ensure it&#x27;s recognized as a learnable parameter</span><br><span class="line">        self.beta = nn.Parameter(torch.ones(1))</span><br><span class="line">        self.register_parameter(&quot;beta&quot;, self.beta)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # Swish-Gated Linear Unit computation</span><br><span class="line">        swish_gate = self.linear_gate(x) * torch.sigmoid(self.beta * self.linear_gate(x))</span><br><span class="line">        out = swish_gate * self.linear(x)  # Element-wise multiplication of the gate and main branch</span><br><span class="line">        return out</span><br></pre></td></tr></table></figure>

<p>在Python中实现SwiGLU方程后，我们需要将其集成到我们修改过的LLaMA语言模型（RopeModel）中。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">class RopeModel(nn.Module):</span><br><span class="line">    def __init__(self, config):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">        # Embedding layer for input tokens</span><br><span class="line">        self.embedding = nn.Embedding(config[&#x27;vocab_size&#x27;], config[&#x27;d_model&#x27;])</span><br><span class="line">        </span><br><span class="line">        # RMSNorm layer for pre-normalization</span><br><span class="line">        self.rms = RMSNorm((config[&#x27;context_window&#x27;], config[&#x27;d_model&#x27;]))</span><br><span class="line">        </span><br><span class="line">        # Multi-head attention layer with RoPE (Rotary Positional Embeddings)</span><br><span class="line">        self.rope_attention = RoPEMaskedMultiheadAttention(config)</span><br><span class="line"></span><br><span class="line">        # Linear layer followed by SwiGLU activation</span><br><span class="line">        self.linear = nn.Sequential(</span><br><span class="line">            nn.Linear(config[&#x27;d_model&#x27;], config[&#x27;d_model&#x27;]),</span><br><span class="line">            SwiGLU(config[&#x27;d_model&#x27;]),  # Adding SwiGLU activation</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        # Output linear layer</span><br><span class="line">        self.last_linear = nn.Linear(config[&#x27;d_model&#x27;], config[&#x27;vocab_size&#x27;])</span><br><span class="line"></span><br><span class="line">        # Printing total model parameters</span><br><span class="line">        print(&quot;model params:&quot;, sum([m.numel() for m in self.parameters()]))</span><br><span class="line"></span><br><span class="line">    def forward(self, idx, targets=None):</span><br><span class="line">        x = self.embedding(idx)</span><br><span class="line"></span><br><span class="line">        # One block of attention</span><br><span class="line">        x = self.rms(x)  # RMS pre-normalization</span><br><span class="line">        x = x + self.rope_attention(x)</span><br><span class="line"></span><br><span class="line">        x = self.rms(x)  # RMS pre-normalization</span><br><span class="line">        x = x + self.linear(x)  # Applying SwiGLU activation</span><br><span class="line"></span><br><span class="line">        logits = self.last_linear(x)</span><br><span class="line"></span><br><span class="line">        if targets is not None:</span><br><span class="line">            # Calculate cross-entropy loss if targets are provided</span><br><span class="line">            loss = F.cross_entropy(logits.view(-1, self.config[&#x27;vocab_size&#x27;]), targets.view(-1))</span><br><span class="line">            return logits, loss</span><br><span class="line"></span><br><span class="line">        else:</span><br><span class="line">            return logits</span><br></pre></td></tr></table></figure>

<p>让我们使用RMSNorm、旋转嵌入、掩码多头注意力和SwiGLU来执行修改后的NN模型，观察模型中更新的参数数量以及损失</p>
<pre><code># Create an instance of RopeModel (RMSNorm, RoPE, Multi-Head, SwiGLU)
model = RopeModel(MASTER_CONFIG)

# Obtain batches for training
xs, ys = get_batches(dataset, &#39;train&#39;, MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;])

# Calculate logits and loss using the model
logits, loss = model(xs, ys)

# Define the Adam optimizer for model parameters
optimizer = torch.optim.Adam(model.parameters())

# Train the model
train(model, optimizer)
</code></pre>
<p><img src="/../asset_makemillionllm/16.webp"></p>
<p>Once again the validation loss experiences a small decrease, and the parameters of our updated LLM now total approximately 60,000.<br>验证损失再次小幅下降，我们更新的LLM的参数总数约为60,000。</p>
<p>So far, we have successfully implemented the key components of the paper, namely RMSNorm, RoPE, and SwiGLU. We observed that these implementations led to a minimal decrease in the loss.<br>到目前为止，我们已成功实现了论文的关键组件，即RMSNorm、RoPE和SwiGLU。我们观察到这些实现导致了损失的轻微减少。</p>
<p>Now we will add layers to our LLaMA to examine its impact on the loss. The original paper used 32 layers for the 7b version, but we will use only 4 layers. Let’s adjust our model settings accordingly.<br>现在我们将为我们的LLaMA添加层来检查其对损失的影响。原始论文中使用了32层来进行7b版本的训练，但我们只使用4层。让我们相应地调整我们的模型设置。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Update model configurations for the number of layers</span><br><span class="line">MASTER_CONFIG.update(&#123;</span><br><span class="line">    &#x27;n_layers&#x27;: 4,  # Set the number of layers to 4</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>

<p>Let’s start by creating a single layer to understand its impact.<br>让我们先创建一个单一的图层来了解它的影响。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># add RMSNorm and residual connection</span><br><span class="line">class LlamaBlock(nn.Module):</span><br><span class="line">    def __init__(self, config):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.config = config</span><br><span class="line"></span><br><span class="line">        # RMSNorm layer</span><br><span class="line">        self.rms = RMSNorm((config[&#x27;context_window&#x27;], config[&#x27;d_model&#x27;]))</span><br><span class="line"></span><br><span class="line">        # RoPE Masked Multihead Attention layer</span><br><span class="line">        self.attention = RoPEMaskedMultiheadAttention(config)</span><br><span class="line"></span><br><span class="line">        # Feedforward layer with SwiGLU activation</span><br><span class="line">        self.feedforward = nn.Sequential(</span><br><span class="line">            nn.Linear(config[&#x27;d_model&#x27;], config[&#x27;d_model&#x27;]),</span><br><span class="line">            SwiGLU(config[&#x27;d_model&#x27;]),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        # one block of attention</span><br><span class="line">        x = self.rms(x) # RMS pre-normalization</span><br><span class="line">        x = x + self.attention(x)  # residual connection</span><br><span class="line"></span><br><span class="line">        x = self.rms(x) # RMS pre-normalization</span><br><span class="line">        x = x + self.feedforward(x)  # residual connection</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>

<p>Create an instance of the LlamaBlock class and applies it to a random tensor.<br>创建一个 LlamaBlock 类的实例，并将其应用于一个随机张量。</p>
<pre><code># Create an instance of the LlamaBlock class with the provided configuration
block = LlamaBlock(MASTER_CONFIG)

# Generate a random tensor with the specified batch size, context window, and model dimension
random_input = torch.randn(MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;], MASTER_CONFIG[&#39;d_model&#39;])

# Apply the LlamaBlock to the random input tensor
output = block(random_input)
</code></pre>
<p>Having successfully created a single layer, we can now use it to construct multiple layers. Additionally, we will rename our model class from “ropemodel” to “Llama” as we have replicated every component of the LLaMA language model.<br>成功创建了一个单层后，我们现在可以使用它来构建多个层。此外，我们将把我们的模型类从“ropemodel”重命名为“Llama”，因为我们已经复制了LLaMA语言模型的每个组件。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">class Llama(nn.Module):</span><br><span class="line">    def __init__(self, config):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.config = config</span><br><span class="line">        # Embedding layer for token representations</span><br><span class="line">        self.embeddings = nn.Embedding(config[&#x27;vocab_size&#x27;], config[&#x27;d_model&#x27;])</span><br><span class="line">        # Sequential block of LlamaBlocks based on the specified number of layers</span><br><span class="line">        self.llama_blocks = nn.Sequential(</span><br><span class="line">            OrderedDict([(f&quot;llama_&#123;i&#125;&quot;, LlamaBlock(config)) for i in range(config[&#x27;n_layers&#x27;])])</span><br><span class="line">        )</span><br><span class="line">        # Feedforward network (FFN) for final output</span><br><span class="line">        self.ffn = nn.Sequential(</span><br><span class="line">            nn.Linear(config[&#x27;d_model&#x27;], config[&#x27;d_model&#x27;]),</span><br><span class="line">            SwiGLU(config[&#x27;d_model&#x27;]),</span><br><span class="line">            nn.Linear(config[&#x27;d_model&#x27;], config[&#x27;vocab_size&#x27;]),</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        # Print total number of parameters in the model</span><br><span class="line">        print(&quot;model params:&quot;, sum([m.numel() for m in self.parameters()]))</span><br><span class="line"></span><br><span class="line">    def forward(self, idx, targets=None):</span><br><span class="line">        # Input token indices are passed through the embedding layer</span><br><span class="line">        x = self.embeddings(idx)</span><br><span class="line">        # Process the input through the LlamaBlocks</span><br><span class="line">        x = self.llama_blocks(x)</span><br><span class="line">        # Pass the processed input through the final FFN for output logits</span><br><span class="line">        logits = self.ffn(x)</span><br><span class="line"></span><br><span class="line">        # If targets are not provided, return only the logits</span><br><span class="line">        if targets is None:</span><br><span class="line">            return logits</span><br><span class="line">        # If targets are provided, compute and return the cross-entropy loss</span><br><span class="line">        else:</span><br><span class="line">            loss = F.cross_entropy(logits.view(-1, self.config[&#x27;vocab_size&#x27;]), targets.view(-1))</span><br><span class="line">            return logits, loss</span><br></pre></td></tr></table></figure>

<p>Let’s execute the modified LLaMA model with RMSNorm, Rotational Embeddings, Masked Multi Head Attentions, SwiGLU and N_layers to observe the updated number of parameters in the model, along with the loss:<br>让我们使用RMSNorm、旋转嵌入、掩码多头注意力、SwiGLU和N_layers来执行修改后的LLaMA模型，观察模型中更新的参数数量以及损失</p>
<pre><code># Create an instance of RopeModel (RMSNorm, RoPE, Multi-Head, SwiGLU, N_layers)
llama = Llama(MASTER_CONFIG)

# Obtain batches for training
xs, ys = get_batches(dataset, &#39;train&#39;, MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;])

# Calculate logits and loss using the model
logits, loss = llama(xs, ys)

# Define the Adam optimizer for model parameters
optimizer = torch.optim.Adam(llama.parameters())

# Train the model
train(llama, optimizer)
</code></pre>
<p><img src="/../asset_makemillionllm/17.webp"></p>
<p>While there’s a possibility of overfitting, it’s crucial to explore whether extending the number of epochs leads to a further reduction in loss. Additionally, note that our current LLM has over 2 million parameters.<br>虽然存在过拟合的可能性，但探索增加训练轮数是否能进一步降低损失是至关重要的。此外，请注意我们当前的LLM具有超过200万个参数。</p>
<p>Let’s train it for higher number of epochs.<br>让我们将其训练更多个时期。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># Update the number of epochs in the configuration</span><br><span class="line">MASTER_CONFIG.update(&#123;</span><br><span class="line">    &#x27;epochs&#x27;: 10000,</span><br><span class="line">&#125;)</span><br><span class="line"># Train the LLaMA model for the specified number of epochs</span><br><span class="line">train(llama, optimizer, scheduler=None, config=MASTER_CONFIG)</span><br></pre></td></tr></table></figure>
<p><img src="/../asset_makemillionllm/18.webp"></p>
<p>The loss here is 1.08, we can achieve even more lower loss without encountering significant overfitting. This suggests the model is performing well.<br>这里的损失为1.08，我们可以实现更低的损失而不会遇到显著的过拟合。这表明模型表现良好。</p>
<p>Let’s train the model once more, this time incorporating a scheduler<br>让我们再次训练模型，这次加入一个调度器</p>
<pre><code># Training the model again, scheduler for better optimization.
train(llama, optimizer, config=MASTER_CONFIG)
</code></pre>
<p><img src="/../asset_makemillionllm/19.webp"></p>
<p>Up until now, we’ve successfully implemented a scaled-down version of the LLaMA architecture on our custom dataset. Now, let’s examine the generated output from our 2 million-parameter Language Model.<br>到目前为止，我们已经成功地在我们的自定义数据集上实现了LLaMA架构的一个缩小版本。现在，让我们来检查一下我们200万参数的语言模型生成的输出。</p>
<pre><code># Generate text using the trained LLM (llama) with a maximum of 500 tokens
generated_text = generate(llama, MASTER_CONFIG, 500)[0]
print(generated_text)
</code></pre>
<p><img src="/../asset_makemillionllm/20.webp"></p>
<p>Even though some generated words may not be perfect English, our LLM with just 2 million parameters has shown a basic understanding of the English language.<br>尽管一些生成的词语可能不是完美的英语，但我们的LLM只有200万个参数，已经展示了对英语的基本理解。</p>
<p>现在，让我们看看我们的模型在测试集上的表现如何。</p>
<pre><code># Get batches from the test set
xs, ys = get_batches(dataset, &#39;test&#39;, MASTER_CONFIG[&#39;batch_size&#39;], MASTER_CONFIG[&#39;context_window&#39;])

# Pass the test data through the LLaMA model
logits, loss = llama(xs, ys)

# Print the loss on the test set
print(loss)
</code></pre>
<p>测试集上计算得到的损失约为1.236。</p>
<p>A simple way to check for changes in the generated output is to run training for a large number of epochs and observe the results.<br>检查生成输出的简单方法是运行大量的训练周期并观察结果。</p>
<h2 id="尝试调整超参"><a href="#尝试调整超参" class="headerlink" title="尝试调整超参"></a>尝试调整超参</h2><p>Hyperparameter tuning is a crucial step in training neural networks. In the original Llama paper, the authors utilized the Cosine Annealing learning schedule. However, in our experimentation, it didn’t perform well. Here’s an example of experimenting with hyperparameters using a different learning schedule:<br>超参调优是训练神经网络中的关键步骤。在原始的Llama论文中，作者们使用了余弦退火学习率调度。然而，在我们的实验中，它表现不佳。以下是使用不同学习率调度进行超参实验的示例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># Update configuration</span><br><span class="line">MASTER_CONFIG.update(&#123;</span><br><span class="line">    &quot;epochs&quot;: 1000</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"># Create Llama model with Cosine Annealing learning schedule</span><br><span class="line">llama_with_cosine = Llama(MASTER_CONFIG)</span><br><span class="line"></span><br><span class="line"># Define Adam optimizer with specific hyperparameters</span><br><span class="line">llama_optimizer = torch.optim.Adam(</span><br><span class="line">    llama.parameters(),</span><br><span class="line">    betas=(.9, .95),</span><br><span class="line">    weight_decay=.1,</span><br><span class="line">    eps=1e-9,</span><br><span class="line">    lr=1e-3</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># Define Cosine Annealing learning rate scheduler</span><br><span class="line">scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(llama_optimizer, 300, eta_min=1e-5)</span><br><span class="line"></span><br><span class="line"># Train the Llama model with the specified optimizer and scheduler</span><br><span class="line">train(llama_with_cosine, llama_optimizer, scheduler=scheduler)</span><br></pre></td></tr></table></figure>

<h2 id="保存您的语言模型-LLM"><a href="#保存您的语言模型-LLM" class="headerlink" title="保存您的语言模型 (LLM)"></a>保存您的语言模型 (LLM)</h2><p>您可以使用以下方法保存整个LLM或仅保存参数</p>
<pre><code># Save the entire model
torch.save(llama, &#39;llama_model.pth&#39;)

# If you want to save only the model parameters
torch.save(llama.state_dict(), &#39;llama_model_params.pth&#39;)
</code></pre>
<p>为了将您的PyTorch模型保存到Hugging Face的Transformers库中，您可以使用 save_pretrained 方法。这是一个示例：</p>
<pre><code>from transformers import GPT2LMHeadModel, GPT2Config

# Assuming Llama is your PyTorch model
llama_config = GPT2Config.from_dict(MASTER_CONFIG)
llama_transformers = GPT2LMHeadModel(config=llama_config)
llama_transformers.load_state_dict(llama.state_dict())

# Specify the directory where you want to save the model
output_dir = &quot;llama_model_transformers&quot;

# Save the model and configuration
llama_transformers.save_pretrained(output_dir)
</code></pre>
<p>GPT2Config is used to create a configuration object compatible with GPT-2. Then, a GPT2LMHeadModel is created and loaded with the weights from your Llama model. Finally, save_pretrained is called to save both the model and configuration in the specified directory.<br>GPT2Config 用于创建与GPT-2兼容的配置对象。然后，创建并加载一个 GPT2LMHeadModel ，其中包含来自您的Llama模型的权重。最后，调用 save_pretrained 将模型和配置保存在指定的目录中。</p>
<p>您可以使用Transformers库加载模型</p>
<pre><code>from transformers import GPT2LMHeadModel, GPT2Config

# Specify the directory where the model was saved
output_dir = &quot;llama_model_transformers&quot;

# Load the model and configuration
llama_transformers = GPT2LMHeadModel.from_pretrained(output_dir)
</code></pre>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>In this blog, we’ve walked through a step-by-step process on how to implement the LLaMA approach to build your own small Language Model (LLM). As a suggestion, consider expanding your model to around 15 million parameters, as smaller models in the range of 10M to 20M tend to comprehend English better. Once your LLM becomes proficient in language, you can fine-tune it for specific use cases.<br>在这篇博客中，我们逐步介绍了如何实施LLaMA方法来构建自己的小型语言模型（LLM）。作为建议，考虑将模型扩展到大约1500万个参数，因为在10M到20M范围内的较小模型往往更能理解英语。一旦您的LLM在语言方面熟练，您可以对其进行特定用例的微调。</p>
<p>我希望这篇全面的博客能为您提供通过论文来创建个性化LLM的认知。</p>
<p>感谢阅读这篇详尽的帖子！</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p><a target="_blank" rel="noopener" href="https://levelup.gitconnected.com/building-a-million-parameter-llm-from-scratch-using-python-f612398f06c2">https://levelup.gitconnected.com/building-a-million-parameter-llm-from-scratch-using-python-f612398f06c2</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/03/08/springboot-music/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/03/08/springboot-music/" class="post-title-link" itemprop="url">用VSCode实践一个Spring Boot项目</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-03-08 23:23:21" itemprop="dateCreated datePublished" datetime="2024-03-08T23:23:21+08:00">2024-03-08</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-03-10 11:14:02" itemprop="dateModified" datetime="2024-03-10T11:14:02+08:00">2024-03-10</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>8 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>最近在学习Spring Boot，VSCode作为宇宙最强大的IDE，可以方便地开发Spring Boot项目，所以就用VSCode实践一下。</p>
<h2 id="开发环境"><a href="#开发环境" class="headerlink" title="开发环境"></a>开发环境</h2><p>JDK： jdk-8u141   </p>
<p>JDK： OpenJDK17U-jdk_x64_windows_hotspot_17.0.10_7</p>
<p>mysql：mysql-5.7.21-winx64.zip</p>
<p>redis：redis-windows-7.0.9</p>
<p>node：14.21.3</p>
<p>maven：maven-3.6.3</p>
<p>IDE：VSCode</p>
<h3 id="JDK"><a href="#JDK" class="headerlink" title="JDK"></a>JDK</h3><p>安装JDK17和JDK8，添加路径进入环境变量Path</p>
<p>可如下配置</p>
<p><img src="/../asset_sprintbootmusic/03.png"></p>
<p>添加 %JAVA_HOME%\bin 到path环境变量</p>
<p>验证 java -version</p>
<h3 id="mysql"><a href="#mysql" class="headerlink" title="mysql"></a>mysql</h3><p>具体见上文mysql 命令行安装</p>
<h3 id="node"><a href="#node" class="headerlink" title="node"></a>node</h3><p>下载 <a target="_blank" rel="noopener" href="https://nodejs.org/dist/v14.21.3/node-v14.21.3-win-x64.zip">https://nodejs.org/dist/v14.21.3/node-v14.21.3-win-x64.zip</a></p>
<p>解压，添加路径到环境变量Path</p>
<p>验证 node -v</p>
<h3 id="redis"><a href="#redis" class="headerlink" title="redis"></a>redis</h3><p>1、应用下载地址：<a target="_blank" rel="noopener" href="https://github.com/zkteco-home/redis-windows/archive/refs/tags/7.0.9.zip">https://github.com/zkteco-home/redis-windows/archive/refs/tags/7.0.9.zip</a></p>
<p>2、下载安装包。下载7.0.9。解压，把路径 F:\redis-windows-7.0.9 加到环境变量path</p>
<p>3、安装和取消安装服务<br><code>redis-server --service-install F:\redis-windows-7.0.9\redis.conf --loglevel verbose</code></p>
<p><code>redis-server --service-uninstall</code></p>
<p>4、启动和停止服务<br><code>redis-server --service-start</code></p>
<p><code>redis-server --service-stop</code></p>
<p>记得要启动服务</p>
<h3 id="maven"><a href="#maven" class="headerlink" title="maven"></a>maven</h3><p>下载 <a target="_blank" rel="noopener" href="https://archive.apache.org/dist/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.zip">https://archive.apache.org/dist/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.zip</a> ， 解压到 F:\apache-maven-3.6.3</p>
<p>配置环境变量</p>
<p>MAVEN_HOME ： F:\apache-maven-3.6.3</p>
<p>Path添加 %MAVEN_HOME%\bin</p>
<p>验证 mvn -v</p>
<p>修改settings.xml<br>修改 .&#x2F;conf&#x2F;settings.xml 文件</p>
<p><img src="/../asset_sprintbootmusic/01.png"></p>
<p>配置本地仓库<br><code>  &lt;localRepository&gt;D:\MVN_repository&lt;/localRepository&gt;</code></p>
<p>设置阿里云镜像</p>
<pre><code>&lt;!-- 阿里云中央仓库 159行 --&gt;
&lt;mirror&gt;
  &lt;id&gt;alimaven&lt;/id&gt;
  &lt;name&gt;aliyun maven&lt;/name&gt;
  &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;
  &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;        
&lt;/mirror&gt;
</code></pre>
<p><img src="/../asset_sprintbootmusic/02.png"></p>
<h2 id="用VSCode开发一个Spring-Boot-demo项目"><a href="#用VSCode开发一个Spring-Boot-demo项目" class="headerlink" title="用VSCode开发一个Spring Boot demo项目"></a>用VSCode开发一个Spring Boot demo项目</h2><p>1.在 Visual Studio Code 中打开扩展视图(Ctrl+Shift+X)，<br>搜索并安装</p>
<p>Java Extension Pack (Java 扩展包)<br>Spring Boot Extension Pack</p>
<p>2.配置maven<br>打开设置，搜索maven，编辑 settings.json</p>
<p>然后把maven的可执行文件路径配置、maven的setting路径配置、java.home的路径配置，拷贝到右侧的用户设置区域并且设置为自己电脑的实际路径</p>
<pre><code>&quot;maven.executable.path&quot;: &quot;F:\\apache-maven-3.6.3\\bin\\mvn.cmd&quot;,
&quot;maven.terminal.useJavaHome&quot;: true,
&quot;maven.terminal.customEnv&quot;: [
    &#123;
        &quot;environmentVariable&quot;: &quot;JAVA_HOME&quot;,
        &quot;value&quot;: &quot;C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.10.7-hotspot&quot;
    &#125;
],
&quot;java.configuration.maven.userSettings&quot;: &quot;F:\\apache-maven-3.6.3\\conf\\settings.xml&quot;,
&quot;java.jdt.ls.java.home&quot;:&quot;C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.10.7-hotspot&quot;,
</code></pre>
<p>配置完成重启 VSCode</p>
<p>3.创建springboot项目Demo<br>使用快捷键(Ctrl+Shift+P)命令窗口，输入 Spring 选择创建 Maven 项目。</p>
<p>选择springboot 3.2.3</p>
<p>选择java</p>
<p>设置包名</p>
<p>选择jar</p>
<p>jdk 选择17, 因为springboot 3.2.3最低版本是17</p>
<p>选择依赖：Spring Web web 和 SQL下的 MySQL Driver SQL和 MyBatis Framework </p>
<p>最后选择一个工作空间，打开项目</p>
<p>修改配置文件 application.properties， 添加数据库配置</p>
<pre><code>spring.datasource.url=jdbc:mysql://localhost:3306/sys?serverTimezone=Asia/Shanghai&amp;useSSL=false
spring.datasource.username=root
spring.datasource.password=123456
spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver
</code></pre>
<p>在项目目录下启动项目：  <code>mvn spring-boot:run</code></p>
<p>启动成功后，浏览器访问 <a target="_blank" rel="noopener" href="http://localhost:8080/">http://localhost:8080/</a></p>
<p>可以看到Whitelabel Error Page</p>
<p>修改DemoApplication.java文件，添加helloWorld接口如下：</p>
<pre><code>package com.example.demo;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

@SpringBootApplication
public class DemoApplication &#123;

    public static void main(String[] args) &#123;
        SpringApplication.run(DemoApplication.class, args);
    &#125;


    @RestController
    public class HelloWorldController &#123;

        @GetMapping(&quot;/hello&quot;)
        public String helloWorld() &#123;
            return &quot;Hello World!&quot;;
        &#125;

    &#125;

&#125;
</code></pre>
<p>保存后，重新运行demo，浏览器刷新页面，可以看到Hello World!</p>
<p>至此，环境已经准备ready，可以开始开发springboot项目了。</p>
<h2 id="下载开源music-website并运行"><a href="#下载开源music-website并运行" class="headerlink" title="下载开源music-website并运行"></a>下载开源music-website并运行</h2><p>1、下载项目到本地<br><code>git clone https://github.com/Yin-Hongwei/music-website.git</code></p>
<p>2、下载数据库中记录的资源<br>去【链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1TCV3bQ9npxx--9cOrDSsbQ">https://pan.baidu.com/s/1TCV3bQ9npxx--9cOrDSsbQ</a><br>提取码：pi7r  】下载网站依赖的歌曲及图片，将 data 夹里的文件放到 music-website&#x2F;music-server 文件夹下。<br>将 data 夹里的sql文件放到 music-website&#x2F;music-server&#x2F;sql 文件夹下，原因是这个开源项目的sql文件里面有少量数据有些问题，需要修改。</p>
<p>3、修改配置文件<br>1）创建数据库 将 music-website&#x2F;music-server&#x2F;sql 文件夹中的 tp_music.sql 文件用mysql workbench或者Navicat导入数据库。<br><img src="/../asset_sprintbootmusic/04.png"></p>
<p>2）修改用户名密码 修改 music-website&#x2F;music-server&#x2F;src&#x2F;main&#x2F;resources&#x2F;application-dev.properties 文件里的 spring.datasource.username 和 spring.datasource.password；</p>
<p>4、启动项目<br>启动服务端：进入 music-server 文件夹，运行下面命令启动服务器</p>
<p><code>mvn spring-boot:run</code> &#x2F;&#x2F; 前提装了 maven</p>
<p>启动客户端：进入 music-client 目录，运行下面命令<br><code>npm install</code> &#x2F;&#x2F; 安装依赖</p>
<p><code>npm run serve</code> &#x2F;&#x2F; 启动前台项目</p>
<p>启动管理端：进入 music-manage 目录，运行下面命令<br><code>npm install</code> &#x2F;&#x2F; 安装依赖</p>
<p><code>npm run serve</code> &#x2F;&#x2F; 启动后台管理项目</p>
<h2 id="技术栈"><a href="#技术栈" class="headerlink" title="技术栈"></a>技术栈</h2><h3 id="后端"><a href="#后端" class="headerlink" title="后端"></a>后端</h3><p>SpringBoot + MyBatis + Redis</p>
<h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><p>docker</p>
<h3 id="前端"><a href="#前端" class="headerlink" title="前端"></a>前端</h3><p>Vue3.0 + TypeScript + Vue-Router + Vuex + Axios + ElementPlus + Echarts</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/03/07/mysql5_7_21-cmd-install/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/03/07/mysql5_7_21-cmd-install/" class="post-title-link" itemprop="url">命令行安装MySQL 5.7.21</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-03-07 10:49:42" itemprop="dateCreated datePublished" datetime="2024-03-07T10:49:42+08:00">2024-03-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-03-08 16:31:07" itemprop="dateModified" datetime="2024-03-08T16:31:07+08:00">2024-03-08</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="下载MySQL安装包"><a href="#下载MySQL安装包" class="headerlink" title="下载MySQL安装包"></a>下载MySQL安装包</h2><p>从MySQL官网下载MySQL 5.7.21的安装包，下载地址：<a target="_blank" rel="noopener" href="https://downloads.mysql.com/archives/community/">https://downloads.mysql.com/archives/community/</a></p>
<h2 id="解压安装包"><a href="#解压安装包" class="headerlink" title="解压安装包"></a>解压安装包</h2><p>将下载好的安装包解压到指定目录，如：<code>D:\mysql-5.7.21-winx64</code></p>
<h2 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h2><p>打开系统环境变量编辑器，如：<code>控制面板 -&gt; 系统 -&gt; 系统设置 -&gt; 高级系统设置 -&gt; 环境变量</code></p>
<p>在系统变量中找到<code>Path</code>，双击编辑，在弹出的编辑框中将<code>D:\mysql-5.7.21-winx64\bin</code>添加到变量值末尾，并点击确定。</p>
<h2 id="启动MySQL服务"><a href="#启动MySQL服务" class="headerlink" title="启动MySQL服务"></a>启动MySQL服务</h2><p>用管理员身份打开命令行，在命令行中输入<code>mysqld --install</code>，回车，等待服务启动完成。</p>
<p>因为5.7以上版本中，D:\mysql-5.7.21-winx64\目录下没有data文件夹，需要在窗口输入<code>mysqld --initialize-insecure --user=mysql</code>，回车，等待初始化完成。</p>
<p>创建完成data后，再输入<code>mysqld –install</code>，然后按回车键</p>
<p>在命令行中输入<code>net start mysql</code>，回车，如果出现<code>The MySQL Server service is starting.</code>字样，则表示服务启动成功。启动服务器</p>
<h2 id="登录MySQL"><a href="#登录MySQL" class="headerlink" title="登录MySQL"></a>登录MySQL</h2><p>在命令行中输入<code>mysql -u root -p</code>，回车，因为没有设置登录密码，所以什么都不用输入，直接按回车键即可，即可登录MySQL。</p>
<p>设置密码<br><code>update mysql.user set authentication_string = password(&quot;your_password&quot;) where user = &quot;root&quot;;</code></p>
<p>刷新权限：<code>flush privileges;</code></p>
<p>退出MySQL：<code>exit;</code></p>
<h2 id="卸载MySQL"><a href="#卸载MySQL" class="headerlink" title="卸载MySQL"></a>卸载MySQL</h2><p>在命令行中输入<code>mysqld --remove</code>，回车，等待服务停止，然后输入<code>sc delete MySQL</code>，回车，即可卸载MySQL。</p>
<h2 id="workbench安装使用"><a href="#workbench安装使用" class="headerlink" title="workbench安装使用"></a>workbench安装使用</h2><p>workbench是MySQL的图形化管理工具，可以用来管理MySQL数据库。<br>我下载了较低的版本的workbench： mysql-workbench-community-6.3.3-winx64.msi</p>
<p>安装后，打开workbench，出现以下界面：</p>
<p><img src="/../asset_mysql/01.png"></p>
<p>双击 Local Instance 连接到本地的 MySQL 实例，输入 root 密码，点击 Connect 连接成功。然后出现以下界面</p>
<p><img src="/../asset_mysql/02.png"></p>
<p>点击上图中的 New Schema 新建数据库，输入数据库名称tp_music，选择编码方式utf-8,点击 Create 即可。</p>
<p><img src="/../asset_mysql/03.png">  </p>
<p>tp_music创建成功后，右键单击 Tables –&gt; Create table,新建表，输入表名称，点击 Create 即可。</p>
<p><img src="/../asset_mysql/04.png">  </p>
<p><img src="/../asset_mysql/05.png">  </p>
<p>构建数据库最终如下图</p>
<p><img src="/../asset_mysql/06.png">  </p>
<p>导入本地数据库(Server栏里Data Import)</p>
<ul>
<li>选择导入文件的路径</li>
<li>Start Import</li>
<li>刷新查看结果</li>
</ul>
<p><img src="/../asset_mysql/07.png">  </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Howard Huang</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">224k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">6:47</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">
    <!--由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动-->
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
