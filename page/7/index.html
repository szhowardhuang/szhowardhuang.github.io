<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.33/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"szhowardhuang.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="嵌入式老兵博客">
<meta property="og:url" content="https://szhowardhuang.github.io/page/7/index.html">
<meta property="og:site_name" content="嵌入式老兵博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Howard Huang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://szhowardhuang.github.io/page/7/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/7/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>嵌入式老兵博客</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">嵌入式老兵博客</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Howard Huang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">70</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/03/01/sora/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/03/01/sora/" class="post-title-link" itemprop="url">OpenAI Sora视频生成模型技术报告</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-03-01 23:00:16" itemprop="dateCreated datePublished" datetime="2024-03-01T23:00:16+08:00">2024-03-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-03-02 15:31:13" itemprop="dateModified" datetime="2024-03-02T15:31:13+08:00">2024-03-02</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>4.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>8 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h4 id="报告总结"><a href="#报告总结" class="headerlink" title="报告总结"></a>报告总结</h4><p>不管是在视频的保真度、长度、稳定性、一致性、分辨率、文字理解等方面，Sora都做到了SOTA（当前最优）。</p>
<p>技术细节写得比较泛（防止别人模仿）大概就是用视觉块编码（visual patch）的方式，把不同格式的视频统一编码成了用transformer架构能够训练的embeding，然后引入类似diffusion的unet的方式做在降维和升维的过程中做加噪和去噪，然后把模型做得足够大，大到能够出现涌现能力。</p>
<p>简单来说，在别家做视频模型的时候还是基于“小”模型的思路（基于上一帧预测下一帧，并且用文字或者笔刷遮罩做约束）的时候，OpenAI则是用做“大”模型的思路做视频生成——准备足够大量的视频，用多模态模型给视频做标注，把不同格式的视频编码成统一的视觉块嵌入，然后用足够大的网络架构+足够大的训练批次（batch size）+ 足够强的算力，让模型对足够多的训练集做全局拟合（理解），在模型更好地还原细节的同时让模型出现智能涌现能力——例如在一定程度上理解真实世界的物理影响和因果关系。</p>
<p>最让人期待（不安）的是，这个视频生成模型仿佛只是OpenAI世界模型（理解和模拟真实世界的各种复杂因果关系的通用模型）路上点亮的一个成就，而不是终点。</p>
<h4 id="Sora发布的潜在影响"><a href="#Sora发布的潜在影响" class="headerlink" title="Sora发布的潜在影响"></a>Sora发布的潜在影响</h4><p>C端 &#x2F; 对于普通人</p>
<p>这或许是独立创作者最好的年代，Sora发布之后，文案、音效、视频AI生成的可用工具都已齐备，一个人可以无痛carry一个短片，好故事将价值千金，有才华的人更难被埋没。但是从另一个角度将，创作门槛降低之后故事的竞争将异常激烈。</p>
<p>以vision pro为代表的XR产业将再次获得助力——内容匮乏将不再是问题。</p>
<p>目前当红的短视频推荐的形态可能会发生改变——从系统根据用户喜好推荐短视频，变成针对性生成短视频？或者说，同一个短视频在不同的用户对可以有不同的（实时）微调版本？</p>
<p>B端 &#x2F; 对于商业公司</p>
<p>所有做AI视频生成的公司将面临第一波危机，但是危中有机。因为OpenAI证明了用大模型的思路做视频是可行的，那么他们需要做的只是证明我也可以用大模型做视频。参考chatGPT火了之后做大语言模型的公司反而更多了而不是更少。</p>
<p>AI三维生成的公司将面临第二波冲击，由于多目重建技术的存在，视频生成和3D生成的界限是模糊的。所以3D生成可能要重新考虑当前技术路线的合理性和商业叙事逻辑。</p>
<p>虽然OpenAI没有明说，但是Sora需要的算力不会小，所以显卡公司会迎来新的一波利好，但是不一定利好英伟达。因为现在算力越来越呈现基础设施的特征，而基础设施是各个国家的命脉，即便不考虑禁运，我国不会是唯一一个要求算力自主可控的国家，甚至每个大厂都开始想自己搞显卡或者AI专用算力卡（参考google、特斯拉、openAI、阿里），所以算力领域的竞争者会越来越多。</p>
<h3 id="报告具体内容"><a href="#报告具体内容" class="headerlink" title="报告具体内容"></a>报告具体内容</h3><p>技术报告地址：<a target="_blank" rel="noopener" href="https://openai.com/research/video-generation-models-as-world-simulators">https://openai.com/research/video-generation-models-as-world-simulators</a></p>
<p>OpenAI 探索了视频数据生成模型的大规模训练。具体来说，研究人员在可变持续时间、分辨率和宽高比的视频和图像上联合训练了一个文本条件扩散模型。作者利用对视频和图像潜在代码的时空补丁进行操作的 transformer 架构，其最大的模型 Sora 能够生成长达一分钟的高质量视频。<br>OpenAI 认为，新展示的结果表明，扩展视频生成模型是构建物理世界通用模拟器的一条有前途的途径。</p>
<p>OpenAI 在技术报告中重点展示了：（1）将所有类型的视觉数据转化为统一表示，从而能够大规模训练生成模型的方法；以及（2）对 Sora 的能力和局限性进行定性评估。</p>
<p>令人遗憾的是，OpenAI 的报告不包含模型和训练的细节。<br>最近一段时间，视频生成是 AI 领域的重要方向，先前的许多工作研究了视频数据的生成建模方向，包括循环网络、生成对抗网络、自回归 transformer 和扩散模型。这些工作通常关注一小类视觉数据、较短的视频或固定大小的视频。</p>
<p>与之不同的是，OpenAI 的 Sora 是视觉数据的通用模型，它可以生成不同时长、长宽比和分辨率的视频和图像，而且最多可以输出长达一分钟的高清视频。</p>
<h4 id="视觉数据转为-Patches"><a href="#视觉数据转为-Patches" class="headerlink" title="视觉数据转为 Patches"></a>视觉数据转为 Patches</h4><p>大型语言模型通过在互联网规模的数据上进行训练，获得了出色的通用能力中，OpenAI 从这一点汲取了灵感。LLM 得以确立新范式，部分得益于创新了 token 使用的方法。研究人员们巧妙地将文本的多种模态 —— 代码、数学和各种自然语言统一了起来。</p>
<p>在这项工作中，OpenAI 考虑了生成视觉数据的模型如何继承这种方法的好处。大型语言模型有文本 token，而 Sora 有视觉 patches。此前的研究已经证明 patches 是视觉数据模型的有效表示。OpenAI 发现 patches 是训练生成各种类型视频和图像的模型的可扩展且有效的表示。</p>
<p>在更高层面上，OpenAI 首先将视频压缩到较低维的潜在空间，然后将表示分解为时空 patches，从而将视频转换为 patches。<br><img src="/../asset_sora/01.png"></p>
<h4 id="视频压缩网络"><a href="#视频压缩网络" class="headerlink" title="视频压缩网络"></a>视频压缩网络</h4><p>OpenAI 训练了一个降低视觉数据维度的网络。该网络将原始视频作为输入，并输出在时间和空间上压缩的潜在表示。Sora 在这个压缩的潜在空间中接受训练，而后生成视频。OpenAI 还训练了相应的解码器模型，将生成的潜在表示映射回像素空间。</p>
<h4 id="时空潜在-patches"><a href="#时空潜在-patches" class="headerlink" title="时空潜在 patches"></a>时空潜在 patches</h4><p>给定一个压缩的输入视频，OpenAI 提取一系列时空 patches，充当 Transformer 的 tokens。该方案也适用于图像，因为图像可视为单帧视频。OpenAI 基于 patches 的表示使 Sora 能够对不同分辨率、持续时间和长宽比的视频和图像进行训练。在推理时，OpenAI 可以通过在适当大小的网格中排列随机初始化的 patches 来控制生成视频的大小。</p>
<h4 id="扩展Transformer用于视频生成"><a href="#扩展Transformer用于视频生成" class="headerlink" title="扩展Transformer用于视频生成"></a>扩展Transformer用于视频生成</h4><p>Sora是一个扩散模型；给定输入的噪声块（和像文本提示这样的条件信息），它被训练来预测原始的“干净”块。重要的是，Sora是一个扩散变换器。变换器在包括语言建模、计算机视觉和图像生成等多个领域展现了显著的扩展属性。<br><img src="/../asset_sora/02.png"></p>
<p>在这项工作中，我们发现扩散变换器作为视频模型也能有效地扩展。下面，我们展示了训练进展过程中，使用固定种子和输入的视频样本比较。随着训练计算量的增加，样本质量显著提高。<br><img src="/../asset_sora/03.png"></p>
<h4 id="可变持续时间、分辨率、宽高比"><a href="#可变持续时间、分辨率、宽高比" class="headerlink" title="可变持续时间、分辨率、宽高比"></a>可变持续时间、分辨率、宽高比</h4><p>过去在图像和视频生成中的方法通常会将视频调整大小、裁剪或剪辑到一个标准尺寸——例如，4秒长的视频，分辨率为256x256。我们发现，直接在数据的原始尺寸上进行训练可以带来几个好处。</p>
<h5 id="采样灵活性"><a href="#采样灵活性" class="headerlink" title="采样灵活性"></a>采样灵活性</h5><p>Sora可以采样宽屏1920x1080p视频、竖屏1080x1920视频以及介于两者之间的所有格式。这使得Sora能够直接按照不同设备的原生宽高比创建内容。它还允许我们在使用同一模型生成全分辨率内容之前，快速原型化较小尺寸的内容。</p>
<p><img src="/../asset_sora/04.png"></p>
<h5 id="改进的构图和画面组成"><a href="#改进的构图和画面组成" class="headerlink" title="改进的构图和画面组成"></a>改进的构图和画面组成</h5><p>我们通过实证发现，在视频的原始宽高比上进行训练可以改善构图和取景。我们将Sora与一个版本的模型进行了比较，该模型将所有训练视频裁剪成正方形，这是训练生成模型时的常见做法。在正方形裁剪上训练的模型（左侧）有时会生成主体只部分出现在视野中的视频。相比之下，来自Sora的视频（右侧）具有改善的取景。<br><img src="/../asset_sora/05.png"></p>
<h4 id="语言理解"><a href="#语言理解" class="headerlink" title="语言理解"></a>语言理解</h4><p>训练文本到视频生成系统需要大量带有相应文字标题的视频。我们将在DALL·E 3中引入的重新标注技术应用到视频上。我们首先训练一个高度描述性的标注模型，然后使用它为我们训练集中的所有视频生成文字标题。我们发现，在高度描述性的视频标题上进行训练可以提高文本的准确性以及视频的整体质量。</p>
<p>类似于DALL·E 3，我们也利用GPT将用户的简短提示转换成更长的详细说明，然后发送给视频模型。这使得Sora能够生成高质量的视频，准确地遵循用户的提示。</p>
<p><img src="/../asset_sora/06.png"></p>
<h4 id="使用图片和视频进行提示"><a href="#使用图片和视频进行提示" class="headerlink" title="使用图片和视频进行提示"></a>使用图片和视频进行提示</h4><p>上述结果以及我们的登录页面展示了文本到视频的样本。但是Sora也可以通过其他输入进行提示，例如预先存在的图片或视频。这项能力使得Sora能够执行广泛的图像和视频编辑任务——创建完美循环的视频，为静态图像添加动画，向前或向后延长视频的时间等。</p>
<h4 id="视频到视频编辑"><a href="#视频到视频编辑" class="headerlink" title="视频到视频编辑"></a>视频到视频编辑</h4><p>扩散模型使得从文本提示编辑图像和视频的方法层出不穷。下面我们将其中一种方法，SDEdit，应用于Sora。这项技术使得Sora能够零次学习地转换输入视频的风格和环境。<br><img src="/../asset_sora/07.png"></p>
<h4 id="连接视频"><a href="#连接视频" class="headerlink" title="连接视频"></a>连接视频</h4><p>我们还可以使用Sora在两个输入视频之间逐渐插值，创建在完全不同主题和场景构成的视频之间的无缝过渡。在下面的例子中，中间的视频在左右两边对应视频之间进行插值。<br><img src="/../asset_sora/08.png"></p>
<h4 id="图像生成能力"><a href="#图像生成能力" class="headerlink" title="图像生成能力"></a>图像生成能力</h4><p>Sora也能够生成图像。我们通过在具有一个帧时间范围的空间网格中排列高斯噪声块来实现这一点。该模型可以生成不同大小的图像——分辨率最高可达2048x2048。<br><img src="/../asset_sora/09.png"><br>A snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f&#x2F;1.2</p>
<p>一个雪山村庄，有着舒适的小木屋和北极光展示，高清晰度和逼真的数码单反相机，50mm f&#x2F;1.2镜头拍摄。</p>
<h4 id="涌现的模拟能力"><a href="#涌现的模拟能力" class="headerlink" title="涌现的模拟能力"></a>涌现的模拟能力</h4><p>我们发现，当在大规模上训练时，视频模型展现出许多有趣的新兴能力。这些能力使得Sora能够模拟现实世界中人类、动物和环境的某些方面。这些属性并没有任何针对3D、物体等的明确归纳偏见——它们纯粹是规模效应的现象。</p>
<p>3D一致性。Sora能够生成具有动态相机运动的视频。随着相机的移动和旋转，人物和场景元素在三维空间中保持一致地移动。</p>
<p>长距离一致性和物体恒存性。对于视频生成系统来说，一个重大挑战是在采样长视频时保持时间上的连贯性。我们发现，尽管不总是如此，Sora通常能够有效地建模短距离和长距离依赖关系。例如，我们的模型即使在人、动物和物体被遮挡或离开画面时，也能持续保持它们的存在。同样，它能在单个样本中生成同一角色的多个镜头，并在整个视频中保持其外观。</p>
<p>与世界互动。Sora有时可以模拟一些简单的动作来影响世界的状态。例如，画家可以在画布上留下随时间持续存在的新笔触，或者一个人可以吃一个汉堡并留下咬痕。</p>
<p>模拟数字世界。Sora也能够模拟人工过程，一个例子是视频游戏。Sora可以在同时控制《我的世界》中的玩家采用基本策略的同时，还能以高保真度渲染世界及其动态。通过用提到“我的世界”的字幕提示Sora，可以零次尝试地引发这些能力。</p>
<p>Sora作为一个模拟器目前展现出许多限制。例如，它并没有准确地模拟许多基本互动的物理效应，比如玻璃破碎。其他互动，比如吃食物，不总是产生正确的物体状态变化。我们在我们的登录页面列举了模型的其他常见故障模式，比如在长时间样本中发展的不连贯性或物体的自发出现。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/02/27/digital-human-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/02/27/digital-human-2/" class="post-title-link" itemprop="url">如何设计一个数字人续篇 --- Video Retalking 模型篇</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-02-27 23:00:16" itemprop="dateCreated datePublished" datetime="2024-02-27T23:00:16+08:00">2024-02-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-02-29 17:56:17" itemprop="dateModified" datetime="2024-02-29T17:56:17+08:00">2024-02-29</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h3 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h3><pre><code>git clone https://github.com/vinthony/video-retalking.git
cd video-retalking
conda create -n video_retalking python=3.8
conda activate video_retalking

conda install ffmpeg
</code></pre>
<p>win10安装cuda11.1， cuda各版本可以共存。通常你现有N卡驱动会很新，所以安装cuda11.1时，不要安装N卡驱动。</p>
<p>where nvcc，可以看到路径<br><img src="/../asset_digitalhuman2/01.png"></p>
<p>我们修改环境变量，将cuda11.1的目录添加到PATH中。<br>主要是C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\bin;<br>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.8\libnvvp这两个变量；<br>先建立cuda11.1 和 cuda12.1的环境变量，再建立CUDA_PATH的环境变量，以后根据需要修改这个变量，选择不同的CUDA版本路径。</p>
<p><img src="/../asset_digitalhuman2/02.png"></p>
<p>修改PATH环境变量如下图</p>
<p><img src="/../asset_digitalhuman2/03.png"></p>
<pre><code>pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html

pip install -r requirements.txt
</code></pre>
<p>清华源会出现tb-nightly的错误，因为在tsinghua镜像库中没有tb-nightly</p>
<p>备注：tb-nightly 是一个 Python 包，它是 TensorFlow 的 TensorBoard 的夜间版本（nightly version）。TensorBoard 是一个用于可视化和监控 TensorFlow 训练过程和模型的工具。TensorBoard 提供了一组交互式的仪表板，可以显示训练过程中的指标、损失函数、模型结构图、梯度直方图等信息。它可以帮助您理解和调试 TensorFlow 模型，以及优化模型的性能。</p>
<p>pip临时使用阿里源</p>
<pre><code>pip install -i https://mirrors.aliyun.com/pypi/simple basicsr==1.4.2
</code></pre>
<p>安装cmake</p>
<p>然后继续 pip install -r requirements.txt</p>
<p>环境安装完成</p>
<h3 id="模型下载"><a href="#模型下载" class="headerlink" title="模型下载"></a>模型下载</h3><p>下载<a target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/18rhjMpxK8LVVxf7PI6XwOidt8Vouv_H0?usp=share_link">预训练模型</a>，放到checkpoints目录下,这个下载链接没有放GFPGANv1.4模型，可以去<a target="_blank" rel="noopener" href="https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth">此处</a>下载然后放到checkpoints目录下</p>
<p>GFPGAN1.4人脸超分效果最好</p>
<p>修改inference.py的 GFPGANer的model_path参数为checkpoints&#x2F;GFPGANv1.4.pth，如下：</p>
<pre><code>restorer = GFPGANer(model_path=&#39;checkpoints/GFPGANv1.4.pth&#39;, upscale=1, arch=&#39;clean&#39;, channel_multiplier=2, bg_upsampler=None)
</code></pre>
<p>模型如果不好下载，可以去百度云盘下载：<br>链接：<a target="_blank" rel="noopener" href="https://pan.baidu.com/s/1GTccvI8QUTQOhHrRnYeeAQ">https://pan.baidu.com/s/1GTccvI8QUTQOhHrRnYeeAQ</a><br>提取码：9l0u </p>
<h3 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h3><pre><code>python3 inference.py --face examples/face/1.mp4 --audio examples/audio/1.wav --outfile results/1_1.mp4
</code></pre>
<p>–exp_img ：预定义的表情模板。默认为“neutral”。您可以选择“smile”或图像路径。</p>
<p>–up_face ：您可以选择“surprise”或“angry”来修改脸上的表情，参考<a target="_blank" rel="noopener" href="https://github.com/donydchen/ganimation_replicate">GANimation</a>。</p>
<p>注意： 1.mp4的分辨率要正方形，用FFMPEG处理一下</p>
<pre><code>ffmpeg -i 6.mp4 -vf crop=720:720:0:0 7.mp4
corp： w h x y， 裁剪的宽、高、起始横坐标、起始纵坐标（左上角为0，0）。
</code></pre>
<p>生成的视频文件已经配上口型，画质有点模糊，可以用GFPGAN1.4做整个画面的超分。<br>方法如下：</p>
<h4 id="视频转图片流"><a href="#视频转图片流" class="headerlink" title="视频转图片流"></a>视频转图片流</h4><pre><code>ffmpeg -i 1_4.mp4  image%03d.png
</code></pre>
<h4 id="将图片流进行超分"><a href="#将图片流进行超分" class="headerlink" title="将图片流进行超分"></a>将图片流进行超分</h4><pre><code>git clone https://github.com/TencentARC/GFPGAN.git

cd GFPGAN

pip install realesrgan

python inference_gfpgan.py -i inputs/whole_imgs -o results -v 1.4 -s 1

-h                   show this help
-i input             Input image or folder. Default: inputs/whole_imgs
-o output            Output folder. Default: results
-v version           GFPGAN model version. Option: 1 | 1.2 | 1.3. Default: 1.3
-s upscale           The final upsampling scale of the image. Default: 2
-bg_upsampler        background upsampler. Default: realesrgan
-bg_tile             Tile size for background sampler, 0 for no tile during testing. Default: 400
-suffix              Suffix of the restored faces
-only_center_face    Only restore the center face
-aligned             Input are aligned faces
-ext                 Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto
</code></pre>
<h4 id="将图片流合成视频"><a href="#将图片流合成视频" class="headerlink" title="将图片流合成视频"></a>将图片流合成视频</h4><pre><code>ffmpeg -framerate 30 -i image%03d.png -b:v 575k gfpganwoa.mp4
</code></pre>
<p>视频码率属性 -b:v ， 具体码率要参考生成的原视频</p>
<p>如有必要，可以把两个视频横向合并，便于对比画质，合成视频的音频只会用左边视频的音频</p>
<pre><code>ffmpeg -i left.mp4 -i right.mp4 -filter_complex hstack output.mp4
</code></pre>
<p>如果要重新合成音频，如下：</p>
<pre><code>ffmpeg -i input.mp4 -i 3.wav -vcodec copy -acodec aac -map 0:v:0 -map 1:a:0 output.mp4

-map 0:v:0 选择了第0个输入文件（视频输入）的第0个轨道。
–map 1:a:0 选择了第1个输入文件（音频输入）的第0个轨道。
</code></pre>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/02/22/digital-human/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/02/22/digital-human/" class="post-title-link" itemprop="url">如何设计一个数字人 --- 3D模型篇</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-02-22 23:00:16" itemprop="dateCreated datePublished" datetime="2024-02-22T23:00:16+08:00">2024-02-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-02-23 22:08:16" itemprop="dateModified" datetime="2024-02-23T22:08:16+08:00">2024-02-23</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>10 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>下面我就以制作一个数字人卖货视频为例</p>
<p>首先介绍一下将会用到的 AI 工具。</p>
<ol>
<li><p>ChatGPT: 在制作数字人的过程中，ChatGPT 可以用于设定数字人的性格、生成演讲文本、回答技术问题等。</p>
</li>
<li><p>MetaHuman Creator: 这是一款由 Epic Games 开发的三维人物建模软件，可以通过对头部、身体等部位个性化的调整，实现高度逼真的数字人三维建模。</p>
</li>
<li><p>微软语音合成助手，由微软公司开发，用于语音识别与合成，可以将 ChatGPT 生成的文本内容转化为数字人所需要的自然语音输出。</p>
</li>
<li><p>Audio2Face: 由英伟达公司开发的一款表情驱动技术软件，可以将音频的语调、音量等信息转化为数字人的面部表情，从而使数字人的表现更加生动真实</p>
</li>
<li><p>UE5: Epic Games 开发的一款高级游戏引擎，可以提供实时渲染、动态光照、体积雾等功能，可用于打造更加真实的数字人场景。</p>
</li>
</ol>
<p>下面是制作一个数字人卖货视频的大概流程:</p>
<p>AI人物设定和内容生成 一 3D建模 一 AI语音生成 一 AI表情驱动 一 AI场景搭建</p>
<p>接下来让我为你具体介绍每一部分的流程。</p>
<h3 id="AI人物设定和内容生成"><a href="#AI人物设定和内容生成" class="headerlink" title="AI人物设定和内容生成"></a>AI人物设定和内容生成</h3><p>人物设定是制作数字人的第一步，这里我们可以使用 ChatGPT 来进行人物设定，为数字人定制其个性化特征和语言风格等。</p>
<p><img src="/../asset_digitalhuman/003.png"></p>
<p>生成售卖文案</p>
<p><img src="/../asset_digitalhuman/004.png"></p>
<h3 id="3D建模"><a href="#3D建模" class="headerlink" title="3D建模"></a>3D建模</h3><p>登录 MetaHuman Creator 进行 3D 建模<br>进入 MetaHuman Creator <a target="_blank" rel="noopener" href="https://metahuman.unrealengine.com/mhc">https://metahuman.unrealengine.com/mhc</a> 在预设数字人中选择一个人物模型，结合chatgpt的描述进行个性化调整，如头发、服饰、发型等。</p>
<p><img src="/../asset_digitalhuman/005.png"></p>
<p>调整后如下图：</p>
<p><img src="/../asset_digitalhuman/006.png"><br>此模型会存在云端，后续从UE5导入此模型。</p>
<h3 id="AI语音生成"><a href="#AI语音生成" class="headerlink" title="AI语音生成"></a>AI语音生成</h3><p>使用语音合成助手，将 ChatGPT 生成的文本内容转化为数字人所需要的自然语音输出。<br>这里用小白兔AI工具包里面的微软语音合成把文本转成语音，语音文件会自动下载到本地。</p>
<p><img src="/../asset_digitalhuman/007.png"></p>
<p>微软语音合成需要用到Microsoft Azure API，需要注册Azure账号，并申请语音合成服务。</p>
<p><img src="/../asset_digitalhuman/008.png"><br>把密钥和区域填到小白兔AI里面</p>
<p>注：Microsoft Azure 是微软的公用云端服务平台，首先得有个 Microsoft 账号，然后再基于这个 Microsoft 账号开通 Microsoft Azure 帐户。<br>Microsoft Azure收费模式</p>
<table>
<thead>
<tr>
<th>服务</th>
<th>免费额度</th>
<th>超出免费额度</th>
<th>并发请求数</th>
</tr>
</thead>
<tbody><tr>
<td>文本转语音</td>
<td>每月50万字符</td>
<td>16美元&#x2F;100万字符</td>
<td>-</td>
</tr>
</tbody></table>
<p>Microsoft Azure支持通过国内的 Visa 卡申请。</p>
<p>后面讲一下如何开通 Azure 帐户</p>
<h3 id="AI表情驱动"><a href="#AI表情驱动" class="headerlink" title="AI表情驱动"></a>AI表情驱动</h3><p>Audio2Face是由英伟达公司开发的一款表情驱动技术软件，可以将音频的语调、音量等信息转化为数字人的面部表情，从而使数字人的表现更加生动真实。<br>我们将微软语音助手生成的语音转成WAV格式，并导入到Audio2Face中，进行表情驱动。</p>
<p>去nvidia官网下载Audio2Face标准版 <a target="_blank" rel="noopener" href="https://www.nvidia.com/en-us/omniverse/apps/audio2face/">https://www.nvidia.com/en-us/omniverse/apps/audio2face/</a></p>
<p>打开audio2face，如下图设置，确认可以说话动嘴。<br><img src="/../asset_digitalhuman/010.png"><br>然后存项目到本地</p>
<h3 id="AI场景搭建"><a href="#AI场景搭建" class="headerlink" title="AI场景搭建"></a>AI场景搭建</h3><p>先安装UE5，下载地址 <a target="_blank" rel="noopener" href="https://www.unrealengine.com/en-US/download/">https://www.unrealengine.com/en-US/download/</a></p>
<p>然后下载MetaHuman示例项目 </p>
<p><img src="/../asset_digitalhuman/011.png"></p>
<p>打开此工程，点击窗口，选择Quixel Bridge</p>
<p><img src="/../asset_digitalhuman/012.png"></p>
<p>出现如下界面，My MetaHuman 的模型就是之前MetaHuman Creator生成的模型，点击 add按钮，会把这个模型导入到UE5的工程里面。 下载模型的时间很长，耐心等。</p>
<p><img src="/../asset_digitalhuman/013.png"><br>然后可以在UE5里面使用你导入的模型，因为我修改的模型和示例工程的模型一致，所以UE5会直接替换掉。</p>
<p>然后设置Omniverse livelink，让UE5和Audio2Face连接起来，让数字人可以和你说话。</p>
<p>首先把Audio2Face的插件拷贝到UE5的Plugins文件夹下，Audio2Face的插件路径一般是 user目录下面的AppData目录下面</p>
<p><img src="/../asset_digitalhuman/017.png"></p>
<p>拷贝到UE5安装目录</p>
<p><img src="/../asset_digitalhuman/018.png"></p>
<p>打开UE5的插件，选择Nvidia</p>
<p><img src="/../asset_digitalhuman/015.png"></p>
<p>使能 NVIDIA Omniverse ACE</p>
<p><img src="/../asset_digitalhuman/016.png"></p>
<p>打开UE5的窗口，选择 虚拟制片 - Live Link，点击「启动 Live Link」</p>
<p><img src="/../asset_digitalhuman/014.png"></p>
<p>添加源 NVIDIA Omniverse Live Link，点击 OK<br><img src="/../asset_digitalhuman/019.png"></p>
<p>点击Audio2face的stage - audio2face - StreamLivelink， 勾选 Activate</p>
<p><img src="/../asset_digitalhuman/020.png"></p>
<p>点击audio2face的wave Play按钮，让数字人开始说话。</p>
<p>之后参考youtube视频，调整audio2face的模型参数，让数字人更加动人。</p>
<p>最后，修改UE5的场景，调整场景的光照、音效、材质，让场景更加生动。</p>
<p>教程结束，祝你玩得开心！</p>
<h2 id="附注"><a href="#附注" class="headerlink" title="附注"></a>附注</h2><h3 id="注册Azure账号"><a href="#注册Azure账号" class="headerlink" title="注册Azure账号"></a>注册Azure账号</h3><p>网页 <a target="_blank" rel="noopener" href="https://signup.azure.com/signup">https://signup.azure.com/signup</a></p>
<p>进入网页之后，先登陆 Microsoft 账号。</p>
<p>填写个人信息<br><img src="/../asset_digitalhuman/azure_signup_1.jpg"></p>
<p>填写卡信息用于身份验证，点击「注册」<br><img src="/../asset_digitalhuman/azure_signup_2.jpg"></p>
<p>创建 Azure 订阅, Azure 订阅主要就是决定如何计费，一般是有「免费使用版」和 「即用即付」两种选择。</p>
<p>如果是刚注册 Azure 确实是可以选择「免费使用版」的，但是只能使用 30 天，30 天之后还是得转为「即用即付」才可以继续使用。</p>
<p>所以我建议直接选「即用即付」订阅，也是享有同等的免费额度。从注册 Azure 开始，Microsoft 翻译可以拥有 12 个月的免费额度，12 个月之后就没有了，想要继续使用需要收费。但是 Microsoft 语音合成目前是每个月都有免费额度，没有 12 个月的限制。（当然，前提是 Microsoft 不改变免费规则）</p>
<p>进入「订阅页面」<a target="_blank" rel="noopener" href="https://portal.azure.com/?quickstart=True#view/Microsoft_Azure_Billing/SubscriptionsBlade">https://portal.azure.com/?quickstart=True#view/Microsoft_Azure_Billing/SubscriptionsBlade</a> ，点击「添加」，点击「即用即付」下方的「选择产品&#x2F;服务」</p>
<p><img src="/../asset_digitalhuman/azure_subscription_1.jpg"></p>
<p>选中「我同意…」、「我愿意…」，点击「下一步」</p>
<p><img src="/../asset_digitalhuman/azure_subscription_2.jpg"></p>
<p>之前注册的时候应该已经填写过卡信息了，这里会默认选中，然后点击「下一步」</p>
<p><img src="/../asset_digitalhuman/azure_subscription_3.jpg"></p>
<p>然后这里选择「没有任何技术支持…」，点击「注册」</p>
<p><img src="/../asset_digitalhuman/azure_subscription_4.jpg"></p>
<p>创建资源组, 进入 「资源组页面」 <a target="_blank" rel="noopener" href="https://portal.azure.com/?quickstart=True#view/HubsExtension/BrowseResourceGroups">https://portal.azure.com/?quickstart=True#view/HubsExtension/BrowseResourceGroups</a> ，点击「创建」</p>
<p><img src="/../asset_digitalhuman/azure_resource_1.jpg"></p>
<ul>
<li>订阅选刚才创建好的订阅，默认应该就一个</li>
<li>资源组可以直接命名为 Bob</li>
<li>区域可以选离自己地理位置近的，例如在中国就选 East Aisa 就行</li>
<li>然后点击「查看+创建」<br><img src="/../asset_digitalhuman/azure_resource_2.jpg"></li>
</ul>
<p>点击「创建」<br><img src="/../asset_digitalhuman/azure_resource_3.jpg"></p>
<p>如下所示即为创建成功<br><img src="/../asset_digitalhuman/azure_resource_4.jpg"></p>
<p>创建语音服务资源, 进入「Azure AI services | 语音服务 」页面 <a target="_blank" rel="noopener" href="https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices">https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices</a> ，点击「创建」<br><img src="/../asset_digitalhuman/009.png"></p>
<ul>
<li>订阅选刚才创建好的订阅</li>
<li>资源组选刚才创建好的资源组</li>
<li>区域可以选离自己地理位置近的，选跟前面创建资源组一样的就行，例如在中国就选 East Aisa 就行</li>
<li>名称这一栏随便取名，但是不能重名，所以输入和我一样的应该会报错，随便输入不一样的就行，由数字、字母和横线组成即可</li>
<li>定价层选中「Free F0」</li>
<li>然后点击「审阅并创建」<br><img src="/../asset_digitalhuman/azure_tts_open_2.jpg"></li>
</ul>
<p>点击创建<br><img src="/../asset_digitalhuman/azure_tts_open_3.jpg"></p>
<p>如下所示即为创建成功<br><img src="/../asset_digitalhuman/azure_tts_open_4.jpg"></p>
<p>获取秘钥, 进入「认知服务 | 语音服务 」页面 <a target="_blank" rel="noopener" href="https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices">https://portal.azure.com/#view/Microsoft_Azure_ProjectOxford/CognitiveServicesHub/~/SpeechServices</a> ，应该能看到你刚才创建语音服务资源，点击进入详情<br><img src="/../asset_digitalhuman/azure_tts_secret_1.jpg"></p>
<p>选中「密钥和终结点」，如下图所示即为需要的秘钥<br><img src="/../asset_digitalhuman/azure_tts_secret_2.jpg"></p>
<p>填写秘钥<br>在 小白兔AI 的 设置 &gt; 微软付费订阅 中，将刚才获取到的秘钥填写到对应位置即可。</p>
<p>微软视素的文档可以看看， <a target="_blank" rel="noopener" href="https://learn.microsoft.com/zh-cn/azure/ai-services/speech-service/how-to-speech-synthesis-viseme">https://learn.microsoft.com/zh-cn/azure/ai-services/speech-service/how-to-speech-synthesis-viseme</a></p>
<h3 id="FFMPEG"><a href="#FFMPEG" class="headerlink" title="FFMPEG"></a>FFMPEG</h3><h4 id="使用FFmpeg将MP3文件转换成WAVE格式，你可以使用以下命令："><a href="#使用FFmpeg将MP3文件转换成WAVE格式，你可以使用以下命令：" class="headerlink" title="使用FFmpeg将MP3文件转换成WAVE格式，你可以使用以下命令："></a>使用FFmpeg将MP3文件转换成WAVE格式，你可以使用以下命令：</h4><pre><code>ffmpeg -i input.mp3 -ar 16000 output.wav
</code></pre>
<p>-i input.mp3 指定输入文件，其中 input.mp3 是你的源MP3文件的名称。</p>
<p>-ar 16000 指定了输出文件的采样率为16000 Hz。</p>
<h4 id="录屏视频截取，使用FFmpeg截取视频文件的一部分，你可以通过指定开始时间（-ss-参数）和持续时间（-t-参数）或结束时间（-to-参数）来完成。以下是一些常见的使用场景和相应的命令示例："><a href="#录屏视频截取，使用FFmpeg截取视频文件的一部分，你可以通过指定开始时间（-ss-参数）和持续时间（-t-参数）或结束时间（-to-参数）来完成。以下是一些常见的使用场景和相应的命令示例：" class="headerlink" title="录屏视频截取，使用FFmpeg截取视频文件的一部分，你可以通过指定开始时间（-ss 参数）和持续时间（-t 参数）或结束时间（-to 参数）来完成。以下是一些常见的使用场景和相应的命令示例："></a>录屏视频截取，使用FFmpeg截取视频文件的一部分，你可以通过指定开始时间（-ss 参数）和持续时间（-t 参数）或结束时间（-to 参数）来完成。以下是一些常见的使用场景和相应的命令示例：</h4><h5 id="使用-ss-和-t-参数"><a href="#使用-ss-和-t-参数" class="headerlink" title="使用 -ss 和 -t 参数"></a>使用 -ss 和 -t 参数</h5><p>在这个例子中，-ss 指定了开始截取的时间点，-t 指定了从开始点后要截取的持续时间。</p>
<pre><code>ffmpeg -i input.mp4 -ss 00:00:10 -t 00:00:20 -c copy output.mp4
</code></pre>
<p>input.mp4 是源视频文件。<br>-ss 00:00:10 表示从视频的第10秒开始截取。<br>-t 00:00:20 表示截取从开始点算起20秒的视频内容。<br>-c copy 表示使用“复制”模式，这样可以避免重新编码视频和音频流，加快处理速度并保持原有质量。<br>output.mp4 是输出的视频文件。</p>
<h5 id="使用-ss-和-to-参数"><a href="#使用-ss-和-to-参数" class="headerlink" title="使用 -ss 和 -to 参数"></a>使用 -ss 和 -to 参数</h5><p>与上面的例子不同，-to 参数指定的是截取的结束时间点，而不是持续时间。</p>
<pre><code>ffmpeg -i input.mp4 -ss 00:00:10 -to 00:00:30 -c copy output.mp4
</code></pre>
<p>-to 00:00:30 表示截取到视频的第30秒为止。</p>
<h4 id="音画不同步，画面延迟大约1秒，用FFMPEG调整，相对视频对音频时间戳进行偏移："><a href="#音画不同步，画面延迟大约1秒，用FFMPEG调整，相对视频对音频时间戳进行偏移：" class="headerlink" title="音画不同步，画面延迟大约1秒，用FFMPEG调整，相对视频对音频时间戳进行偏移："></a>音画不同步，画面延迟大约1秒，用FFMPEG调整，相对视频对音频时间戳进行偏移：</h4><p>音频相对于视频移后时间00:00:01.000</p>
<pre><code>ffmpeg -y -itsoffset 00:00:01.000 -i out.mp4 -i out.mp4 -map 0:a -map 1:v -vcodec copy -acodec copy -f mp4 -threads 2 -v warning out.sync.mp4
</code></pre>
<p>-y 可覆盖，如果文件已存在强制替换；</p>
<p>-itsoffset offset 设置以秒为基准的时间偏移，该选项影响所有后面的输入文件。该偏移被加到输入文件的时戳，定义一个正偏移意味着相应的流被延迟了 offset秒。 [-]hh:mm:ss[.xxx]的格式也支持</p>
<p>-i 输入，后面是空格，紧跟着就是输入视频文件</p>
<p>-f fmt 强迫采用格式fmt</p>
<p>-v：调试信息级别（quiet、panic、fatal、error、warning、info、verbose、debug）</p>
<p>-vcodec copy 和 -acodec copy表示所要使用的视频和音频的编码格式，这里指定为copy表示原样拷贝</p>
<p>-map file_number:stream_type[:stream_number]    选择媒体流语法</p>
<p>这有一些特别流符号的说明：</p>
<p>1、-map 0 选择第一个文件的所有流</p>
<p>2、-map i:v 从文件序号i(index)中获取所有视频流， -map i:a 获取所有音频流，-map i:v 获取所有视频流，-map i:s 获取所有字幕流等等。</p>
<p>3、特殊参数-an,-vn,-sn分别排除所有的音频，视频，字幕流。</p>
<h4 id="播放验证"><a href="#播放验证" class="headerlink" title="播放验证"></a>播放验证</h4><pre><code>ffplay out.sync.mp4
</code></pre>
<h3 id="如何定制数字人的人脸"><a href="#如何定制数字人的人脸" class="headerlink" title="如何定制数字人的人脸"></a>如何定制数字人的人脸</h3><p>用polycam扫描人脸</p>
<p>导出GLTF</p>
<p>打开blender，导入GLTF，删除多余部分，选中，按X键删除， 然后merge mesh</p>
<p>导出FBX，path mode选择copy，并点击旁边的按钮；object选择Animate和mesh；Geometry选择face；disable Bake Animation；然后点击export，保存为.fbx文件<br><img src="/../asset_digitalhuman/021.png"></p>
<p>到UE5，新建项目，影视和现场活动 - 空白<br><img src="/../asset_digitalhuman/023.png"></p>
<p>把fbx文件拖到UE5内容窗口，导入.fbx文件</p>
<p><img src="/../asset_digitalhuman/024.png"></p>
<p>导入后，双击material，调整material，添加常量节点</p>
<p><img src="/../asset_digitalhuman/025.png"></p>
<p>调整后，如下图</p>
<p><img src="/../asset_digitalhuman/026.png"></p>
<p>MetaHuman Plugin安装, 在Epic商城安装<br><a target="_blank" rel="noopener" href="https://www.unrealengine.com/marketplace/en-US/product/metahuman-plugin#">https://www.unrealengine.com/marketplace/en-US/product/metahuman-plugin#</a></p>
<p>打开插件菜单，enable metahuman plugin</p>
<p>在内容窗口右键，选择metahuman本体</p>
<p><img src="/../asset_digitalhuman/027.png"></p>
<p>双击这个本体，进入编辑窗口，导入mesh</p>
<p><img src="/../asset_digitalhuman/028.png"></p>
<p>调整人脸的位置， 快捷键 W&#x2F;R&#x2F;E ， 设置viewport的FOV到20</p>
<p><img src="/../asset_digitalhuman/029.png"></p>
<p>打开提升帧，选择自动追踪</p>
<p>点击metahuman本体解算</p>
<p>点击body，选择一个体型</p>
<p>点击Mesh to Metahuman，如下图</p>
<p><img src="/../asset_digitalhuman/030.png"></p>
<p>进入Quilel Bridge，选择metahuman - my metahuman， 选择你新建立的本体，启动MHC</p>
<p><img src="/../asset_digitalhuman/031.png"></p>
<p>进入metahuman后，调整人物各部分</p>
<p><img src="/../asset_digitalhuman/022.png"></p>
<p>结束，后续在UE5项目导入你定制的人脸即可。</p>
<h3 id="参考视频"><a href="#参考视频" class="headerlink" title="参考视频"></a>参考视频</h3><p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=HEFIvFnUfpw">Audio2Face to MetaHuman | How to Animate MetaHuman using Audio2Face Live Link</a></p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=2tGPx0Athvk">Importing a Metahuman into Unreal Engine 5 (simplified way) </a></p>
<p><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=xFk_WU32igA">How to Use Mesh to Metahuman (From Scan to Metahuman)</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/02/01/lagent_sft/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/02/01/lagent_sft/" class="post-title-link" itemprop="url">微调Lagent的实践</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-02-01 21:00:16 / 修改时间：23:06:12" itemprop="dateCreated datePublished" datetime="2024-02-01T21:00:16+08:00">2024-02-01</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>9.2k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>17 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>InternLM的Lagent是智能代理的一个框架，近几日实践了智能代理调用MM yolo进行图像类识别的任务，特写此文作为记录。</p>
<h3 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h3><pre><code>conda create --name xtuner0.1.9 --clone=/root/share/conda_envs/internlm-base
conda activate xtuner0.1.9
git clone -b v0.1.9 https://gitee.com/internlm/xtuner
cd xtuner
pip install -e &#39;.[all]&#39;
</code></pre>
<h3 id="基座模型"><a href="#基座模型" class="headerlink" title="基座模型"></a>基座模型</h3><p>用InternLM chat 7B基座模型，下载huggingface格式的模型，放到&#x2F;root&#x2F;ft-msagent目录下。</p>
<h3 id="数据集："><a href="#数据集：" class="headerlink" title="数据集："></a>数据集：</h3><p>去魔搭下载魔搭通用agent数据集：   </p>
<pre><code>apt install git-lfs
git clone https://www.modelscope.cn/datasets/iic/ms_agent.git
</code></pre>
<p>打开train_agent_react.jsonl ， 搜索图像识别，发现 image_recognition , ImageRecognitionAPI, image_classifier 字段</p>
<p>需要修改 ImageRecognitionAPI 为 ImageRecognition，<br>修改 image_recognition 为 ImageRecognition，<br>修改 image_classifier 为 ImageClassifier，    </p>
<p>用sed直接修改    </p>
<pre><code>sed -i &#39;s/ImageRecognitionAPI/ImageRecognition/g&#39; train_agent_react.jsonl
sed -i &#39;s/image_recognition/ImageRecognition/g&#39; train_agent_react.jsonl
sed -i &#39;s/image_classifier/ImageClassifier/g&#39; train_agent_react.jsonl
</code></pre>
<p>可以看一下是否修改成功 </p>
<pre><code>grep ImageClassifier train_agent_react.jsonl
grep ImageRecognition train_agent_react.jsonl
</code></pre>
<p>修改配置文件 &#x2F;root&#x2F;ft-msagent&#x2F;internlm_7b_qlora_msagent_react_e3_gpu8_copy.py<br><img src="/../asset_lagentsft/sft001.png"><br>看到这里你一定要疑问，为什么不把 data_path 指向 本地刚刚git clone下来的数据集路径呢？<br>其实是因为lagent有bug，会把本地数据集当作json格式的客户定制数据集，导致出错。<br>所以用一个work around的方法，就是先让lagent跑通，下载数据集到cache目录，然后进入lagent目录，再修改数据集文件。</p>
<p>先进入ft-msagent目录，运行</p>
<pre><code>xtuner train internlm_7b_qlora_msagent_react_e3_gpu8_copy.py --deepspeed deepspeed_zero2
</code></pre>
<p>跑一次train，不需要跑完，进入train就可以 CTRL+C 关掉</p>
<p>然后</p>
<pre><code>cd ~/.cache/modelscope/hub/datasets/iic/ms_agent/master/data_files 
</code></pre>
<p><img src="/../asset_lagentsft/sft003.png"></p>
<p>修改数据集文件 f0d878b6a17a5cec981643649ea383cc , 这个文件名具体看你们自己的环境，会变化的。</p>
<pre><code>sed -i &#39;s/ImageRecognitionAPI/ImageRecognition/g&#39; f0d878b6a17a5cec981643649ea383cc
sed -i &#39;s/image_recognition/ImageRecognition/g&#39; f0d878b6a17a5cec981643649ea383cc
sed -i &#39;s/image_classifier/ImageClassifier/g&#39; f0d878b6a17a5cec981643649ea383cc
</code></pre>
<p>再进入ft-msagent目录，运行</p>
<pre><code>xtuner train internlm_7b_qlora_msagent_react_e3_gpu8_copy.py --deepspeed deepspeed_zero2
</code></pre>
<p><img src="/../asset_lagentsft/sft002.png"><br>可以看到reusing dataset，然后开始训练。<br><img src="/../asset_lagentsft/sft004.png"><br>如果训练一个epoch想停掉，可以按CTRL+C，下次想继续训练，要看一下last_checkpoint文件，把最后一个checkpoint的路径复制到命令行参数里。如下：</p>
<pre><code>xtuner train internlm_7b_qlora_msagent_react_e3_gpu8_copy.py --deepspeed deepspeed_zero2 --resume /root/ft-msagent/work_dirs/internlm_7b_qlora_msagent_react_e3_gpu8_copy/epoch_2.pth
</code></pre>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>为了避免后续重复以上数据集繁琐操作，我在魔搭上创建了一个数据集，大家可以直接用这个数据集 szhowardhuang&#x2F;msagent_lite 。</p>
<p><a target="_blank" rel="noopener" href="https://modelscope.cn/datasets/szhowardhuang/msagent_lite">https://modelscope.cn/datasets/szhowardhuang/msagent_lite</a></p>
<p>配置文件修改如下，拷贝即可使用：</p>
<pre><code># Copyright (c) OpenMMLab. All rights reserved.
import torch
from bitsandbytes.optim import PagedAdamW32bit
from mmengine.dataset import DefaultSampler
from mmengine.hooks import (CheckpointHook, DistSamplerSeedHook, IterTimerHook,
                            LoggerHook, ParamSchedulerHook)
from mmengine.optim import AmpOptimWrapper, CosineAnnealingLR
from modelscope.msdatasets import MsDataset
from peft import LoraConfig
from transformers import (AutoModelForCausalLM, AutoTokenizer,
                        BitsAndBytesConfig)

from xtuner.dataset import process_ms_dataset
from xtuner.dataset.collate_fns import default_collate_fn
from xtuner.dataset.map_fns import (msagent_react_map_fn,
                                    template_map_fn_factory)
from xtuner.engine import DatasetInfoHook, EvaluateChatHook
from xtuner.model import SupervisedFinetune
from xtuner.utils import PROMPT_TEMPLATE


#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Model
pretrained_model_name_or_path = &#39;/root/ft-msagent/internlm-chat-7b&#39;

# Data
data_path = &#39;szhowardhuang/msagent_lite&#39;
prompt_template = PROMPT_TEMPLATE.default
max_length = 2048
pack_to_max_length = False

# Scheduler &amp; Optimizer
batch_size = 4  # per_device
accumulative_counts = 1
dataloader_num_workers = 2
max_epochs = 3
optim_type = PagedAdamW32bit
lr = 2e-4
betas = (0.9, 0.999)
weight_decay = 0
max_norm = 1  # grad clip

# Evaluate the generation performance during the training
evaluation_freq = 500
SYSTEM = (
    &#39;你是一个可以调用外部工具的助手，可以使用的工具包括：\n&#39;
    &quot;&#123;&#123;\'ImageRecognition\': \'用来进行图像识别的API。\\n"
        '当你需要对于一个图片进行识别时，可以使用这个API。\\n'
        "输入应该是一张图片文件的路径，或者是图片的URL。\\n\\n\',"
        "\'ImageClassifier\': \"用来进行图像分类的API，\\n"
        "当你需要对于一个图片进行分类时，可以使用这个API。\\n"
        "输入应该是一张图片文件的路径，或者是图片的URL。\\n\"&#125;&#125;\n&quot;
    &#39;如果使用工具请遵循以下格式回复：\n```\n&#39;
    &#39;Thought:思考你当前步骤需要解决什么问题，是否需要使用工具\n&#39;
    &quot;Action:工具名称，你的工具必须从 [[\&#39;ImageRecognition\&#39;, \&#39;ImageClassifier\&#39;]] 选择&quot;
    &#39;\nAction Input:工具输入参数\n```\n工具返回按照以下格式回复：\n&#39;
    &#39;```\nResponse:调用工具后的结果\n```&#39;
    &#39;\n如果你已经知道了答案，或者你不需要工具，请遵循以下格式回复\n```&#39;
    &#39;\nThought:给出最终答案的思考过程\nFinal Answer:最终答案\n```\n开始!\n&#39;)
evaluation_inputs = [&#39;识别图片中的物体类别？&#39;]

#######################################################################
#                      PART 2  Model &amp; Tokenizer                      #
#######################################################################
tokenizer = dict(
    type=AutoTokenizer.from_pretrained,
    pretrained_model_name_or_path=pretrained_model_name_or_path,
    trust_remote_code=True,
    padding_side=&#39;right&#39;)

model = dict(
    type=SupervisedFinetune,
    llm=dict(
        type=AutoModelForCausalLM.from_pretrained,
        pretrained_model_name_or_path=pretrained_model_name_or_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        quantization_config=dict(
            type=BitsAndBytesConfig,
            load_in_4bit=True,
            load_in_8bit=False,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type=&#39;nf4&#39;)),
    lora=dict(
        type=LoraConfig,
        r=64,
        lora_alpha=16,
        lora_dropout=0.1,
        bias=&#39;none&#39;,
        task_type=&#39;CAUSAL_LM&#39;))

#######################################################################
#                      PART 3  Dataset &amp; Dataloader                   #
#######################################################################
train_dataset = dict(
    type=process_ms_dataset,
    dataset=dict(type=MsDataset.load, dataset_name=data_path, split=&#39;train&#39;),
    tokenizer=tokenizer,
    max_length=max_length,
    dataset_map_fn=msagent_react_map_fn,
    template_map_fn=dict(
        type=template_map_fn_factory, template=prompt_template),
    remove_unused_columns=True,
    shuffle_before_pack=True,
    pack_to_max_length=pack_to_max_length)

train_dataloader = dict(
    batch_size=batch_size,
    num_workers=dataloader_num_workers,
    dataset=train_dataset,
    sampler=dict(type=DefaultSampler, shuffle=True),
    collate_fn=dict(type=default_collate_fn))

#######################################################################
#                    PART 4  Scheduler &amp; Optimizer                    #
#######################################################################
# optimizer
optim_wrapper = dict(
    type=AmpOptimWrapper,
    optimizer=dict(
        type=optim_type, lr=lr, betas=betas, weight_decay=weight_decay),
    clip_grad=dict(max_norm=max_norm, error_if_nonfinite=False),
    accumulative_counts=accumulative_counts,
    loss_scale=&#39;dynamic&#39;,
    dtype=&#39;float16&#39;)

# learning policy
# More information: https://github.com/open-mmlab/mmengine/blob/main/docs/en/tutorials/param_scheduler.md  # noqa: E501
param_scheduler = dict(
    type=CosineAnnealingLR,
    eta_min=0.0,
    by_epoch=True,
    T_max=max_epochs,
    convert_to_iter_based=True)

# train, val, test setting
train_cfg = dict(by_epoch=True, max_epochs=max_epochs, val_interval=1)

#######################################################################
#                           PART 5  Runtime                           #
#######################################################################
# Log the dialogue periodically during the training process, optional
custom_hooks = [
    dict(type=DatasetInfoHook, tokenizer=tokenizer),
    dict(
        type=EvaluateChatHook,
        tokenizer=tokenizer,
        every_n_iters=evaluation_freq,
        evaluation_inputs=evaluation_inputs,
        system=SYSTEM,
        prompt_template=prompt_template)
]

# configure default hooks
default_hooks = dict(
    # record the time of every iteration.
    timer=dict(type=IterTimerHook),
    # print log every 100 iterations.
    logger=dict(type=LoggerHook, interval=10),
    # enable the parameter scheduler.
    param_scheduler=dict(type=ParamSchedulerHook),
    # save checkpoint per epoch.
    checkpoint=dict(type=CheckpointHook, interval=1),
    # set sampler seed in distributed evrionment.
    sampler_seed=dict(type=DistSamplerSeedHook),
)

# configure environment
env_cfg = dict(
    # whether to enable cudnn benchmark
    cudnn_benchmark=False,
    # set multi process parameters
    mp_cfg=dict(mp_start_method=&#39;fork&#39;, opencv_num_threads=0),
    # set distributed parameters
    dist_cfg=dict(backend=&#39;nccl&#39;),
)

# set visualizer
visualizer = None

# set log level
log_level = &#39;INFO&#39;

# load from which checkpoint
load_from = None

# whether to resume training from the loaded checkpoint
resume = False

# Defaults to use random seed and disable `deterministic`
randomness = dict(seed=None, deterministic=False)
</code></pre>
<h3 id="模型转换"><a href="#模型转换" class="headerlink" title="模型转换"></a>模型转换</h3><p>将PTH模型转换成HuggingFace模型，即生成Adapter文件夹, hf文件夹即为我们平时所理解的所谓 “LoRA模型文件”<br>    mkdir hf<br>    export MKL_SERVICE_FORCE_INTEL&#x3D;1<br>    xtuner convert pth_to_hf internlm_7b_qlora_msagent_react_e3_gpu8_copy.py .&#x2F;work_dirs&#x2F;internlm_7b_qlora_msagent_react_e3_gpu8_copy&#x2F;epoch_2.pth .&#x2F;hf</p>
<h3 id="模型测试"><a href="#模型测试" class="headerlink" title="模型测试"></a>模型测试</h3><p>先下载并安装VisionAgent（即lagent）<br>    <a target="_blank" rel="noopener" href="https://github.com/szhowardhuang/VisionAgent.git">https://github.com/szhowardhuang/VisionAgent.git</a><br>    cd VisionAgent<br>    git checkout a93162536f04679ace6572365a1884e584b4cb03<br>    pip install -e .<br>然后安装 mmdetection<br>    pip install -U openmim<br>    mim install mmengine<br>    mim install “mmcv&gt;&#x3D;2.0.0”<br>    mim install mmdet<br>注释掉 &#x2F;root&#x2F;xtuner019&#x2F;xtuner&#x2F;xtuner&#x2F;tools&#x2F;chat.py 的139行<br><img src="/../asset_lagentsft/sft005.png"></p>
<pre><code>export SERPER_API_KEY=abcdefg
xtuner chat ./internlm-chat-7b --adapter hf --lagent
</code></pre>
<h3 id="将-HuggingFace-adapter-合并到大语言模型"><a href="#将-HuggingFace-adapter-合并到大语言模型" class="headerlink" title="将 HuggingFace adapter 合并到大语言模型"></a>将 HuggingFace adapter 合并到大语言模型</h3><pre><code>xtuner convert merge ./internlm-chat-7b ./hf ./merged --max-shard-size 2GB
</code></pre>
<h3 id="用VisionAgent测试合并后的模型"><a href="#用VisionAgent测试合并后的模型" class="headerlink" title="用VisionAgent测试合并后的模型"></a>用VisionAgent测试合并后的模型</h3><p>修改一下 &#x2F;root&#x2F;code&#x2F;VisionAgent&#x2F;examples&#x2F;react_web_demo.py的92行<br><img src="/../asset_lagentsft/sft006.png"><br>    pip install modelscope&#x3D;&#x3D;1.9.5<br>    pip install transformers&#x3D;&#x3D;4.35.2<br>    pip install streamlit&#x3D;&#x3D;1.24.0<br>    pip install sentencepiece&#x3D;&#x3D;0.1.99<br>    pip install accelerate&#x3D;&#x3D;0.24.1<br>    streamlit run &#x2F;root&#x2F;code&#x2F;lagent&#x2F;examples&#x2F;react_web_demo.py –server.address 127.0.0.1 –server.port 6006</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/01/28/InternLm-training/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/01/28/InternLm-training/" class="post-title-link" itemprop="url">书生-浦语实战训练营</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-01-28 23:00:16" itemprop="dateCreated datePublished" datetime="2024-01-28T23:00:16+08:00">2024-01-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-01-29 21:59:29" itemprop="dateModified" datetime="2024-01-29T21:59:29+08:00">2024-01-29</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>1.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>历经2周的InternLM实战训练营，我收获颇丰。</p>
<p>本次训练营的主要内容是：</p>
<ul>
<li>书生·浦语大模型全链路开源体系</li>
<li>书生·浦语大模型 Demo</li>
<li>基于 InternLM 和 LangChain 搭建知识库</li>
<li>XTuner 大模型单卡低成本微调实战</li>
<li>LMDeploy 大模型量化部署实践</li>
<li>OpenCompass 大模型评测</li>
</ul>
<p>我将每一课的内容和作业都发布在GitHub上，欢迎大家参与讨论和学习。</p>
<h2 id="课程内容"><a href="#课程内容" class="headerlink" title="课程内容"></a>课程内容</h2><h4 id="第1课"><a href="#第1课" class="headerlink" title="第1课"></a>第1课</h4><p>笔记 <a target="_blank" rel="noopener" href="https://github.com/InternLM/tutorial/discussions/36#discussioncomment-8013121">https://github.com/InternLM/tutorial/discussions/36#discussioncomment-8013121</a></p>
<h4 id="第2课"><a href="#第2课" class="headerlink" title="第2课"></a>第2课</h4><p>笔记 <a target="_blank" rel="noopener" href="https://github.com/InternLM/tutorial/discussions/37#discussioncomment-8035063">https://github.com/InternLM/tutorial/discussions/37#discussioncomment-8035063</a>	<br>作业 <a target="_blank" rel="noopener" href="https://github.com/InternLM/tutorial/discussions/92#discussioncomment-8035219">https://github.com/InternLM/tutorial/discussions/92#discussioncomment-8035219</a></p>
<h4 id="第3课"><a href="#第3课" class="headerlink" title="第3课"></a>第3课</h4><p>笔记 <a target="_blank" rel="noopener" href="https://github.com/InternLM/tutorial/discussions/38#discussioncomment-8061447">https://github.com/InternLM/tutorial/discussions/38#discussioncomment-8061447</a>	<br>作业 <a target="_blank" rel="noopener" href="https://github.com/InternLM/tutorial/discussions/93#discussioncomment-8081121">https://github.com/InternLM/tutorial/discussions/93#discussioncomment-8081121</a></p>
<h4 id="第4课"><a href="#第4课" class="headerlink" title="第4课"></a>第4课</h4><p>笔记 <a target="_blank" rel="noopener" href="https://github.com/InternLM/tutorial/discussions/39#discussioncomment-8108951">https://github.com/InternLM/tutorial/discussions/39#discussioncomment-8108951</a>	<br>作业 <a target="_blank" rel="noopener" href="https://github.com/InternLM/tutorial/discussions/94#discussioncomment-8116585">https://github.com/InternLM/tutorial/discussions/94#discussioncomment-8116585</a></p>
<h4 id="第5课"><a href="#第5课" class="headerlink" title="第5课"></a>第5课</h4><p>笔记 <a target="_blank" rel="noopener" href="https://github.com/InternLM/tutorial/discussions/40#discussioncomment-8122800">https://github.com/InternLM/tutorial/discussions/40#discussioncomment-8122800</a>	<br>作业 <a target="_blank" rel="noopener" href="https://github.com/InternLM/tutorial/discussions/95#discussioncomment-8122839">https://github.com/InternLM/tutorial/discussions/95#discussioncomment-8122839</a></p>
<h4 id="第6课"><a href="#第6课" class="headerlink" title="第6课"></a>第6课</h4><p>笔记 <a target="_blank" rel="noopener" href="https://github.com/InternLM/tutorial/discussions/41#discussioncomment-8217329">https://github.com/InternLM/tutorial/discussions/41#discussioncomment-8217329</a>	<br>作业 <a target="_blank" rel="noopener" href="https://github.com/InternLM/tutorial/discussions/96#discussioncomment-8217311">https://github.com/InternLM/tutorial/discussions/96#discussioncomment-8217311</a></p>
<p>后续还要在通用算力平台实践一把，欢迎大家关注！</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/01/19/make-react/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/01/19/make-react/" class="post-title-link" itemprop="url">构建 ReAct AI 代理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-01-19 23:00:16" itemprop="dateCreated datePublished" datetime="2024-01-19T23:00:16+08:00">2024-01-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-01-20 16:03:59" itemprop="dateModified" datetime="2024-01-20T16:03:59+08:00">2024-01-20</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>8.7k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>16 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>人工智能代理是一种系统或软件，它可以根据其所处环境和收到的输入，自主执行操作或任务以实现特定目标。人工智能代理旨在做出决策并采取行动。</p>
<p>人工智能代理可能是人工智能改变我们日常生活方式的下一个创新方式。借助人工智能代理，我们可以完全委托一些工作流程并更快地获得结果。</p>
<p>人工智能代理面临的挑战是推理和可靠性。最近出现的大多数人工智能代理都基于 LLMs。但 LLMs 本身并不是推理引擎。只有 GPT-4 在伪造推理方面相当先进。</p>
<p>为了提高 LLMs 的推理能力，已经发明了几种提示策略，包括但不限于思想链、思想树和 ReAct（推理和行动）。</p>
<p>本文将向您展示如何从0 开始实现 ReAct 人工智能代理。</p>
<p>ReAct 是一种将推理和行动相结合的策略，作为提高基于 LLM 的代理的整体可靠性和事实充分性的方法，因为采取行动并从行动中获取反馈可以帮助奠定 LLM 的回应。</p>
<h3 id="创建代理"><a href="#创建代理" class="headerlink" title="创建代理"></a>创建代理</h3><p>第一步是创建代理类。代理必须由一个充当其大脑的LLM来定义。代理还应该能够访问一组工具，这些工具将帮助它采取行动并从这些行动中接收反馈。最后，代理将通过经历“反应链”来解决每个问题解决任务，包括以下步骤：思考、选择行动和工具、确定行动的输入、采取行动并观察结果、确定是否结束推理链。</p>
<p><img src="/../asset_makereact/1__Rs9OHRaD7xVPKjkhmtsMg.webp"></p>
<p>在下面的 Agent 类定义中，您会注意到我使用 OpenAI 的 LLM 作为我的代理的 Brain。为此，我使用了我几个月前开发的 anonLLM 库。anonLLM 库的核心目标是从 LLM API 中隐藏个人身份信息，但在这里我主要会将它用作 OpenAI API 的 Python 客户端的包装器。使用 anonLLM 的优势源于它生成结构化 JSON 输出的能力。</p>
<p><a target="_blank" rel="noopener" href="https://github.com/fsndzomga/anonLLM">https://github.com/fsndzomga/anonLLM</a></p>
<p>为什么它很重要？因为我们需要一种方法来在 ReAct 链的步骤之间传递信息。例如，在定义操作的参数时，您希望能够将这些参数直接传递给操作函数。但是，LLM 会生成格式不可预测的文本，这使得使用正则表达式解析来利用结果变得很困难。生成结构化的 JSON 输出使我们能够简化代理堆栈不同组件之间的通信过程。anonLLM 库将为我们处理这些事情。</p>
<p>要使用 anonLLM 生成结构化的 JSON 输出，我们只需要定义并传递一个 BaseModel 类作为 output_format，如下所示：</p>
<pre><code>from anonLLM.llm import OpenaiLanguageModel as brain
from pydantic import BaseModel

class ReactEnd(BaseModel):
    stop: bool
    final_answer: str


class ToolChoice(BaseModel):
    tool_name: str
    reason_of_choice: str

response = brain.generate(prompt=prompt, output_format=ToolChoice)

check_final = brain.generate(prompt=f&quot;Is &#123;self.background_info()&#125; enough to finally answer to this request: &#123;self.messages[0]&#125;&quot;,
                output_format=ReactEnd)
</code></pre>
<p>这是我首次定义代理类的方式。代理基本上会循环遍历 ReAct 链，直到找到用户请求的答案。这是第一个实现，我将在未来的文章中使其更加健壮。</p>
<pre><code>from anonLLM.llm import OpenaiLanguageModel as Brain
from typing import Callable
from dotenv import load_dotenv
from pydantic import BaseModel
import inspect
import datetime
import wikipedia
import os

load_dotenv()

class Agent:
    def __init__(self, api_key: str = os.environ.get(&#39;OPENAI_API_KEY&#39;), model: str = &#39;gpt-4&#39;) -&gt; None:
        self.api_key = api_key
        self.model = model
        self.brain = Brain(api_key=api_key,
                        model=model,
                        anonymize=False)  # I know it violates the dependency injection rule,
                                            # but it is not a big issue here.
        self.tools = []

        self.messages = []

        self.request = &quot;&quot;

        self.token_count = 0

        self.token_limit = 5000

    def add_tool(self, tool: Tool) -&gt; None:
        self.tools.append(tool)

    def append_message(self, message):
        self.messages.append(message)
        self.token_count += len(message)

        # Check if token_count exceeds the limit
        while self.token_count &gt; self.token_limit and len(self.messages) &gt; 1:
            # Remove messages from the end until token_count is within the limit
            removed_message = self.messages.pop(1)  # Keep the first message, remove the second one
            self.token_count -= len(removed_message)

    @staticmethod
    def extract_first_nested_dict(data_dict):
        for key, value in data_dict.items():
            if isinstance(value, dict):
                return value
        return &#123;&#125;

    def background_info(self) -&gt; str:
        return f&quot;Here are your previous think steps: &#123;self.messages[1:]&#125;&quot; if len(self.messages) &gt; 1 else &quot;&quot;

    def think(self) -&gt; None:

        prompt = f&quot;&quot;&quot;Answer the following request as best you can: &#123;self.request&#125;.
                    &#123;self.background_info()&#125;
                    First think about what to do. What action to take first if any.
                    Here are the tools at your disposal: &#123;[tool.name for tool in self.tools]&#125;&quot;&quot;&quot;

        self.append_message(prompt)

        response = self.brain.generate(prompt=prompt, max_tokens=100)

        print(f&quot;Thought: &#123;response&#125;&quot;)

        self.append_message(response)

        self.choose_action()

    def choose_action(self) -&gt; None:
        prompt = f&quot;&quot;&quot;To Answer the following request as best you can: &#123;self.request&#125;.
                    &#123;self.background_info()&#125;
                    Choose the tool to use if need be. The tool should be among:
                    &#123;[tool.name for tool in self.tools]&#125;.
                    &quot;&quot;&quot;
        self.append_message(prompt)

        response = self.brain.generate(prompt=prompt, output_format=ToolChoice)

        print(f&quot;&quot;&quot;Action: I should use this tool: &#123;response[&quot;tool_name&quot;]&#125;.
            &#123;response[&quot;reason_of_choice&quot;]&#125;&quot;&quot;&quot;)

        self.append_message(response)

        tool = [tool for tool in self.tools if tool.name == response[&quot;tool_name&quot;]].pop()

        self.action(tool)

    def action(self, tool: Tool) -&gt; None:
        prompt = f&quot;&quot;&quot;To Answer the following request as best you can: &#123;self.request&#125;.
                    &#123;self.background_info()&#125;
                    Determine the inputs to send to the tool: &#123;tool.name&#125;
                    Given that the source code of the tool function is: &#123;inspect.getsource(tool.func)&#125;.
                    &quot;&quot;&quot;
        self.append_message(prompt)

        parameters = inspect.signature(tool.func).parameters

        class DynamicClass(BaseModel):
            pass

        for name, param in parameters.items():
            # Setting default value if it exists, else None
            default_value = param.default if param.default is not inspect.Parameter.empty else None
            setattr(DynamicClass, name, (param.annotation, default_value))

        response = self.brain.generate(prompt=prompt, output_format=DynamicClass)

        self.append_message(response)

        input_parameters = self.extract_first_nested_dict(response)

        action_result = tool.func(**input_parameters)

        self.append_message(f&quot;Results of action: &#123;action_result&#125;&quot;)

        self.observation()

    def observation(self) -&gt; None:
        prompt = f&quot;Observation:&#123;self.messages[-1]&#125;.&quot;
        self.append_message(prompt)

        check_final = self.brain.generate(prompt=f&quot;Is &#123;self.background_info()&#125; enough to finally answer to this request: &#123;self.messages[0]&#125;&quot;,
                output_format=ReactEnd)

        if check_final[&quot;stop&quot;]:
            print(&quot;Thought: I now know the final answer. \n&quot;)
            prompt = f&quot;&quot;&quot;Give the final answer the following request: &#123;self.request&#125;.
                    given &#123;self.background_info()&#125;
                    &quot;&quot;&quot;
            print(f&quot;Final Answer: &#123;self.brain.generate(prompt=prompt)&#125;&quot;)
        else:
            self.think()


    def react(self, input: str) -&gt; str:
        self.append_message(input)
        self.request = input
        self.think()
</code></pre>
<p>Agent 类旨在与语言模型（称为 Brain ）交互、处理请求并利用各种工具来生成响应。以下是其功能的细分：</p>
<p>初始化 ( <strong>init</strong> ):</p>
<ul>
<li>代理使用 API 密钥和模型规范（默认为“gpt-4”）进行初始化。</li>
<li>self.brain 是 Brain 的一个实例，它似乎是一个语言模型接口（可能是针对 OpenAI 的 GPT 模型）。</li>
<li>代理维护一个 tools 列表（ Tool 类的实例）， messages 用于跟踪对话历史记录，并管理令牌以遵守 API 限制。</li>
</ul>
<p>添加工具 ( add_tool ):</p>
<ul>
<li>使用此方法将工具添加到代理。每个工具都是 Tool 类的实例，封装特定功能（如维基百科搜索、计算等）。</li>
</ul>
<p>消息处理 ( append_message ):</p>
<ul>
<li>此方法将消息添加到代理的历史记录中，管理令牌计数以保持在限制范围内。如果超过令牌限制，则会删除较旧的消息。</li>
</ul>
<p>对输入做出反应 ( react ):</p>
<ul>
<li>这是与代理交互的入口点。它接收输入请求，将其附加到消息中，设置请求并启动思考过程。</li>
</ul>
<p>思考过程 ( think ):</p>
<ul>
<li>代理根据当前请求和背景信息（之前的消息）生成一个思考过程。它使用 self.brain 为提示生成一个响应。</li>
</ul>
<p>动作选择 ( choose_action ):</p>
<ul>
<li>经过思考，代理根据请求和背景信息选择一个动作（要使用的工具）。通过使用 self.brain 生成响应来做出此选择，该响应格式化为 ToolChoice 。</li>
</ul>
<p>执行操作 ( action ):</p>
<ul>
<li>代理通过调用所选工具的 act 方法来执行所选操作。它首先通过检查工具函数的参数来确定该函数所需的输入。</li>
</ul>
<p>观察和响应 ( observation ):</p>
<ul>
<li>执行操作后，代理观察结果并决定是否已获得足够的信息来提供最终答案。如果没有，则返回思考过程。</li>
</ul>
<h3 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h3><p>定义代理后，我需要定义代理将使用的某些工具。工具基本上是一个函数，表示将执行的操作，例如执行计算、在维基百科上进行搜索或检索当前日期。</p>
<p>以下是工具类的定义：</p>
<pre><code>class Tool:
    def __init__(self, name: str, func: Callable) -&gt; None:
        self.name = name
        self.func = func

    def act(self, **kwargs) -&gt; str:
        return self.func(**kwargs)
</code></pre>
<p>以下是具体定义工具并将其添加到代理中的方法，然后进行测试：</p>
<pre><code># Equivalent of the perform_calculation function
def perform_calculation(operation, a, b):
    # Validating the operation
    if operation not in [&#39;add&#39;, &#39;subtract&#39;, &#39;multiply&#39;, &#39;divide&#39;]:
        return f&quot;Invalid operation: &#123;operation&#125;, should be among [&#39;add&#39;, &#39;subtract&#39;, &#39;multiply&#39;, &#39;divide&#39;]&quot;

    if operation == &#39;add&#39;:
        return a + b
    elif operation == &#39;subtract&#39;:
        return a - b
    elif operation == &#39;multiply&#39;:
        return a * b
    elif operation == &#39;divide&#39;:
        if b == 0:
            return &quot;Division by zero&quot;
        return a / b


# Equivalent of the search_wikipedia function
def search_wikipedia(search_query):
    # Fetch the page content
    page = wikipedia.page(search_query)

    # Extract the text
    text = page.content

    # Print and return the first 100 characters
    return text[:300]


# Equivalent of the date_req function
def date_of_today():
    return datetime.date.today()

# Creating instances of the Tool class
wikipedia_search_tool = Tool(&quot;WikipediaSearch&quot;, search_wikipedia)
calculator_tool = Tool(&quot;Calculator&quot;, perform_calculation)
date_request_tool = Tool(&quot;Date_of_request&quot;, date_of_today)

# Creating an Agent

agent = Agent()

agent.add_tool(wikipedia_search_tool)
agent.add_tool(calculator_tool)
agent.add_tool(date_request_tool)

agent.react(&quot;What is the double of barack obama&#39;s age?&quot;)
</code></pre>
<p>最后，以下是代理的推理过程：</p>
<pre><code>Thought: First, I would use the &#39;WikipediaSearch&#39; tool to find out Barack Obama&#39;s current age. Then, I would use the &#39;Calculator&#39; tool to double the age I found.

Action: I should use this tool: WikipediaSearch. I need to find out Barack Obama&#39;s current age.

Thought: From the Wikipedia search, we found out that Barack Obama was born on August 4, 1961. To determine his current age, we need to know the current date. Therefore, I will use the &#39;Date_of_request&#39; tool. After finding out his current age, I will use the &#39;Calculator&#39; tool to find the double of his age.

Action: I should use this tool: Date_of_request. I need to find out the current date to calculate Barack Obama&#39;s age.

Thought: I now know the final answer. 

Final Answer: Barack Obama was born on August 4, 1961. So as of January 3, 2024, he would be 62 years old. Therefore, the double of his age would be 124 years.
</code></pre>
<p>现在，可以做更多的事情来增强此 AI 代理的推理能力，例如在代理内部实现 RAG，让它可以访问互联网等等。本文的目标只是让您了解可以做些什么来提高 AI 代理的推理能力。</p>
<h3 id="完整代码"><a href="#完整代码" class="headerlink" title="完整代码"></a>完整代码</h3><p><a target="_blank" rel="noopener" href="https://github.com/fsndzomga/react-ai-agent">https://github.com/fsndzomga/react-ai-agent</a></p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p><a target="_blank" rel="noopener" href="https://medium.com/gitconnected/building-a-react-ai-agent-from-scratch-using-python-3adc2030b198">https://medium.com/gitconnected/building-a-react-ai-agent-from-scratch-using-python-3adc2030b198</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/01/18/exploring-llm-agents/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/01/18/exploring-llm-agents/" class="post-title-link" itemprop="url">探索大型语言模型代理的世界</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-01-18 16:00:16 / 修改时间：16:27:33" itemprop="dateCreated datePublished" datetime="2024-01-18T16:00:16+08:00">2024-01-18</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.6k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p><img src="/../asset_exploringagent/0_8bpi__c0zKetL_WY.png"></p>
<p>大型语言模型代理，通常称为 LLM 代理，标志着语言模型技术领域的一项重大进步。这些代理不仅擅长处理和生成自然语言文本，而且还能够利用各种工具，例如计算器和代码执行。可以将它们视为一种高度智能的多功能工具，它将大型语言模型 (LLMs) 的强大功能与特定工具的实用性相结合，以便独立执行各种任务。   </p>
<h3 id="了解-LLM-代理"><a href="#了解-LLM-代理" class="headerlink" title="了解 LLM 代理"></a>了解 LLM 代理</h3><p>LLMs 在其基本形式中，非常善于理解和响应提示。然而，代理通过授权 LLMs 来决定采取哪些行动，从而使代理更进一步，从而赋予它们一定程度的自主权。简单来说，代理是 LLM 链（即 LLMs 的序列）和工具的融合。与简单的链不同，代理能够决定哪种工具或操作最适合特定任务。这种决策过程由 ReACT 等方法指导，在大语言模型中提供了一种自治机制。</p>
<h3 id="使用代理的优势"><a href="#使用代理的优势" class="headerlink" title="使用代理的优势"></a>使用代理的优势</h3><p>代理为表格带来了大量好处。其中一个最重要的优势是任务自动化，它有可能通过提高效率来彻底改变各个行业。 LLM 代理可以处理重复性和常规性任务，从而使人类工人能够专注于更复杂的任务。</p>
<p>此外，这些代理对残疾人士尤其有益。例如，它们可以帮助打破沟通障碍、执行体力要求高的任务并培养独立感。计算器的类比在这里很合适。正如计算器可以加快数学计算的速度，让人类可以专注于更抽象和复杂的问题一样，LLM 代理可以接管特定任务，使我们能够专注于更具挑战性的任务。</p>
<h3 id="潜在风险-LLM-代理"><a href="#潜在风险-LLM-代理" class="headerlink" title="潜在风险 LLM 代理"></a>潜在风险 LLM 代理</h3><p>然而，承认与LLM代理相关的潜在风险非常重要。主要担忧在于可能滥用此技术进行有害活动。这些代理看似无限的力量也引发了对不受监管的、类似科幻小说的场景的恐惧，在该场景中，代理可能会接管一切。</p>
<p>许多人担心的另一个问题是工作流失。必须记住，虽然这些技术可能会改变某些工作的性质，但它们也会创造新的机会，就像以前的技术进步所做的那样。</p>
<p>人们还担心LLM代理会导致 AGI 或通用人工智能的发展，即能够理解、学习、适应和应用各种任务知识的机器。</p>
<h3 id="开发代理的框架"><a href="#开发代理的框架" class="headerlink" title="开发代理的框架"></a>开发代理的框架</h3><p>有许多可供开发人员创建代理的框架。其中包括 LangChain、Hugging Face、BabyAGI 和 AutoGPT。<br>这些框架中的每一个都提供独特的功能，旨在简化在各种应用程序中构建和部署LLMs 的过程。</p>
<h3 id="试一试"><a href="#试一试" class="headerlink" title="试一试"></a>试一试</h3><p>您可以使用以下代码试一试。它利用 langchain 来初始化和运行一个LLM 代理。</p>
<pre><code># Import libraries
from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType
from langchain.llms import OpenAI

# Initialize OpenAI LLM model
llm = OpenAI(temperature=0)

# Load tools that the agent can decide to use
# serpapi allows the agent to search the web
# llm-math allows the agent to use a calculator
tools = load_tools([&quot;serpapi&quot;, &quot;llm-math&quot;], llm=llm)

# Initialize the LLM agent
agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)

# Run the LLM Agent
agent.run(&quot;Who is Leo DiCaprio&#39;s girlfriend? What is her current age raised to the 0.43 power?&quot;)
</code></pre>
<p>输出信息如下：</p>
<pre><code>&gt; Entering new AgentExecutor chain...
 I need to find out who Leo DiCaprio&#39;s girlfriend is and then calculate her age raised to the 0.43 power.
Action: Search
Action Input: &quot;Leo DiCaprio girlfriend&quot;
Observation: Camila Morrone
Thought: I need to find out Camila Morrone&#39;s age
Action: Search
Action Input: &quot;Camila Morrone age&quot;
Observation: 25 years
Thought: I need to calculate 25 raised to the 0.43 power
Action: Calculator
Action Input: 25^0.43
Observation: Answer: 3.991298452658078

Thought: I now know the final answer
Final Answer: Camila Morrone is Leo DiCaprio&#39;s girlfriend and her current age raised to the 0.43 power is 3.991298452658078.

&gt; Finished chain.

&quot;Camila Morrone is Leo DiCaprio&#39;s girlfriend and her current age raised to the 0.43 power is 3.991298452658078.&quot;
</code></pre>
<h3 id="LLM-代理的当前用途"><a href="#LLM-代理的当前用途" class="headerlink" title="LLM 代理的当前用途"></a>LLM 代理的当前用途</h3><p>目前，LLM 代理的应用并不广泛，这主要是因为让代理正常运行所涉及的复杂性和挑战。如果代理中的一个组件或动作失败，则由于错误传播，整个代理都可能失败。</p>
<p>LLM 代理最常见的设想应用之一是个人数字助理，它能够执行各种任务，例如自动且准确地回复电子邮件、安排约会等。</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>尽管LLM代理领域仍处于起步阶段，但它所承诺的潜在应用和好处使其成为激动人心的技术领域。随着我们实施措施以确保其道德和负责任地使用，我们可以利用这项技术得到更大的利益。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p><a target="_blank" rel="noopener" href="https://medium.com/@minh.hoque/exploring-the-world-of-large-language-model-agents-18a32598829e">https://medium.com/@minh.hoque/exploring-the-world-of-large-language-model-agents-18a32598829e</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/01/18/it-for-ai-swe/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/01/18/it-for-ai-swe/" class="post-title-link" itemprop="url">商业模式与 IT 组织：做好迎接AI冲击的准备！</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-01-18 15:00:16 / 修改时间：15:07:40" itemprop="dateCreated datePublished" datetime="2024-01-18T15:00:16+08:00">2024-01-18</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>16k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>30 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>许多读者可能听说过克莱顿·克里斯坦森的开创性著作《创新者的窘境》。我想在这里引用他书中的一段内容，来说明人工智能软件工程 (AI-SWE) 将如何站稳脚跟。</p>
<p>克莱顿·克里斯坦森将创新分为两种类型：维持型创新和颠覆型创新。维持型创新涉及增强现有技术，新进入市场的企业通常难以与老牌企业竞争,因为他们能够轻松将此类改进集成到现有产品中。另一方面，当产品变得过于复杂而无法满足其市场需求时，就会出现颠覆性创新，从而导致市场“供过于求”。这为新创新创造了机会，尽管这些创新可能不如当前最先进的产品，但它们以更低的成本或领先产品无法实现的方式提供功能。</p>
<p>好的，现在您已经复习了这个概念，请记住这一点！我们很快会重新讨论它，但首先让我们介绍一个新兴组织，即人工智能原生公司。</p>
<h3 id="当心人工智能原生公司的崛起"><a href="#当心人工智能原生公司的崛起" class="headerlink" title="当心人工智能原生公司的崛起"></a>当心人工智能原生公司的崛起</h3><p>就像我们看到云原生公司随着云计算的日益普及和经济性而出现，并开始进入并改变格局一样，我们现在将看到人工智能原生公司形成。</p>
<p>随着生成式人工智能使软件工程变得越来越容易，新公司将能够在不雇佣大量昂贵的软件开发人员的情况下创建软件。事实上，现在出现了一个新职业，称为人工智能工程师 [16]。它将在高于实际代码开发的层面上工作，并将重点放在训练模型和创建将由这些人工智能功能创建的软件交付到最终用户手中的粘合剂上。<br><img src="/../asset_aiswe/1_KMiFS4QBNm6jYHnzImbooA.webp"></p>
<p>人工智能原生公司不必处理官僚主义和摩擦，这些官僚主义和摩擦不可避免地会出现在遗留软件流程或过去做出的较旧架构、基础设施、平台、框架或语言决策中。他们将能够使用最优秀、最普遍和最经济的可用模式构建和部署软件。更重要的是，如果他们做出了错误的技术决策，不用担心，只需废弃所有内容并快速重新构建即可！</p>
<p>以人工智能为基础的公司不会像硅谷常见的做法那样，以隐身模式花费 9 到 18 个月。从原型到最小可行产品 (MVP) 的所有新技术开发阶段都将以周为单位衡量。对于风投公司来说，为这些公司提供资金将变得更加容易和便宜，因为当有前景的产品变成废品时，他们能够轻松地调整方向。这些公司将开始渗透到从税收到医疗保健管理的所有行业，并且要么开始威胁现有公司，要么被收购（或考虑到员工人数如此之少而被收购）。更重要的是，它们会迅速缩短客户眼中产品上市的时间，这几乎会迫使现有的成功公司进行自我颠覆。</p>
<p>因此，回到克莱顿关于颠覆性技术的概念……AI-SWE 在开始时将逊于人类软件工程，但没有什么能够与它的惊人速度和低成本相竞争。客户将进行成本效益评估，选择绕过花里胡哨的功能，选择更简单但更便宜的替代方案，这些替代方案能够快速发展。所有的人工智能原生公司必须确保的是不损害可靠性和安全性等基本要素，并提供客户正在寻找的核心价值。</p>
<h3 id="这如何影响-IT"><a href="#这如何影响-IT" class="headerlink" title="这如何影响 IT"></a>这如何影响 IT</h3><p>目前在现有的成功公司中，IT（集中式或分布式）构建或购买了大量用于“进入市场”的技术。这包括从销售技术（如 CRM 系统和电子商务）到营销技术（如数字在线形象和活动技术），再到客户支持技术（如工单系统），甚至数据工程、分析和科学（如数据管道、仪表板或购买倾向模型）。</p>
<p>注意：此处，当我提及 IT 时，我指的是公司中的所有技术组织。它们可以集中在一个正式的 IT 功能中，或分布在独立的业务线中。这场 AI 浪潮不会区别对待。它将席卷所有这些组织。</p>
<p>公司只能随着这些技术的发展而发展。由 AI 原生公司创造的缩短上市时间的压力最终会让现有公司中的 IT 团队感受到。现在，大多数 IT 公司都在鼓励其工程师安全地试验 AI，这是件好事。然而，我相信那些开始积极为这场浪潮做好准备的公司，不仅能够摆脱竞争，还能在这个新的 AI 驱动的经济中保持弹性。</p>
<h3 id="每个-IT-组织的旅程都是独一无二的"><a href="#每个-IT-组织的旅程都是独一无二的" class="headerlink" title="每个 IT 组织的旅程都是独一无二的"></a>每个 IT 组织的旅程都是独一无二的</h3><p><img src="/../asset_aiswe/1_YX1vABv3Iz4P8ylpgjm8KQ.webp"></p>
<p>我希望你把上面森林中间的瀑布想象成 AI 将创造的价值。所有 AI 原生公司都会扎营在离瀑布更近的地方，它们的应用程序架构、开发流程和团队专门为 AI-SWE 而精简</p>
<p>然而，所有现有的公司都位于更远的地方。他们每个人都从不同的地方开始。一些公司使用语言和平台，这些语言和平台无法从人工智能中受益。其他公司拥有脆弱且复杂的架构，这些架构可能是多年无机增长（收购）的结果，这些架构不会让人工智能代理轻松地自主插入并做出贡献，等等。</p>
<p>在软件开发生命周期 (SDLC) 方面，所有这些现有公司都将对工作管理系统进行自己独特的定制，对低级功能分解如何发生有独特的流程，他们自己的源代码分支策略，他们自己的软件门（代码气味百分比、单元测试覆盖率），直至他们自己的标准，即每个开发人员如何为每个 GitHub 拉取请求构建注释</p>
<p>在人工智能驱动的世界中，上述每个方面都需要改变。人工智能代理需要学习拉取请求中必须包含的内容、工作管理系统中必须更新的文档、如何构建和与测试环境交互、如何创建变更批准文档等方面的期望。而且，这对于每家 IT 公司来说都是不同的。简而言之，</p>
<blockquote>
<p>成为人工智能驱动的动力必须来自 IT 内部，因为没有哪个外部实体比 IT 本身更了解 IT。</p>
</blockquote>
<p>回想一下我们所有人将数据中心迁移到公有云的时候，或者甚至在多年前，我们为软件团队建立持续集成的时候，或者当我们从临时工作方式转向标准化的 PaaS 平台的时候。我们所要做的就是定义一个高级结构并计划实现它。借鉴我职业生涯中的众多变革举措，我对这波人工智能浪潮进行了大量思考，并将尝试解释一种可能的方法来实现这一目标。请注意，这只是我的观点，我知道还会有其他方法。</p>
<p>我分享这种方法的目的是启动你的创意，让你在这次旅程中有一个良好的开端。</p>
<h3 id="通往未来的可能桥梁"><a href="#通往未来的可能桥梁" class="headerlink" title="通往未来的可能桥梁"></a>通往未来的可能桥梁</h3><p>在我作为领导者的职业生涯中，我学到的一件事是，谈论理想的目标状态很容易，因为它已经在书籍和演讲中被大量发表，而且也很容易欣赏我们当前的状态问题。困难的工作是从当前状态构建到未来的桥梁。桥上的每一步都必须经过规划和思考。第一步需要足够接近当前状态，以便组织能够自然地过渡。后续步骤不能相距太远，否则人们会犹豫不决。这些步骤应该在彼此之上合乎逻辑地构建。<br><img src="/../asset_aiswe/1_0Vu6XIrljZtUo5R0YY5KPQ.webp"></p>
<p>我尝试根据阶段来阐述这些步骤，从我们大多数人发现自己在 IT 中的当前状态开始：</p>
<h3 id="当前状态"><a href="#当前状态" class="headerlink" title="当前状态"></a>当前状态</h3><p>目前，一些 IT 组织正在试验人工智能，并鼓励其软件工程师使用和接受人工智能增强型代码开发。工程师们采取的典型方法是在其交互式开发环境 (IDE) 上安装扩展，然后在他们编写代码时，使用特殊的键盘快捷键启动代码助手代理，该代理会检查他们的代码，或者可能是一条注释，指示该文件部分中需要什么，然后助手会自动建议几行代码，工程师可以选择接受或拒绝 [17]。</p>
<p>这些角色有多种选择可以获得帮助，以下仅列出几个示例：</p>
<ul>
<li><p>Github co-pilot：迄今为止最普遍的开发人员 IDE 扩展，由 OpenAI 的 GPT4 LLM 提供支持。</p>
</li>
<li><p>许多 IDE 扩展（例如 Code GPT），其中几个允许您使用 OSS LLMs 而不是 GPT4。</p>
</li>
<li><p>技术公司通过免费的 IDE 扩展来扩展其功能，以实现更广泛的采用。示例：</p>
</li>
</ul>
<p>a) 专有编程语言（例如 Salesforce 为 Apex 语言提供的 Einstein for Developers 扩展，可实现实时自动完成）</p>
<p>b) 核心技术能力（SonarSource 的 SonarLint 扩展，专注于在您编码时发现质量问题）</p>
<p>c) 扩展平台功能（CAST Highlight 扩展，用于审查软件结构安全性、软件知识产权的收购前尽职调查等）</p>
<p>如今，IT 中一个典型产品交付团队可能看起来像这样：<br><img src="/../asset_aiswe/1_6Ce1NkSo2g6FgaxAdevU_Q.webp"></p>
<p>一些工程师已经开始使用这些 IDE，也许还有一些敏捷团队的首席工程师。在整个 IT 部门，人工智能辅助开发也可能正在自然而然地流行起来。<br><img src="/../asset_aiswe/1_XuhXDI6o0IKLyTA3KQXH_g.webp"></p>
<p>人工智能辅助开发是一个极好的第一步，所有软件工程师都必须受到鼓励和内部支持，以便在公司安全政策的范围内尽可能多地利用这些功能。</p>
<p>然而，借用统计学中的一句话，这是必要的，但还不够。</p>
<p>虽然使用这些功能对所有开发人员来说都有普遍的无定形提升，但为了真正引入我们之前提到的“颠覆性”变化，需要投入一定程度的专门精力、思想和结构。这将我们带到了旅程的第一阶段：</p>
<h3 id="阶段-0：评估和规划"><a href="#阶段-0：评估和规划" class="headerlink" title="阶段 0：评估和规划"></a>阶段 0：评估和规划</h3><blockquote>
<p>如果你没有计划，那么你计划失败。</p>
</blockquote>
<p>在进行任何具体行动（如团队组建等）之前，需要进行评估练习，以了解和定义将人工智能驱动的工程引入 IT 组织的总体范围和目标。一个可靠的计划将推动高管信心，并为未来的阶段正确调整任何投资要求。</p>
<p>为了创建一个切实可行的计划，重要的是从我们将要引入的人工智能软件代理的角度来看待流程、工具和技术。通过此练习要记录的关键领域包括：</p>
<ol>
<li>评估关键软件开发流程</li>
</ol>
<p>从核心剥离，当今每个人类软件工程师必须参与哪些当前流程才能为交付功能做出代码贡献？这里的示例包括规划、调整规模、开发、遵守特殊的分支策略、运行特定的测试套件、特定的编码、单元测试和文档要求等。</p>
<p>换句话说，人工智能软件开发代理也需要集成哪些关键的低级软件开发流程才能在实践中发挥作用。</p>
<p>这是一项重要的练习，因为在所有 IT 组织中，有很多用于协调、沟通目的的高阶流程，但不会直接有助于交付功能。示例包括每周领导力报告、向高级领导层提交状态更新、跨多个域的协调会议。通常，由技术项目经理推动的任何工作都可以包含在此类别中。集成人工智能驱动的工程不会直接影响这些工作（是的，人工智能将以深远的方式间接影响这些高阶流程，但这超出了本文的范围）。</p>
<p>此评估和文档将帮助确定可能需要为 AI-SWE 团队编写的额外服务，以便将它们插入到现有的软件交付工作流中。</p>
<ol start="2">
<li>评估当前的 IT 基础设施和环境</li>
</ol>
<p>在使用 AI 代理工程时，存在与应用程序内的依赖关系管理（即运行时版本、正在使用的库等）和对邻近应用程序的基于 API 的依赖关系相关的非平凡问题。我最初对自主 AI 的实验揭示了在环境中部署软件时出现的各种此类挑战（我将在稍后的更具技术性的撰写中记录它们）。除了“本地”设置之外，还有应用程序的身份验证管理问题。开发人员可能需要执行一些手动步骤才能完全准备好部署到该环境。此类设置需要为每个 AI 代理复制，并且显然需要尽可能地自动化，以便能够轻松添加 AI 代理容量，而无需大量手动工作。简而言之，软件工程师从头开始在本地开发一些软件，然后部署到共享的 QA 环境，然后将该代码提升到更高的环境，一直到生产环境，详细的“低级别”文档对于识别 AI 代理将面临的障碍至关重要。</p>
<p>在许多情况下，如果目前 CD 尚未在以人为驱动的软件工程 (H-SWE) 中实施，那么一开始我们就需要做好准备，不会有 100% 的自动化持续部署 (CD) 路径来投入生产。例如，如果目前有人工参与变更和发布管理流程以将功能运送到生产环境，那么当半自主 AI-SWE 团队自行创建软件功能时，这种情况仍然存在。这并不会阻止 AI-SWE 团队创建和提交所有必需的文档和测试输出给变更委员会和发布经理。（随着时间的推移，变更和发布流程肯定也可以由 AI（非编码，而是推理）代理提供支持，但这超出了本文档的范围）</p>
<p>与软件流程评估类似，环境评估还将显示哪些其他功能需要简化，以便 AI-SWE 能够在代码通过所有更高级别环境一直提升到生产环境的过程中保持自主性。</p>
<ol start="3">
<li>识别所有可以集成 AI 的应用程序（复杂性和影响）</li>
</ol>
<p>可以插入 AI-SWE 的候选应用程序将有数百个。但是，需要仔细评估围绕每个应用程序的 SDLC 的流程的复杂性、针对每个应用程序语言的 AI 代码员大模型的成熟度以及 AI-SWE 对该应用程序的影响程度。就像在故事点中一样，斐波那契评分可以针对成本和收益维度使用，并且可以创建一个总体 ROI 指标，该指标可以帮助高管决定 AI-SWE 代理部署的顺序（更多内容见下面的第 2 阶段）。</p>
<p>重要说明：在应用程序优先级列表的底部应该是所有那些已经存在大量上下文丢失的应用程序 [18]。问题陈述必须保持为构建 AI-SWE 团队，而不是稀释为包括神秘地运行且没有人真正理解其工作原理的遗留应用程序。</p>
<ol start="4">
<li>制定过渡时间表和路线图</li>
</ol>
<p>当然，没有发布的路线图，任何规划阶段都是不完整的，这里也需要一个路线图。作为一名高管，我喜欢定义时间线，因为它迫使我和我的团队以实际日期和截止日期来思考，并使我们能够从整体上考虑项目的各个方面和阶段。当然，“没有计划能在与敌人的第一次接触中幸存下来”，正如孙子所说。这些时间线确实会改变，而且也会改变，特别是由于这种类型的工作以前从未做过。我们真的不知道在这段旅程中会有什么惊喜等着我们，而且目前还没有用户手册。（事实上，据我所知，本文可能是经验丰富的产品领导者首次尝试建立一个框架，而这些领导者一直身处一线）。</p>
<p>抛开所有这些不谈，该计划仍将提供一个明确的基准并确定所有假设。随着惊喜的出现，可以审查和调整计划。但是，那些截止日期仍然存在，公开传达，并且它们始终对团队施加健康的压力，以保持敏锐的焦点并交付成果。对于所有新项目或功能，我总是说：</p>
<p>“日期不能更改，但范围可以更改”</p>
<ol start="5">
<li>人工智能技术选型</li>
</ol>
<p>应该留出一些时间来进行以 POC 为导向的技术选择。最好能明确要支持哪种（即要微调哪些基础模型）。这些模型应该能够服务于优先级列表前四分之一中的最大数量的应用程序，该列表来自上述步骤 3。但是，我们必须能够保持灵活性。这一领域正在快速发展，今年晚些时候可能会出现创新，要求我们改变选择。这里重要的是关注不可后悔的事情，例如提出以下问题：</p>
<ul>
<li><p>人工智能技术（多智能体框架等）是否允许我们转换架构、处理编码、调试、质量保证和运营任务？</p>
</li>
<li><p>这些框架是否允许我们轻松地交换 LLMs？</p>
</li>
<li><p>我们是否可以定义一些通用基准，以便评估框架的性能？</p>
</li>
</ul>
<p>此选择必须基于多个概念验证，这些概念验证展示了 AI-SWE 如何与环境和软件工具集成。有很多可爱的 AI-SWE 应用，例如零食游戏、数到 100 以及用多种语言打印 hello world。此处的 POC 应重点关注 IT 中适用的简单但真实的用例。正是这些 POC 将为我们提供一定程度的信心，并展示从这里到我们可投入合理程度的努力估算的预期目标状态的实际差异。</p>
<p>为了开展此评估和规划评估，我建议成立一个小团队，其中包括：</p>
<ul>
<li><p>一位称职的软件工程领导者，了解公司在软件创建方面的工作方式，并且对生成式 AI 的交叉点有了解和热情。他们将成为这场转型的核心变革推动者，并将推动组织的这场变革。</p>
</li>
<li><p>一位经验丰富的架构师，能够与工程团队紧密合作，将架构投入生产。此人需要了解 IT 中的开发流程，并对 IT 企业架构组定义的核心架构原则有深入的了解，以确保这些原则反映在解决方案中。</p>
</li>
<li><p>一位来自初始目标域之一的首席工程师，对从需求到规模、环境再到代码审查的整个软件生命周期有深入的实践经验。我猜想，首席工程师会直觉地知道哪些 AI 代理最有可能对团队做出富有成效的贡献。</p>
</li>
</ul>
<h3 id="第一阶段：建立-AI-共享服务组"><a href="#第一阶段：建立-AI-共享服务组" class="headerlink" title="第一阶段：建立 AI 共享服务组"></a>第一阶段：建立 AI 共享服务组</h3><p><img src="/../asset_aiswe/1_UqqXXyIP1lL7X72hmIYKaQ.webp"></p>
<p>虽然人工智能辅助人类软件开发的自下而上的有机使用必须继续，但一个专门的团队将集中精力使人工智能软件工程变为现实。</p>
<p>具体来说，该小组将重点关注四个不同领域。我根据独特的技能组合和适用于每个领域的独特认知复杂性将这些领域分开。</p>
<p>世界上没有完美组织设计。它们往往会优化某些收益，同时牺牲另一些收益。我提出的组织设计优化了并行性和规模，并尽可能地为各个团队提供自主权，其关键操作重点是“快速行动”。这些团队是：</p>
<ol>
<li>人工智能开发团队</li>
</ol>
<p>主要职责：开发、定制和维护 AI 模型，特别是LLMs，用于各种软件开发任务。此开发将从预训练的基础模型开始，并在应用程序团队的内部软件和文档上对它们进行微调。</p>
<h4 id="职责："><a href="#职责：" class="headerlink" title="职责："></a>职责：</h4><ul>
<li><p>AI 模型定制：重点是微调预训练的 LLMs，以尽可能多地了解领域团队的特定代码库、软件架构文档和领域知识。这将依赖数据管理团队策划的高质量数据（参见下文第 2 点）。在很多情况下，模型将针对其进行微调的应用程序代码将使用基本模型已经理解的语言编写（例如 Microsoft 的 Phi-2）。但是，对于尚未针对这些模型进行训练的语言，可能需要进行新的研究，例如主要由基础架构自动化团队使用的 Terraform。AI 模型质量保证 (QA)：测试 AI 模型以确保它们满足功能要求和性能基准。这些 AI 模型必须满足稳健的评估标准和客观性能指标。对输出进行良好的外部测试将是 AI-SWE 代理发送给首席工程师的拉取请求。但是，根据首席工程师的反馈，性能指标需要被编纂并纳入模型创建管道。 该团队还应探索学术界新开发的方法，如 BotChat [9]，以衡量多智能体 AI-SWE 团队产生的质量。</p>
</li>
<li><p>研究与开发：随时了解最新的 AI 进展，并将它们纳入您的系统。</p>
</li>
</ul>
<p>所需技能：了解软件团队的特定技术栈、软件开发和系统集成。</p>
<p>我认为这个团队实际上并不需要从业者级别的的数据科学知识，因为已经创建了许多 OSS 功能（如 AutoGPT [19]）来帮助微调基础模型。该团队不会在 GCP 中的 TPU 芯片上做大量的数据科学或 ML 工作，如预训练模型，但它将是一个真正的工程团队，利用乐高积木构建 AI 软件工程能力，并在此基础上构建 OSS 基础模型。从本质上讲，我们在这里需要的是扎实的基础软件工程师。然而，该团队需要对预训练编码模型 [20] 的创建方式有一个大致的了解，并且需要随时了解多智能体软件开发领域的最新发展，这就是上面提到的研发责任。</p>
<p>在更高级的阶段，此团队可能需要预先训练自己的模型，这可能需要数据科学家和对云计算支出的投资，但在证明实际价值之前如此大规模地投资于此领域在商业上没有意义，因为可以通过构建预先训练的模型来获得很多提升。</p>
<ol start="2">
<li>数据管理团队</li>
</ol>
<p>主要角色：管理训练和优化 AI 模型所需的数据。此团队需要了解团队 1 训练模型所需的数据类型、质量和格式，然后能够收集和注释此数据。在早期，此团队和 AI 开发团队可能是一体的，但最终出于扩展目的，此团队需要作为一个独立的团队存在。事实上，在开始时，创建样本数据集以微调模型可能还是由 AI 开发团队 (1) 来完成，并且只有在跨多个应用程序扩展然后跨多个域扩展时才会将此工作交给此团队。我发现 Autogen 发布了一个非常适用的原型 [21]，通过将 AI 代理指向合适的代码文档（又称检索增强生成或 RAG），他们演示了 AI 代理如何能够学习库中一些新方法的语法并提供基于此的代码。我们可能选择在此处使用 RAG 方法，也可能不选择，但该示例清楚地表明这是可以做到的。</p>
<h4 id="职责：-1"><a href="#职责：-1" class="headerlink" title="职责："></a>职责：</h4><ul>
<li><p>数据收集和注释：收集并标记数据，例如代码库、项目文档和用户故事，以供 AI 训练。了解格式化和注释需求对于其他微调编码器模型非常重要。在自然语言模型的微调阶段，需要创建大量经过整理的数据，包括问答（键：值）对类型 [2]。该团队需要翻译这在 AI-SWE 领域意味着什么。</p>
</li>
<li><p>数据质量保证：确保用于训练 AI 模型的数据的准确性、一致性和相关性。这是模型成功和质量的一个非常关键的领域。回想一下之前的内容，较新的小型语言模型的有效性直接来自于这样一个事实，即高质量的教科书级别的数据被用于训练这些模型。这意味着该团队的很多精力将用于确保此数据与 AI 代理将被插入的应用程序相关。这可能需要查看和修改现有应用程序文档（或许还有代码）并与现有团队协作，以便在训练前改进其文档。虽然需要向这些 AI 代理教授应用程序的代码和一般内部文档，但研究在学校教授给计算机科学本科生的其他通用工程模式会很好。这也需要对实际的高质量学术文本进行格式化、分块和准备，以便对 LLMs 进行微调。最后，还需要设计一些关于如何衡量数据质量的客观指标。</p>
</li>
<li><p>数据隐私和安全性：按照隐私法和安全标准处理数据。该团队的任务是确保所有正在策划的数据都遵守公司的隐私和安全标准。</p>
</li>
</ul>
<p>所需技能：数据管理、数据注释、了解数据隐私法。</p>
<p>注意：该团队的输出的一个有用的附带好处是提高目标应用程序的代码和文档的总体质量。这可能会帮助现有团队完成诸如新工程师入职以及帮助现有工程师更深入地了解他们可能没有构建但继承的应用程序等工作。</p>
<ol start="3">
<li>人工智能集成和测试团队</li>
</ol>
<p>主要角色：将人工智能解决方案集成到现有流程和系统中，并确保其功能和可靠性。该团队为人工智能-软件工程师代理和团队提供“最后一英里”连接。如果没有所需集成工具和库，人工智能将产生的软件和文档将无法进行签入、审查和部署到测试环境。</p>
<h4 id="职责：-2"><a href="#职责：-2" class="headerlink" title="职责："></a>职责：</h4><ul>
<li><p>AI 工具：开发工具，将 AI 模型无缝集成到现有的软件开发管道中。实际上，这些可能是 API 和帮助程序库，可用于对内部身份管理系统进行身份验证、与 git 交互、与环境和现有的 CI&#x2F;CD 管道交互，以及可能与团队的工作管理系统（例如 Jira）进行交互。<br>这些都是人类编码人员通常在编写代码之外执行的所有活动。AI 代理将调用实用程序帮助软件来执行类似的操作。根据公司的成熟度和自动化水平，这项工作实际上可能比实际的 AI 开发需要更多的精力。</p>
</li>
<li><p>AI 性能监控：持续监控 AI 系统是否存在任何问题或与预期性能的偏差。注意：如果存在 MLOps 团队，这项工作也可以交给该团队。</p>
</li>
</ul>
<p>所需技能：软件集成、质量保证测试、性能监控和故障排除方面的经验。</p>
<ol start="4">
<li>AI 道德与合规（虚拟）团队</li>
</ol>
<p>主要职责：确保人工智能系统的道德开发和部署。最好是一个不直接向共享服务组织汇报的团队。具有这种经验的人员通常已经作为治理团队的一部分存在于公司的首席数据办公室中。他们将作为咨询和知情方参与进来，以确保在创建这些 AI-SWE 系统时考虑他们的意见。</p>
<h4 id="职责：-3"><a href="#职责：-3" class="headerlink" title="职责："></a>职责：</h4><ul>
<li><p>道德准则开发：提供道德人工智能使用准则，重点关注公平性、透明度和问责制，以便共享服务团队在人工智能实施中纳入这些准则。</p>
</li>
<li><p>合规框架：提供一套切实可行且可衡量的合规规则、法规和道德标准，共享服务团队可以在实施清单中纳入这些规则、法规和道德标准。这是一个不断发展的领域，因此应该与数据治理团队建立一个持续的接口，以获取行业中创建的任何新的重要法规或关键人工智能标准。<br>虽然这不是该团队的责任，但它可以帮助指导集成团队 (3) 创建可审计性的扩展包，例如 SOX 等，这些扩展包能够根据人工智能-SWE 团队创建的文档自动生成报告，这些报告可以被内部甚至外部审计员接受。</p>
</li>
</ul>
<p>所需技能：了解道德人工智能原则、法律和法规遵从性、利益相关者管理。选择具有“支持”态度的合作伙伴很重要，而不是“阻碍”态度。前者将与团队合作寻找可行的解决方案，而后者只是喜欢说“不，你不能那样做！”但没有动力找到前进的道路。</p>
<p>这些共享服务团队在成功过渡到人工智能驱动的软件开发过程中发挥着关键作用。它们的有效性在于它们协作、创新和适应人工智能技术不断变化的格局的能力。</p>
<h3 id="第一阶段的退出标准"><a href="#第一阶段的退出标准" class="headerlink" title="第一阶段的退出标准"></a>第一阶段的退出标准</h3><p>一旦该组的关键方面已创建并发挥作用，从第一阶段到第二阶段的退出标准是创建一个功能齐全且高效的人工智能软件工程代理，该代理构建在一个经过定制的微调LLM上，该LLM针对第一个目标应用程序&#x2F;scrum 团队的高质量数据进行了训练。此时，人工智能软件工程在受控环境中发挥作用。</p>
<p>然而，正如 DC Palter [22] 雄辩地指出的那样：</p>
<blockquote>
<p>90% 完成意味着产品已完成一半</p>
</blockquote>
<p>“在受控环境下运行的原型与在任何使用情况下都不会崩溃的商业产品之间存在巨大差距。当产品完成 90% 时，实际上才完成了一半。”</p>
<p>第二阶段从将这个完成 90% 的原型（AI-SWE 代理）引入真实环境开始。</p>
<h3 id="第二阶段-a：引入第一个-AI“代理”（又称小团队）"><a href="#第二阶段-a：引入第一个-AI“代理”（又称小团队）" class="headerlink" title="第二阶段 a：引入第一个 AI“代理”（又称小团队）"></a>第二阶段 a：引入第一个 AI“代理”（又称小团队）</h3><p>在此阶段，我们将第一个 AI-SWE 代理（注意使用代理与代理的区别，稍后会详细介绍）谨慎地引入到一个真实的 scrum 团队中，并随着代理证明其有效性而逐渐扩大代理的职责。我们必须在引入过程中小心谨慎，因为我们不希望这会扰乱目标 scrum 团队的速度，该团队已为正在构建的史诗故事提交了故事点，以便进行该版本。</p>
<p>我建议我们将 AI-SWE 代理引入由参与该项目从第 0 阶段开始的首席工程师领导的团队。负责人将指导 AI 代理，最初的任务可能只是开始为简单的技术债务问题（例如我们知道 AI 代理擅长解决的开放缺陷修复 [23] 或删除 sonarQube 运行中发现的简单技术债务问题 [24] 等）或甚至缺少代码和可用性的文档提供拉取请求。</p>
<p>在此阶段，所有集成和帮助程序库都经过微调，目标是让 AI 代理对团队的贡献变得顺畅、一致且与团队中的人类开发人员一样好（或更好）。</p>
<p><img src="/../asset_aiswe/1_HlxBoXudHrCqKHSkG_1lAg.webp"></p>
<p>在幕后，我们正在引入的人工智能代理实际上是由多个较小的代理相互交互组合而成的。这一点很重要，因为多代理输出通常远优于与人类的单轮交互。</p>
<p>这里使用一个更恰当但远不酷的术语是代理团队。这就是我采用大写“A”的术语人工智能代理来区别于实际原子代理的原因。然而，我在整个文档中交替使用团队和代理<br>放大到人工智能代理（又称团队）中，我们可以在下面看到许多不同的代理，每个代理都由相同或不同的 LLMs（取决于其专业化）支持<br><img src="/../asset_aiswe/1_MmrCJ-1TMu7RGrGts5gY6A.webp"></p>
<p>团队中显示的代理可能具有以下特征和职责</p>
<p><img src="/../asset_aiswe/1_TxlwwbZniyBrIlTB7ReYxA.webp"></p>
<p>以上只是完成人类工程师工作编码部分所需的交互和代理的一个示例。它并不完整。例如，它缺少如何处理环境等。这可能需要一个完全独立的团队来准备，然后才能逐步执行其指令。这里的目标只是解释具有大写“A”的代理的一般概念。</p>
<p>此阶段允许负责人微调各种多个代理的提示和角色。</p>
<p>以下是取自 Autogen 参考示例之一 [21] 的这些代理之间的示例对话。</p>
<p><img src="/../asset_aiswe/1_Ta-kyAwEAD8DTXUiG2g7Qw.webp"><br>来源：[25] Autogen：多代理示例，展示了小型代理如何相互交谈并相互批评以产生出色的最终输出。</p>
<h3 id="第-2a-阶段的退出标准"><a href="#第-2a-阶段的退出标准" class="headerlink" title="第 2a 阶段的退出标准"></a>第 2a 阶段的退出标准</h3><p>当 AI-SWE 代理（小团队）能够从看板中获取真实的编码用户故事、处理这些故事并最重要的是，他们的拉取请求达到始终获得首席工程师批准并合并到主线代码的质量水平时，可以认为第 2a 阶段取得了成功。</p>
<p>我们现在已准备好将这些代理引入其他领域。</p>
<h3 id="第-2b-阶段：将-AI-软件代理引入其他领域"><a href="#第-2b-阶段：将-AI-软件代理引入其他领域" class="headerlink" title="第 2b 阶段：将 AI 软件代理引入其他领域"></a>第 2b 阶段：将 AI 软件代理引入其他领域</h3><p>此阶段更多的是对第 2a 阶段的清洗和重复。此处的不同之处在于向其他首席工程师和架构师介绍这种工作模式，以及培训适合该领域的自定义LLMs。例如，为客户支持技术团队创建的自定义LLM编码器需要针对不同的软件存储库、应用程序文档进行微调，甚至可能需要一组修改过的帮助程序函数（如果其软件工作流不同），而不是为财务技术团队创建的自定义LLM。</p>
<p><img src="/../asset_aiswe/1_8dFkGeuqbzqivQmostYsJg.webp"></p>
<p>此时，我们应该开始在平台基础设施域（如上图中的团队 1）的 Scrum 团队中引入多个 AI 代理。这不仅有助于并行处理（即代理在同一个sprint中同时从看板中获取多个用户故事），而且还可以隔离非常不同的职责，例如一个代理完全负责环境创建和测试工具包设置，而另一个代理负责编写和将代码部署到该环境。</p>
<h3 id="阶段-2b-的退出标准"><a href="#阶段-2b-的退出标准" class="headerlink" title="阶段 2b 的退出标准"></a>阶段 2b 的退出标准</h3><p>当每个适用的域至少有一个 AI 代理在线并成功从看板中获取用户故事、处理这些故事并更新工作管理系统时，可以认为阶段 2b 已完成。最重要的是，他们的拉取请求的质量水平始终得到首席工程师的批准并合并到主线中。</p>
<p>我们现在可以进入阶段 3。</p>
<h3 id="第三阶段-a：将半自主-AI-团队引入一个领域"><a href="#第三阶段-a：将半自主-AI-团队引入一个领域" class="headerlink" title="第三阶段 a：将半自主 AI 团队引入一个领域"></a>第三阶段 a：将半自主 AI 团队引入一个领域</h3><p>这是团队完全由 AI-SWE 代理组成的阶段。</p>
<p>从第 2 阶段到第 3 阶段的一个根本性转变是，在这个阶段，团队正在处理完整且完整的史诗，其每个代理都会选取用户故事并相互协调以完成工作。</p>
<p>另外，这种 AI 全面开发软件的范式转变也可能会影响我们在软件中划分工作的方式（项目 -&gt; 史诗 -&gt; 用户故事 -&gt; 任务）。请记住，分解工作以适应人类驱动的软件开发。AI 驱动的开发的工作分解方式超出了本文的范围，并且将在未来几年自然演变。为了熟悉（和简单！）我们将继续在此处使用传统的敏捷术语，如史诗和用户故事。</p>
<p><img src="/../asset_aiswe/1_Mn0kDDd4dq2HMIsBI2Nx7w.webp"></p>
<p>在这个阶段，需要在各个代理（小团队）之间建立一个更高层次的沟通平面，这意味着一个复合代理（小团队）内的某个原子代理的交互连接并与另一个复合代理内的另一个代理进行沟通。这也可能意味着这些代理正在通过工作管理系统和问题委员会（如 Github 问题）进行协作。这些细节并不相关。更重要的是要注意，每个代理对于某些任务都具有完全的自主权。这些角色可以是：</p>
<ul>
<li><p>软件开发代理（在第 2a 阶段中进行了详细说明）</p>
</li>
<li><p>产品管理代理，可以在人类产品经理的监督下编写和充实产品需求文档并规划路线图</p>
</li>
<li><p>技术撰稿代理，负责面向客户的文档、内部软件文档、全体胜利的幻灯片创建、IT 组合更新</p>
</li>
<li><p>研究助理，支持架构师和产品经理，帮助进行有关业务能力、SWOT 分析等的外部研究。</p>
</li>
<li><p>运营支持代理，通过系统警报直接激活，或在触发并指向要解决的问题时手动激活</p>
</li>
</ul>
<p>从本质上讲，这些团队成为一个独立的单元，对端到端责任负责。需要注意的是，AI 代理角色的设计不同于人类角色 [27]，当我们通过最初的尝试获得经验反馈时，我们将为最优角色的需要留出一些灵活性。</p>
<p>最近的研究 [28] 表明，编码、通信和规划是 AI-SWE 在编写纯软件领域开始做得更好的领域。然而，要使 AI 自主完成此阶段预期的某些活动，需要生成式 AI 进行称为系统 2 或慢速思考的更高级别的思考。[29] 截至撰写本文时，我们尚未达到这一步，这意味着在运营故障排除或产品管理活动期间所需的许多理性和复杂思考仍需要人工支持和人工干预。</p>
<p>然而，预计 2024 年期间该领域将取得突破，可能在 2H [2] 期间。尽管如此，仍应能够利用当前状态能力和LLM定制实现大部分史诗级开发。</p>
<p><img src="/../asset_aiswe/1_5EndGJH37vAF163aURM3UA.webp"></p>
<p>需要注意的一个领域是，AI 在功能性需求方面做得很好，但它在非功能性需求 (NFR) 方面（包括性能、信任和规模）的能力还不够（目前）。需要人工干预来确保确实按照规范开发了组织针对这些 NFR 的基准。这可能意味着只需具备人工端到端测试能力（当然，测试也由 AI 代理开发和运行，但需要人工签字），或者尽可能多地进行人工驱动的软件增强以添加预期的 NFR 能力。</p>
<h3 id="第-3a-阶段的退出标准"><a href="#第-3a-阶段的退出标准" class="headerlink" title="第 3a 阶段的退出标准"></a>第 3a 阶段的退出标准</h3><p>此阶段预期具有一定程度的复杂性，因此无法对确切的退出标准做出规定，但一般来说，成功的良好衡量标准是期望从需求开发出完整的史诗，以在质量保证中运行的功能性解决方案。可能仍需要人工验证是否已按照规范开发了非功能性需求 (NFR)。</p>
<h3 id="第-3b-阶段：将-AI-团队推广到其他领域"><a href="#第-3b-阶段：将-AI-团队推广到其他领域" class="headerlink" title="第 3b 阶段：将 AI 团队推广到其他领域"></a>第 3b 阶段：将 AI 团队推广到其他领域</h3><p><img src="/../asset_aiswe/1_T9V3R853twnodAS6QnJf9A.webp"></p>
<p>利用从一个领域的 AI 团队中学到的经验教训，此阶段将自主 AI 团队引入其他领域。这也开始将一些高级工程师提升到领导级别，以便扩大团队规模。</p>
<p>这里没有描述但值得一提的一个方面是需要一个端到端 QA 团队（它应该是一个由 AI + 人类工程师组成的混合团队）。端到端 QA 很重要，因为这些 AI 团队可能针对单独史诗的用户验收标准进行开发，但在所有这些在发布前组合在一起时，仍然需要验证和集成测试端到端体验。这也扩展到了性能测试需求。最终，当肌肉发育得足够好时，QA 功能也可以训练成一个LLM，但我预见到它会晚于软件开发功能发生，仅仅是因为相互依赖的复杂性。这仍然不排除 QA 功能在早期阶段大量利用 AI 代理来评估、编写和部署自动化测试和自动化报告。</p>
<h3 id="第-3b-阶段的退出标准"><a href="#第-3b-阶段的退出标准" class="headerlink" title="第 3b 阶段的退出标准"></a>第 3b 阶段的退出标准</h3><p>此阶段的退出标准出人意料地简单：已部署大量（80-100%，由执行领导层决定）的功能性 AI 团队，并且它们正在为每个领域做出贡献，并且开始观察到交付周期发生了明显变化。</p>
<h4 id="稳定状态（第-n-阶段）：扩展-AI-团队，分散-AI-共享服务"><a href="#稳定状态（第-n-阶段）：扩展-AI-团队，分散-AI-共享服务" class="headerlink" title="稳定状态（第 n 阶段）：扩展 AI 团队，分散 AI 共享服务"></a>稳定状态（第 n 阶段）：扩展 AI 团队，分散 AI 共享服务</h4><p><img src="/../asset_aiswe/1_tyL9pO1jEpZMdRzmwUkXJw.webp"></p>
<h4 id="扩展人工智能团队"><a href="#扩展人工智能团队" class="headerlink" title="扩展人工智能团队"></a>扩展人工智能团队</h4><p>此时，各领域开始适应人工智能团队的工作质量，越来越多的软件工程师开始升级并提升为负责人或其自身人工智能团队的人工智能工程师 [16]。虽然我只展示了每位工程师对应一个人工智能团队的映射，但没有理由说同一位工程师不能运营多个团队。速度的瓶颈自然会转移到这些人工智能工程师身上，但此时公司应该已经享受到了变更的前置时间大幅缩短的好处。这些人工智能工程师需要专注的技能仍然会有一部分是软件，因为他们仍然需要重点审查和批准各种人工智能代理发送的 Pull Request，并审查他们创建的文档等。然而，所有工程师都需要专注于“优秀工程师无需畏惧”一节中提到的独一无二的人类技能</p>
<p>请注意，始终需要少数由人类驱动的软件团队。然而，这些将是高阶函数，它们的工作将成为该领域的安全网。如前所述，一个很好的例子是确保 NFR 按照规范交付，但还会有其他 AI 缺陷浮出水面。重要的是，这些缺陷已记录在案并与该领域的 AI 开发团队共享，以便该领域自定义LLMs的未来版本更胜一筹。</p>
<h4 id="AI-共享服务的分散化："><a href="#AI-共享服务的分散化：" class="headerlink" title="AI 共享服务的分散化："></a>AI 共享服务的分散化：</h4><p>根据我的经验，当技术和产品开始成熟时，共享服务团队开始成为速度的瓶颈。因此，在稳定状态下，所有组织都必须计划将任何 AI 共享服务团队分散到每个领域。这意味着每个领域都会构建本地肌肉来定制自己的 AI LLM模型，以整理训练数据，并维护对它们来说很特别的自定义集成。</p>
<p>共享服务团队开始转变为卓越中心 (CoE) 组织，负责始终需要横向且最好保持标准化的服务。这包括一些常见的集成。例如，所有域都将继续需要其 AI 代理才能与 IT CI&#x2F;CD 管道团队集成，或与 Github、sonarQube 和 Jira、Asana、BaseCamp 等工作管理系统进行交互。</p>
<p>由于 IT 系统始终可以接受审计，因此此 CoE 可以创建和维护扩展包，团队可以插入这些扩展包并创建审计报告（即，先前共享服务组的道德和合规团队的迭代）。</p>
<p>有研究表明，从人工智能获得编写代码帮助的开发人员往往会编写安全性较低的代码，更令人担忧的是，他们对这种代码的安全性过于乐观 [31]。这个问题可能会随着完全由人工智能驱动的团队而变得更糟。这就是为什么对代码质量和库选择进行集中检查非常重要的原因。这可能要求卓越中心创建自己的哨兵软件，该软件能够跨域跟踪安全代码和基础设施，以及自动跟踪和添加这些团队积压中的项目，并集中报告整体安全态势。如果一家公司已经拥有一个强大、人员配备充足的中央安全组织，该组织拥有自己的软件工程师，并且与 IT 部门紧密相连，那么没有理由不能在那里也运行此功能。</p>
<p>如果应用程序架构一开始不可组合且不是事件驱动的，那么它也可能成为速度的瓶颈。这是需要意识到的事情，如果第 0 阶段评估完成得相当公平，也应该在每个应用程序的评分和优先级步骤中发现这一点。</p>
<h3 id="反驳论据"><a href="#反驳论据" class="headerlink" title="反驳论据"></a>反驳论据</h3><p>这是一个小小的“幕后”故事。当我写这篇文章时，我让 chatGPT 采取反对的立场，并解释对人类驱动的软件工程的依赖减少会带来什么问题。我简要概述的反馈实际上帮助我完善了我的提议阶段，例如将人类工程师的水平提高到更高但更关键的水平，并让团队中的每位现有工程师负责一个自主团队。</p>
<p>以下是 chatGPT 的反驳论点，以及我对其每个论点的回应：</p>
<p>这里有一些需要考虑的反驳论点：</p>
<ol>
<li>复杂的解决问题和创造力：</li>
</ol>
<ul>
<li>人类开发者擅长创造性地解决问题和处理新颖的情况。人工智能虽然擅长处理常规任务，但可能难以处理需要跳出框框思考的独特或复杂问题。</li>
<li>设计和创新中的创造力是人类特有的，人工智能无法完全复制，尤其是在需要情商和理解人类经验的领域。</li>
</ul>
<p>我的回复：观点有效。这让我思考如何提高现有工程师的水平，并将他们安置在组织中，让他们发挥更多的创造力和跳出框框的思维。通常，当您更接近客户时需要这样做，而这正是您担任首席工程师角色时发生的情况（参见稳定状态：第 n 阶段）</p>
<ol start="2">
<li>代码质量和维护</li>
</ol>
<ul>
<li>人工智能生成的代码可能缺乏经验丰富的人类开发人员带来的细微差别理解，可能导致可维护性、可扩展性和性能方面的问题。</li>
<li>人工监督对于确保代码质量至关重要，尤其是在理解更广泛的背景很重要的复杂系统中。</li>
</ul>
<p>我的回复：我也想到了同样的事情，因为我引用过的各种论文都表达了人工智能软件工程在非功能性需求方面的弱点。这就是为什么即使在稳定状态下，我也选择保留一些实际的人类工程团队，以确保软件的信任、运营弹性和性能方面得到考虑。</p>
<ol start="3">
<li>伦理和社会影响：</li>
</ol>
<ul>
<li>用 AI 取代人类工人会引发重大伦理问题，包括对就业的影响以及减少人类劳动力的更广泛的社会影响。</li>
<li>有可能失去多年积累的宝贵的人类技能和知识，而这些技能和知识不易转移给 AI。</li>
</ul>
<p>我的回复：我考虑到了这一点。虽然我预见到 IT 组织将停止以同样的速度增长，但现有工程师将被重新分配去做更高层次的工作这一事实将确保有意义的就业，同时让他们更好地向高级领导和企业展示自己。当然，这意味着现有工程师必须提高传统上属于产品管理的技能，而产品经理需要适应领导 AI 团队和框架。</p>
<ol start="4">
<li>缺乏适应性和直觉：</li>
</ol>
<ul>
<li>AI 系统，即使是高级系统，也可能无法很好地适应快速变化的环境或需求，这是软件开发中的常见情况。</li>
<li>人类可以直观地处理模棱两可或定义不明确的问题，这是 AI 目前所不具备的能力。</li>
</ul>
<p>我的回复：正确，这就是为什么每个 AI-SWE 团队都有一个人类工程师来指导它。AI-SWE 仍然需要明确且低级别的指令和调整，因为应用程序另一端的界面仍然是人类，而只有人类才能感同身受。</p>
<ol start="5">
<li>安全性、可靠性、监管和合规性问题：</li>
</ol>
<ul>
<li>AI 系统可能容易受到特定类型的故障、偏差和安全问题的影响，这些问题不同于人类团队遇到的问题。</li>
<li>在关键开发任务中依赖 AI 可能会在 AI 行为不可预测或错误的情况下带来风险。</li>
<li>AI 系统可能无法完全胜任软件开发的法律和合规性方面，这通常需要人类的判断和对法律的理解。</li>
<li>在某些行业或地区，对于在关键开发角色中使用 AI 可能存在监管挑战或限制。</li>
</ul>
<p>我的回复：好观点。这让我在工程中心团队中增加了一项责任，即使在稳定状态下也要负责安全和合规。在阅读了有关某些行业（或地理位置）中的监管挑战的观点后，我在文章中确实添加了一个警告，即 AI-SWE 模型可能不适用于此类行业，并且对 AI 的使用开发可能必须以更受控的方式进行。</p>
<ol start="6">
<li>沟通与协作：</li>
</ol>
<ul>
<li>有效的软件开发通常需要与利益相关者进行细致入微的沟通和协作，而这是人类团队天生更擅长的。</li>
<li>了解客户需求、同情最终用户问题以及协商需求是人际互动至关重要的领域。</li>
</ul>
<p>我的回复：同意，这就是为什么人工智能工程师和产品经理都将到位，以处理与人类利益相关者的沟通方面。</p>
<ol start="7">
<li>过渡和维护成本：</li>
</ol>
<ul>
<li>最初投资于开发、培训和集成用于软件开发的人工智能系统可能会很大。</li>
<li>持续维护、更新和培训人工智能系统以保持其有效性也可能产生重大成本。</li>
</ul>
<p>我的回复：我能理解 GPT4 可能认为我会在内部进行预训练部分，这是一个错误的假设。我依靠使用可用的预训练基础模型来微调内部 IT 软件代码库。</p>
<ol start="8">
<li>理解范围有限：</li>
</ol>
<ul>
<li>人工智能，即使具有高级功能，也可能无法像人类那样完全掌握业务背景或市场和用户需求的细微差别。</li>
<li>存在人工智能解决方案过于狭隘的风险，而忽略了人类开发人员经常考虑的大局。</li>
</ul>
<p>我的回复：同意，这就是为什么人工智能工程师仍在最终状态中指导人工智能软件工程团队的原因。</p>
<ol start="9">
<li>过度依赖的风险：</li>
</ol>
<ul>
<li>过度依赖人工智能会导致组织内人类专业知识的下降，使其容易受到人工智能故障或局限性的影响。</li>
</ul>
<p>我的回复：这是正确的，我同意这种观点。然而，这是一个整个软件行业都将面临的更普遍的风险。我不确定我们现在如何避免这种情况。简而言之，我认为没有人能回答这个问题，但我想我们将在未来几年里共同找到答案 :)</p>
<h3 id="优秀工程师无需害怕"><a href="#优秀工程师无需害怕" class="headerlink" title="优秀工程师无需害怕"></a>优秀工程师无需害怕</h3><p>首先，让我们正面解决恐惧的话题。从对软件工程师对人工智能情绪进行的社交媒体研究中可以明显看出，恐惧是开发人员中最主要的感受。这一点很重要。</p>
<p><img src="/../asset_aiswe/1_icgBcNIYK_dXhR8PMYkk-g.webp"></p>
<p>与此同时，让我们退一步，看看优秀软件工程师拥有的完整技能集 [32]</p>
<p><img src="/../asset_aiswe/1_vn_i7z6M3HoVr0oMjDv1Mw.webp"></p>
<p>在对众多关于优秀软件工程师特质的研究进行总结时，您会发现编码部分只是成就这些工程师的原因之一。如果您查看上面的图表，我用灰色箭头标记了所有领域，无论人工智能代理如何先进，都永远无法模仿。在我看来，这些是所有软件工程师都应该有意磨练和完善的无法模仿的技能，因为这些技能将使他们与众不同，并在新的 AI-SWE 范式转变中保持其价值。事实上，这些正是上面我描绘的人工智能驱动的 IT 公司的稳定状态阶段中会强调和需要具备的技能。</p>
<h3 id="结论——暴风雨即将来临，但您准备好了吗？"><a href="#结论——暴风雨即将来临，但您准备好了吗？" class="headerlink" title="结论——暴风雨即将来临，但您准备好了吗？"></a>结论——暴风雨即将来临，但您准备好了吗？</h3><p><img src="/../asset_aiswe/1__FckG9Ye7jqeWJgspswgBA.webp"></p>
<p>我在技术行业工作了 20 多年，经历了技术泡沫和破灭、基础设施（内部部署到云端）和流程（瀑布式到敏捷式）的颠覆。我可以说，我们现在所见证的必须是有史以来席卷软件世界的最大海啸。但不要只相信我的话，看看比尔·盖茨怎么说 [34]。进入基于软件的技术构建的准入门槛将被消除，从构思到上市的交付周期将缩短，以前甚至不在同一领域工作的公司将开始竞争，新的灵活公司将涌现，这些公司将威胁到伟大的传统。我们许多人都在这些现有的大公司工作，我们中的一些人身居高位，可以影响公司的发展方向。也许其中一些成功的公司甚至不会承认这是一场即将到来的生存危机，直到为时已晚。我想象会有很多人会感觉到即将到来的变化，但他们更愿意留在现有现状的舒适圈中。</p>
<p>而机会恰恰就在这里。少数积极采取行动的公司将把自己置于脱离群体的位置，并在变革开始加速时处于主导地位。这些公司将成为伟大的公司。它们将比其他公司更长寿，当尘埃落定、硝烟散尽时，它们将比以往任何时候都更加强大。</p>
<p>此内容包含我的观点，不应理解为我雇主的官方立场。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/designing-ai-driven-software-engineering-teams-8afd8de13f1a">https://towardsdatascience.com/designing-ai-driven-software-engineering-teams-8afd8de13f1a</a></p>
<h3 id="术语表"><a href="#术语表" class="headerlink" title="术语表"></a>术语表</h3><p>AI Agent：协同工作的 AI Agent 的多 Agent 超集</p>
<p>AI agent：由通用或专门的大语言模型支持的 Agent</p>
<p>AI Teamlet：与 AI Agent 相同</p>
<p>AI-SWE：人工智能驱动的软件工程：由人工智能代理和人工智能团队编写、调试、构建和部署的软件。</p>
<p>CI&#x2F;CD：持续集成和持续交付</p>
<p>CoE：卓越中心</p>
<p>CRM：客户关系管理系统</p>
<p>H-SWE：以人为驱动的软件工程：由人类调试、构建和部署的软件</p>
<p>MVP：最小可行产品</p>
<p>OSS：开源软件</p>
<p>RAG：检索增强生成</p>
<p>SDLC：软件开发生命周期</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/01/18/ai-swe/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/01/18/ai-swe/" class="post-title-link" itemprop="url">如何设计一个人工智能驱动的软件工程团队</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-01-18 14:00:16 / 修改时间：14:07:35" itemprop="dateCreated datePublished" datetime="2024-01-18T14:00:16+08:00">2024-01-18</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3.5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>6 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>生成式人工智能即将颠覆我们今天开发应用程序的方式。了解它将如何影响技术团队以及我们能做些什么</p>
<h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>欢迎来到 2024 年！空气中弥漫着变革的气息。您感受到了吗？   </p>
<p>人类在生成式人工智能 (Gen AI) 领域内的创造力将我们带到了我们从未想象过的境地。这些创新保证会颠覆我们所知的现有软件工程专业。</p>
<p>新的、以人工智能为基础的公司将崛起并威胁现有企业。结果将是深远的。我们将见证从创意到产品发布的交付时间大幅缩短、运营成本大幅降低，因此经济也发生了根本性的变化，最后，对人类编写的软件工程的依赖性不断降低。</p>
<p>我们这个时代许多伟大的公司都有庞大的技术团队来支持他们的业务流程，例如销售、营销、支持等。这些团队如何应对不仅将决定他们如何通过这些变化保持相关性，还将为他们支持的公司带来潜在的不可战胜的竞争优势。<br>这篇由两部分组成的文章将提供经验证据来支持上述说法，并展示技术团队可以采用的可能框架来驾驭这个AI怪兽。</p>
<h3 id="为什么现在？"><a href="#为什么现在？" class="headerlink" title="为什么现在？"></a>为什么现在？</h3><p>2023 年最后六个月出现了三项关键创新，这些创新几乎可以保证改变以人为基础的软件工程学科</p>
<h3 id="LLM-开源竞赛已经开始"><a href="#LLM-开源竞赛已经开始" class="headerlink" title="LLM 开源竞赛已经开始"></a>LLM 开源竞赛已经开始</h3><p>大型语言模型 (LLMs) 是生成式人工智能的核心，而现在，开源大型语言模型正在兴起，并在性能上赶超当前占主导地位的专有赢家 GPT4 [1]。<br><img src="/../asset_aiswe/1_JDahb5PI3MG2BVHlReZcvQ.png"></p>
<h4 id="为什么这很重要？"><a href="#为什么这很重要？" class="headerlink" title="为什么这很重要？"></a>为什么这很重要？</h4><p>为了回答这个问题，让我们快速总结一下创建 LLMs 的两个阶段过程：[2]<br><img src="/../asset_aiswe/1_jlyDicWR_SmRiJC2SkLg0g.png"></p>
<p>如您在上面看到的，阶段 1，又名LLM的预训练，不可能用少量预算或在某人的车库中完成。然而，Mistral AI、Meta 以及现在甚至微软（不是 OpenAI）[3] 等公司开始推出高质量的开源软件 (OSS) 预训练基础模型，如 Mistral 和 LLAMA，它们改变了游戏规则。突然之间，较小的公司甚至个人开始创建专门的模型，并将它们发布在 huggingface.co 上，这是所有 OSS 模型的必去之处。一些非常有趣的模型开始出现，例如 Samantha [4]，一个伪情感的LLM，以及 Meditron [5]，一个医学上精明的LLM。</p>
<p>与软件工程相关的是，大量针对编码的语言模型开始出现，例如 codellama、deepseek coder、phi 等。我所在的公司 Salesforce 也发布了 CodeGen [6]。而且，就像 OSS 世界中其他任何引起轰动的事物一样，这些模型注定会变得越来越好。我们将看到LLMs开发出越来越大、越来越复杂和越来越精密的软件类型，而且越来越自主。<br>开源LLMs如此重要的另一个关键原因是知识产权问题。并非所有公司都愿意将他们的 AI 工作发送给 openAI，而 openAI 实际上只是一个托管其闭源模型的外部 API。在这个 API 背后，公司无法知道他们的数据是如何被使用或共享的。显然，OSS 消除了这种担忧。使用 OSS，您的 IP 不会离开您的笔记本电脑、数据中心或您的 AWS 虚拟私有云。</p>
<h3 id="“小型语言模型”来了"><a href="#“小型语言模型”来了" class="headerlink" title="“小型语言模型”来了"></a>“小型语言模型”来了</h3><p>对于任何低成本或无成本开发，快速原型设计需要能够在工程师的笔记本电脑上进行。虽然经过 7B 或甚至 11B 参数训练的 LLMs 可以在相当不错的笔记本电脑上运行，但这些仍然会占用大量 RAM 和 CPU&#x2F;GPU。此外，除非您有一台非常强大的机器，否则您还可以忘记运行超过 30B 参数的 LLMs。此外，当您必须将包含这些模型的任何始终在线的功能投入生产时，您可以预期会收到昂贵的托管&#x2F;云账单。为了克服这一进入壁垒，微软等公司开始研究 [7] 如何创建更小但仍然高性能的基本模型，并随后开源小型语言模型 Phi-2 [3]。他们的主要工具是使用高质量的“教科书级”数据 [8] 来训练这些模型，而不是大量质量低劣、可靠性参差不齐的互联网数据。</p>
<p>随着小型模型开始在智能手机甚至树莓派等小型电子设备上本地运行的可能性逐渐显现，该领域的研究将在 2024 年继续加速。特别是对于本文重点关注的软件开发，这将提供一种高性能且环境可持续的选择来扩展人工智能驱动的软件工程 (AI-SWE)，从而降低另一个进入壁垒：运行时成本。</p>
<h3 id="人工智能代理相互协作以获得卓越的成果"><a href="#人工智能代理相互协作以获得卓越的成果" class="headerlink" title="人工智能代理相互协作以获得卓越的成果"></a>人工智能代理相互协作以获得卓越的成果</h3><p>如果您使用过 chatGPT，您就会知道它的第一个回复有时并不是最好的，您必须与 GPT 交谈以调整并获得可接受的答案。好吧，在 2023 年夏季，人们开始进行LLMs聊天并相互批评，经过多次迭代后的输出结果出人意料地提高了很多质量 [9]。</p>
<p>这一激动人心的发展催生了多个 OSS 项目，其中一些项目已经流行起来。我想给您提供一个小示例，说明这些多智能体框架（在某些情况下，是成熟的应用程序）能够做什么：</p>
<h4 id="Autogen-微软-12"><a href="#Autogen-微软-12" class="headerlink" title="Autogen (微软) [12]"></a>Autogen (微软) [12]</h4><p>AutoGen 是一个框架，它支持使用多个可以相互对话以解决任务的代理来开发 LLM 应用程序。</p>
<p>我的看法：在我的早期实验中，这个框架非常灵活。Autogen 还将一些非常有趣的代理功能（如 RAG）直接引入其库中，甚至还有一个可教代理，该代理会将您与它的聊天内容写入持久性存储，并且可以在将来查询它，从而永远记住它。此外，您可以同时为不同的角色使用 LLMs（封闭和开放）的组合！但是，除了在初始需求提示中指定之外，我还没有找到如何明确且以编程方式将工作委派给每个 AI 代理的方法。<br><img src="/../asset_aiswe/1_lda7g5-pX1SlDVCSV3htlQ.webp"></p>
<p>此外，还有新的多代理框架不断涌现。一个新兴框架 crew ai [13] 也引起了我的注意。它拥有最简单、最直观的 API，并且还可以明确地将任务委派给特定代理，这是 autogen 目前所不具备的。然而，它仍然非常简单，并没有很多花里胡哨的东西。</p>
<h3 id="2023-年下半年-—-创新年表"><a href="#2023-年下半年-—-创新年表" class="headerlink" title="2023 年下半年 — 创新年表"></a>2023 年下半年 — 创新年表</h3><p>正如我提到的，2023 年的最后 6 个月对 Agentic AI 领域来说是一个福音。Oliver Morris 在下图中出色地捕捉到了在代理、代理团队和LLMs中发生的具体创新。<br><img src="/../asset_aiswe/1_foYxPY9w_DlsxrGf6nfkQQ.webp"></p>
<p>Oliver 非常出色地捕捉到了这种演变，因此总结上述内容毫无意义。我在这里的目标只是强调他的工作 [14]，因为它与这种演变非常相关，并且可能还会帮助您了解创新走向何方。</p>
<h3 id="软件开发中人工智能代理协作的发现"><a href="#软件开发中人工智能代理协作的发现" class="headerlink" title="软件开发中人工智能代理协作的发现"></a>软件开发中人工智能代理协作的发现</h3><p>独立的公共 [14] 和学术 [15] 研究（通过我自己的实验验证，我将在以后的文章中详细介绍）表明，鉴于提示中针对性和经过深思熟虑的要求，以及代理明确定义的角色和范围，多代理协作产生了惊人良好的结果。</p>
<p>我看到人工智能规划师定义了详细的分步计划，人工智能评论家帮助规划师完善计划。接下来，规划师要求人工智能程序员开发代码，该程序员还有一名配对代码审查员，负责识别代码中的缺陷和问题，以及一名在受控环境中实施此代码并共享结果的执行者。最后，人工智能撰稿人甚至可以最终编写技术文档！此外，如果愿意，可以选择保留“人类参与循环”。在任何步骤中，都可以引入人类进行纠正，无论是更正需求、更改计划的用户验收、更改用户界面的美观性还是 API 的功能。人类开始在比仅仅编写软件更高的层面上发挥作用。这正是让这种多主体协作如此灵活、强大和快速的原因</p>
<p><img src="/../asset_aiswe/1_Jue4gSBD6pRCN7n2zRk56A.webp"></p>
<p>具体来说，研究人员已经确定了以下好处。</p>
<ul>
<li>提高生产力</li>
<li>提高代码质量</li>
<li>可扩展性和适应性</li>
<li>简化的调试</li>
<li>减少人为错误</li>
</ul>
<p>虽然其中一些显而易见，例如减少人为错误和提高生产力，但其他发现也非常引人注目。正如英伟达首席执行官在 2017 年预言的那样：</p>
<blockquote>
<p>软件正在吞噬世界，但人工智能将吞噬软件</p>
</blockquote>
<h3 id="退后一步"><a href="#退后一步" class="headerlink" title="退后一步"></a>退后一步</h3><p>引用 Salesforce AI 研究团队 [6] 的话：</p>
<blockquote>
<p>“我们可能很快就会达到这样的地步，即项目需要对话式人工智能编程等技术，以便创建未来极其复杂的软件系统——无论是所需的大规模，还是人类程序员团队无法自行完成的时间框架。”</p>
</blockquote>
<p>作为一名软件工程领导者，我清楚地看到，在每家技术公司中，基于人工智能的软件工程都会有一席之地。我相信，随着人工智能原生公司的兴起，我们将在 2024 年开始看到这一点的曙光。</p>
<p>出现了这两个问题：</p>
<ol>
<li>这将如何开始在业务中显现，以及</li>
<li>这对现有公司中的 IT 组织意味着什么</li>
</ol>
<p>接下来在”商业模式与 IT 组织：做好迎接AI冲击的准备！”中回答这些问题。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/designing-ai-driven-software-engineering-teams-8afd8de13f1a">https://towardsdatascience.com/designing-ai-driven-software-engineering-teams-8afd8de13f1a</a></p>
<h3 id="术语表"><a href="#术语表" class="headerlink" title="术语表"></a>术语表</h3><p>AI Agent：协同工作的 AI Agent 的多 Agent 超集</p>
<p>AI agent：由通用或专门的大语言模型支持的 Agent</p>
<p>AI Teamlet：与 AI Agent 相同</p>
<p>AI-SWE：人工智能驱动的软件工程：由人工智能代理和人工智能团队编写、调试、构建和部署的软件。</p>
<p>CI&#x2F;CD：持续集成和持续交付</p>
<p>CoE：卓越中心</p>
<p>CRM：客户关系管理系统</p>
<p>H-SWE：以人为驱动的软件工程：由人类调试、构建和部署的软件</p>
<p>MVP：最小可行产品</p>
<p>OSS：开源软件</p>
<p>RAG：检索增强生成</p>
<p>SDLC：软件开发生命周期</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/01/15/hello-world/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/01/15/hello-world/" class="post-title-link" itemprop="url">Hello World</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-01-15 21:00:16 / 修改时间：21:50:01" itemprop="dateCreated datePublished" datetime="2024-01-15T21:00:16+08:00">2024-01-15</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>367</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>1 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/6/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Howard Huang</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">638k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">19:19</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">
    <!--由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动-->
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.33/fancybox/fancybox.umd.js" integrity="sha256-+2+qOqR8CKoHh/AsVR9k2qaDBKWjYNC2nozhYmv5j9k=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  



  <script src="/js/third-party/fancybox.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
