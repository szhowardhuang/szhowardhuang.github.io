<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.33/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"szhowardhuang.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="这篇论文：基于分页注意力的大型语言模型服务的高效内存管理，提供了详细技术理解。 LLM推理的 GPU 内存消耗在LLM推理过程中，主要的 GPU 内存消耗是权重（模型参数）、键值缓存、激活值和临时缓冲区及开销 模型参数（权重）模型参数是神经网络在训练过程中学习到的数值（权重和偏置）。这些参数定义了模型如何处理输入数据以生成输出。 模型大小对 GPU 内存的影响  直接关系：模型越大（参数越多），所">
<meta property="og:type" content="article">
<meta property="og:title" content="如何估算大模型推理占用的VRAM">
<meta property="og:url" content="https://szhowardhuang.github.io/2024/09/26/GPUMemoryNeed4LLMs/index.html">
<meta property="og:site_name" content="嵌入式老兵博客">
<meta property="og:description" content="这篇论文：基于分页注意力的大型语言模型服务的高效内存管理，提供了详细技术理解。 LLM推理的 GPU 内存消耗在LLM推理过程中，主要的 GPU 内存消耗是权重（模型参数）、键值缓存、激活值和临时缓冲区及开销 模型参数（权重）模型参数是神经网络在训练过程中学习到的数值（权重和偏置）。这些参数定义了模型如何处理输入数据以生成输出。 模型大小对 GPU 内存的影响  直接关系：模型越大（参数越多），所">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://szhowardhuang.github.io/assets_llmvram/image-20240926112552-jzkg9ms.png">
<meta property="og:image" content="https://szhowardhuang.github.io/assets_llmvram/image-20240926112611-0xo99sm.png">
<meta property="article:published_time" content="2024-09-26T03:58:47.043Z">
<meta property="article:modified_time" content="2024-09-26T04:02:15.617Z">
<meta property="article:author" content="Howard Huang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://szhowardhuang.github.io/assets_llmvram/image-20240926112552-jzkg9ms.png">


<link rel="canonical" href="https://szhowardhuang.github.io/2024/09/26/GPUMemoryNeed4LLMs/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://szhowardhuang.github.io/2024/09/26/GPUMemoryNeed4LLMs/","path":"2024/09/26/GPUMemoryNeed4LLMs/","title":"如何估算大模型推理占用的VRAM"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>如何估算大模型推理占用的VRAM | 嵌入式老兵博客</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">嵌入式老兵博客</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#LLM%E6%8E%A8%E7%90%86%E7%9A%84-GPU-%E5%86%85%E5%AD%98%E6%B6%88%E8%80%97"><span class="nav-number">1.</span> <span class="nav-text">LLM推理的 GPU 内存消耗</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%EF%BC%88%E6%9D%83%E9%87%8D%EF%BC%89"><span class="nav-number">2.</span> <span class="nav-text">模型参数（权重）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%94%AE%E5%80%BC-KV-%E7%BC%93%E5%AD%98"><span class="nav-number">3.</span> <span class="nav-text">键值(KV)缓存</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AF%8F%E4%B8%AA%E4%BB%A4%E7%89%8C%E7%9A%84%E6%80%BB%E5%90%91%E9%87%8F%E6%95%B0%EF%BC%9A"><span class="nav-number">3.1.</span> <span class="nav-text">每个令牌的总向量数：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%AF%8F%E4%B8%AA%E4%BB%A4%E7%89%8C%E7%9A%84%E5%86%85%E5%AD%98%EF%BC%9A"><span class="nav-number">3.2.</span> <span class="nav-text">计算每个令牌的内存：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-%E6%BF%80%E6%B4%BB%E5%92%8C%E4%B8%B4%E6%97%B6%E7%BC%93%E5%86%B2%E5%8C%BA"><span class="nav-number">4.</span> <span class="nav-text">3. 激活和临时缓冲区</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-%E5%86%85%E5%AD%98%E5%BC%80%E9%94%80"><span class="nav-number">5.</span> <span class="nav-text">4. 内存开销</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97-GPU-%E5%86%85%E5%AD%98%E9%9C%80%E6%B1%82"><span class="nav-number">6.</span> <span class="nav-text">计算 GPU 内存需求</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%80%90%E6%AD%A5%E8%AE%A1%E7%AE%97%EF%BC%9A"><span class="nav-number">6.1.</span> <span class="nav-text">逐步计算：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%B9%E4%BA%8E-13B-%E6%A8%A1%E5%9E%8B%EF%BC%9A"><span class="nav-number">6.2.</span> <span class="nav-text">对于 13B 模型：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#GPU-%E5%86%85%E5%AD%98%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-number">7.</span> <span class="nav-text">GPU 内存的优化方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E9%A1%B5%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-number">7.1.</span> <span class="nav-text">分页注意力</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%A4%E6%8D%A2-KV-%E7%BC%93%E5%AD%98%E5%88%B0-CPU-%E5%86%85%E5%AD%98"><span class="nav-number">7.2.</span> <span class="nav-text">交换 KV 缓存到 CPU 内存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#vLLM"><span class="nav-number">7.3.</span> <span class="nav-text">vLLM</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Howard Huang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">70</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/09/26/GPUMemoryNeed4LLMs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="如何估算大模型推理占用的VRAM | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          如何估算大模型推理占用的VRAM
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-09-26 11:58:47 / 修改时间：12:02:15" itemprop="dateCreated datePublished" datetime="2024-09-26T11:58:47+08:00">2024-09-26</time>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>3k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>5 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>这篇论文：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2309.06180">基于分页注意力的大型语言模型服务的高效内存管理</a>，提供了详细技术理解。</p>
<h1 id="LLM推理的-GPU-内存消耗"><a href="#LLM推理的-GPU-内存消耗" class="headerlink" title="LLM推理的 GPU 内存消耗"></a>LLM推理的 GPU 内存消耗</h1><p>在LLM推理过程中，主要的 GPU 内存消耗是权重（模型参数）、键值缓存、激活值和临时缓冲区及开销</p>
<h1 id="模型参数（权重）"><a href="#模型参数（权重）" class="headerlink" title="模型参数（权重）"></a>模型参数（权重）</h1><p>模型参数是神经网络在训练过程中学习到的数值（权重和偏置）。这些参数定义了模型如何处理输入数据以生成输出。</p>
<p><strong>模型大小对 GPU 内存的影响</strong></p>
<ul>
<li>直接关系：模型越大（参数越多），所需的 GPU 内存就越多来存储这些权重。</li>
<li>内存计算：每个参数通常在使用半精度（FP16）格式时需要 2 个字节，这在推理中很常见，节省内存而不显著损失精度。</li>
</ul>
<p><strong>让我们看看一些模型大小：</strong></p>
<ol>
<li><p>考虑一个小型LLM，具有 3.45 亿个参数：</p>
<p> 所需内存：3.45 亿 × 2 字节 &#x3D; 690 MB。这可以舒适地放在单个 GPU 上。</p>
</li>
<li><p>现在如果我们采用 llama2-13b 模型：</p>
<p> 参数量为 13B，所需内存为：13B × 2 字节 &#x3D; 26 GB。在这种情况下，我们需要一块具有 40 GB 内存的 A100 GPU。</p>
</li>
<li><p>如果我们看看 supposedly 的 GPT-3，175B参数模型：</p>
<p> 所需内存：175B × 2 字节 &#x3D; 350 GB</p>
</li>
</ol>
<p>我们至少需要 9 个 A100 GPU 来保存模型权重。</p>
<h1 id="键值-KV-缓存"><a href="#键值-KV-缓存" class="headerlink" title="键值(KV)缓存"></a>键值(KV)缓存</h1><p>KV 缓存存储生成序列中每个令牌所需的中间表示。简单来说，当模型一次生成一个令牌时，它需要记住之前的令牌以保持上下文。KV 缓存存储到目前为止生成的每个令牌的键和值向量，使模型能够关注过去的令牌，而无需重新计算它们。</p>
<p> <strong>工作原理</strong>：</p>
<ul>
<li>键和值：对于每个令牌，模型在注意力机制中计算一个键和一个值向量。</li>
<li>存储：这些向量存储在 KV 缓存中，并在后续步骤中用于生成新令牌。</li>
</ul>
<p><strong>序列长度和并发请求的影响：</strong></p>
<ul>
<li>更长的序列：更多的令牌意味着 KV 缓存中更多的条目，增加内存使用。</li>
<li>多个用户：同时处理多个请求会增加所需的 KV 缓存内存。</li>
</ul>
<p>如何计算每个令牌的 KV 缓存大小：</p>
<h2 id="每个令牌的总向量数："><a href="#每个令牌的总向量数：" class="headerlink" title="每个令牌的总向量数："></a>每个令牌的总向量数：</h2><blockquote>
<p>层数 (L)：模型深度    x    隐藏层大小 (H)：每个向量的维度。</p>
</blockquote>
<p>以 llama-13b 模型为参考，假设该模型具有：</p>
<ul>
<li>层数（L）：40 层</li>
<li>隐藏层大小（H）：5120 维</li>
</ul>
<h2 id="计算每个令牌的内存："><a href="#计算每个令牌的内存：" class="headerlink" title="计算每个令牌的内存："></a><strong>计算每个令牌的内存：</strong></h2><ol>
<li>键向量：</li>
</ol>
<ul>
<li>总元素：L 层 × H 维度 &#x3D; 40 × 5120 &#x3D; 204,800 个元素</li>
<li>内存大小：204,800 个元素 × 2 字节 (FP16) &#x3D; 409,600 字节 (或 400 KB)</li>
</ul>
<ol start="2">
<li>值向量：</li>
</ol>
<ul>
<li>与关键向量相同：也为 400 KB</li>
</ul>
<p><strong>每个令牌的总 KV 缓存：</strong></p>
<ul>
<li>键 + 值：400 KB + 400 KB &#x3D; 800 KB</li>
</ul>
<p><em><strong>现在考虑输出 2000 个令牌：</strong></em></p>
<blockquote>
<p>800 KB&#x2F;令牌 × 2000 令牌 &#x3D; 1.6 GB 每个序列<br>如果我们有 10 个并发请求（同时为 10 个用户提供模型服务）：<br>16 GB 的 KV 缓存内存</p>
</blockquote>
<p>KV 缓存随着序列长度和并发请求数量线性增长。从论文中了解到，KV 缓存在LLM服务期间可能消耗超过 30%的 GPU 内存。</p>
<h1 id="3-激活和临时缓冲区"><a href="#3-激活和临时缓冲区" class="headerlink" title="3. 激活和临时缓冲区"></a>3. 激活和临时缓冲区</h1><p>激活是推理过程中神经网络层的输出，临时缓冲区用于中间计算。激活和缓冲区通常消耗的内存少于模型权重和 KV 缓存。</p>
<blockquote>
<p><em><strong>它们可能使用大约 5%–10%的总 GPU 内存。</strong></em></p>
</blockquote>
<p>尽管它们的大小较小，但激活是模型在每一层计算输出所必需的。它们在前向传播过程中创建并丢弃，但仍然需要足够的内存分配。</p>
<h1 id="4-内存开销"><a href="#4-内存开销" class="headerlink" title="4. 内存开销"></a>4. 内存开销</h1><p>额外的内存使用是由于内存分配和使用效率低下造成的。以下是快速概述以便理解：</p>
<p><strong>碎片化：</strong></p>
<ul>
<li>内部碎片: 当分配的内存块未被完全利用时发生。</li>
<li>外部碎片化：随着时间的推移，空闲内存被分割成小块，导致在需要时很难分配较大的连续块。</li>
</ul>
<p><strong>中间计算：</strong></p>
<ul>
<li>临时数据：矩阵乘法等操作可能会创建消耗内存的临时张量。</li>
</ul>
<p><strong>低效内存管理的影响：</strong></p>
<ul>
<li>性能降低：浪费的内存可能限制系统可以处理的并发请求数量。</li>
<li>较低的吞吐量：低效可能导致延迟并降低响应用户的整体速度。</li>
</ul>
<p>示例：如果碎片化浪费了 40 GB GPU 上的 20%内存，那就是 8 GB的内存。</p>
<h1 id="计算-GPU-内存需求"><a href="#计算-GPU-内存需求" class="headerlink" title="计算 GPU 内存需求"></a>计算 GPU 内存需求</h1><p>现在我们已经对关键概念有了足够的理解，开始计算完整的 GPU 内存需求！</p>
<h2 id="逐步计算："><a href="#逐步计算：" class="headerlink" title="逐步计算："></a>逐步计算：</h2><p>计算任何模型的需求基本上需要以下内容：权重、KV 缓存、激活值、临时缓冲区和开销。参考 llama-2 13B 模型，公式为：</p>
<p>所需总内存：权重 + KV 缓存 + 激活和开销</p>
<h2 id="对于-13B-模型："><a href="#对于-13B-模型：" class="headerlink" title="对于 13B 模型："></a>对于 13B 模型：</h2><p><strong>权重</strong> &#x3D; 参数数量 × 每个参数的字节数<br><strong>总 KV 缓存内存</strong> &#x3D; 每个令牌的 KV 缓存内存 × 序列长度 × 序列数量<br><strong>激活和开销</strong> &#x3D; 5–10% 的总 GPU 内存</p>
<blockquote>
<p>激活和开销通常占用模型参数和 KV 缓存使用的总 GPU 内存的约 5-10%，用 10%来估算</p>
</blockquote>
<p><strong>假设一个 8192 Token 模型，支持 10 个并行请求</strong><strong>:</strong></p>
<p><strong>权重</strong> &#x3D; 130 亿 × 2 字节 &#x3D; 26 GB<br><strong>总 KV 缓存内存</strong> &#x3D; 800 KB × 8192 令牌 × 10 并发请求 &#x3D; 66 GB<br><strong>激活和开销</strong> &#x3D; 0.1 × (26 GB + 66 GB) &#x3D; 9.2 GB</p>
<p>所需总内存：26 GB + 66 GB + 9.2 GB &#x3D; 101.2 GB</p>
<p>所以，您至少需要 3 个 A100 40GB GPU 来运行 llama-2 13B 模型。</p>
<p><strong>假设一个 8192 Token 模型，支持 1个并行请求</strong><strong>:</strong></p>
<p><strong>权重</strong> &#x3D; 130 亿 × 2 字节 &#x3D; 26 GB<br><strong>总 KV 缓存内存</strong> &#x3D; 800 KB × 8192 令牌 × 1 并发请求 &#x3D; 6.6 GB<br><strong>激活和开销</strong> &#x3D; 0.1 × (26 GB + 6.6 GB) &#x3D; 3.26 GB</p>
<p>所需总内存：26 GB + 6.6 GB + 3.26 GB &#x3D; 29.86 GB</p>
<p>所以，您需要 1 个 A100 40GB GPU 来运行 llama-2 13B 模型。</p>
<p>基于公开可用的模型大小和参数信息，1个用户计算如下所示：</p>
<p><img src="/../assets_llmvram/image-20240926112552-jzkg9ms.png" alt="image.png"></p>
<p>10个用户计算如下所示：</p>
<p><img src="/../assets_llmvram/image-20240926112611-0xo99sm.png" alt="image.png"></p>
<p>在处理多个请求时，内存消耗显著增加。这里的一个主要消耗是 KV 缓存，因为模型权重和开销保持不变，随着令牌数量和并发请求的增加，KV 缓存会大幅增加，直接增加内存消耗。</p>
<h1 id="GPU-内存的优化方法"><a href="#GPU-内存的优化方法" class="headerlink" title="GPU 内存的优化方法"></a>GPU 内存的优化方法</h1><p>我们通常静态分配键值（KV）缓存的内存，为每个请求预留最大可能的空间。这导致了过度分配，因为系统为最长可能的序列保留空间，即使实际序列更短。动态管理的必要性就体现出来。</p>
<h2 id="分页注意力"><a href="#分页注意力" class="headerlink" title="分页注意力"></a>分页注意力</h2><p>分页注意力受到操作系统管理内存方式的启发，将虚拟内存分页的概念应用于 KV 缓存。这允许 KV 缓存数据存储在非连续的内存块（页面）中，而不需要一个大的连续块。分页注意力通过动态内存分配工作，即根据需要为 KV 缓存分配内存，而不预先分配最大序列长度。注意力机制可以无缝地从不同的内存位置检索 KV 缓存数据。</p>
<p>分页注意力的好处在于通过使用更小的内存块来减少碎片化，最小化由于碎片化造成的空间浪费，提高内存利用率。</p>
<h2 id="交换-KV-缓存到-CPU-内存"><a href="#交换-KV-缓存到-CPU-内存" class="headerlink" title="交换 KV 缓存到 CPU 内存"></a><strong>交换 KV 缓存到 CPU 内存</strong></h2><p>当 GPU 内存满时，暂时将 KV 缓存数据从 GPU 内存移动到 CPU 内存。同时释放 GPU 内存以处理新请求。</p>
<p>缺点：</p>
<ul>
<li>增加延迟增加：从 CPU 内存访问数据比从 GPU 内存慢。</li>
<li>数据传输开销：在 CPU 和 GPU 之间移动数据可能会消耗带宽和时间。</li>
</ul>
<h2 id="vLLM"><a href="#vLLM" class="headerlink" title="vLLM"></a>vLLM</h2><p>vLLM是基于分页注意力构建的高吞吐量LLM服务系统。其核心目的是在推理过程中高效管理 GPU 内存，特别是 KV 缓存。理论上，vLLM 是近零内存浪费的解决方案。通过动态分配和非连续存储，它几乎消除了内存浪费。在原则上，它还支持在请求内和请求之间共享 KV 缓存数据，特别适用于高级解码方法。基于这些，vLLM 可以处理更大的批量和更多的并发请求，从而增强整体性能。但是即使经过优化，仍可能出现 GPU 内存不足的情况。vLLM 通过交换KV 缓存到 CPU 内存来解决这个问题。</p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/09/20/aiassistedprogramming10/" rel="prev" title="第10章  重点总结">
                  <i class="fa fa-angle-left"></i> 第10章  重点总结
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/10/08/finetuneembed/" rel="next" title="特定领域Embed模型的微调指南">
                  特定领域Embed模型的微调指南 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Howard Huang</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">638k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">19:19</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">
    <!--由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动-->
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.33/fancybox/fancybox.umd.js" integrity="sha256-+2+qOqR8CKoHh/AsVR9k2qaDBKWjYNC2nozhYmv5j9k=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  



  <script src="/js/third-party/fancybox.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
