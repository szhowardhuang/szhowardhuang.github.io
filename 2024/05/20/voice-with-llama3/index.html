<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.33/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"szhowardhuang.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="技术栈Whisper: 由 OpenAI 开发，Whisper 擅长将口语转录为文本。其理解和处理多种语言的能力使其成为任何基于语音的应用程序的必不可少的工具。 LangChain 用于协调组件处理模型和数据库的复杂用户交互。 矢量数据库（Qdrant）：Qdrant 旨在高效处理高维数据，使其非常适用于依赖机器学习和大规模数据检索的应用程序。 检索增强生成（RAG）：RAG 结合了检索和生成模型">
<meta property="og:type" content="article">
<meta property="og:title" content="用llama3实现语音助手">
<meta property="og:url" content="https://szhowardhuang.github.io/2024/05/20/voice-with-llama3/index.html">
<meta property="og:site_name" content="嵌入式老兵博客">
<meta property="og:description" content="技术栈Whisper: 由 OpenAI 开发，Whisper 擅长将口语转录为文本。其理解和处理多种语言的能力使其成为任何基于语音的应用程序的必不可少的工具。 LangChain 用于协调组件处理模型和数据库的复杂用户交互。 矢量数据库（Qdrant）：Qdrant 旨在高效处理高维数据，使其非常适用于依赖机器学习和大规模数据检索的应用程序。 检索增强生成（RAG）：RAG 结合了检索和生成模型">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://szhowardhuang.github.io/asset_voicellama3/01.png">
<meta property="og:image" content="https://szhowardhuang.github.io/asset_voicellama3/02.png">
<meta property="og:image" content="https://szhowardhuang.github.io/asset_voicellama3/03.png">
<meta property="og:image" content="https://szhowardhuang.github.io/asset_voicellama3/04.png">
<meta property="article:published_time" content="2024-05-20T07:51:55.128Z">
<meta property="article:modified_time" content="2024-05-27T09:28:26.182Z">
<meta property="article:author" content="Howard Huang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://szhowardhuang.github.io/asset_voicellama3/01.png">


<link rel="canonical" href="https://szhowardhuang.github.io/2024/05/20/voice-with-llama3/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://szhowardhuang.github.io/2024/05/20/voice-with-llama3/","path":"2024/05/20/voice-with-llama3/","title":"用llama3实现语音助手"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>用llama3实现语音助手 | 嵌入式老兵博客</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">嵌入式老兵博客</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8A%80%E6%9C%AF%E6%A0%88"><span class="nav-number">1.</span> <span class="nav-text">技术栈</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96"><span class="nav-number">2.</span> <span class="nav-text">安装依赖</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AF%BC%E5%85%A5%E5%BA%93"><span class="nav-number">3.</span> <span class="nav-text">导入库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E8%AF%AD%E9%9F%B3%E5%8A%A9%E6%89%8B%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="nav-number">4.</span> <span class="nav-text">处理语音助手的数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RAG%E7%AE%80%E8%A6%81%E6%A6%82%E8%BF%B0"><span class="nav-number">5.</span> <span class="nav-text">RAG简要概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%87%E6%9C%AC%E5%88%86%E5%89%B2%E5%99%A8"><span class="nav-number">6.</span> <span class="nav-text">文本分割器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%87%E6%A1%A3%E5%8A%A0%E8%BD%BD%E5%99%A8"><span class="nav-number">7.</span> <span class="nav-text">文档加载器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E9%85%8D%E7%BD%AE"><span class="nav-number">8.</span> <span class="nav-text">模型配置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%BE%E7%BD%AE%E6%9F%A5%E8%AF%A2%E7%AE%A1%E9%81%93%E5%B9%B6%E5%88%9D%E5%A7%8B%E5%8C%96%E7%AE%A1%E9%81%93"><span class="nav-number">9.</span> <span class="nav-text">设置查询管道并初始化管道</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%84%E7%90%86%E6%A8%A1%E5%9E%8B%E5%8A%A0%E8%BD%BD%E5%B9%B6%E5%9B%9E%E9%80%80%E5%88%B0%E6%9C%AC%E5%9C%B0%E8%B5%84%E6%BA%90"><span class="nav-number">10.</span> <span class="nav-text">处理模型加载并回退到本地资源</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%9B%86%E6%88%90-Qdrant-%E7%94%A8%E4%BA%8E%E5%B5%8C%E5%85%A5%E5%BC%8F%E5%AD%98%E5%82%A8%E5%92%8C%E6%A3%80%E7%B4%A2"><span class="nav-number">11.</span> <span class="nav-text">集成 Qdrant 用于嵌入式存储和检索</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%BE%E7%BD%AE%E6%A3%80%E7%B4%A2%E5%99%A8"><span class="nav-number">12.</span> <span class="nav-text">设置检索器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B5%8B%E8%AF%95%E5%92%8C%E5%8F%AF%E8%A7%86%E5%8C%96-RAG-%E7%B3%BB%E7%BB%9F"><span class="nav-number">13.</span> <span class="nav-text">测试和可视化 RAG 系统</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Llama-3-%E5%92%8C-Whisper-%E7%9A%84%E6%96%87%E6%9C%AC%E8%BD%AC%E8%AF%AD%E9%9F%B3%E5%A4%84%E7%90%86"><span class="nav-number">14.</span> <span class="nav-text">Llama 3 和 Whisper 的文本转语音处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-number">15.</span> <span class="nav-text">参考</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Howard Huang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">55</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/05/20/voice-with-llama3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="用llama3实现语音助手 | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          用llama3实现语音助手
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-05-20 15:51:55" itemprop="dateCreated datePublished" datetime="2024-05-20T15:51:55+08:00">2024-05-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-05-27 17:28:26" itemprop="dateModified" datetime="2024-05-27T17:28:26+08:00">2024-05-27</time>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>13k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>24 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h2 id="技术栈"><a href="#技术栈" class="headerlink" title="技术栈"></a>技术栈</h2><p>Whisper: 由 OpenAI 开发，Whisper 擅长将口语转录为文本。其理解和处理多种语言的能力使其成为任何基于语音的应用程序的必不可少的工具。</p>
<p>LangChain 用于协调组件处理模型和数据库的复杂用户交互。</p>
<p>矢量数据库（Qdrant）：Qdrant 旨在高效处理高维数据，使其非常适用于依赖机器学习和大规模数据检索的应用程序。</p>
<p>检索增强生成（RAG）：RAG 结合了检索和生成模型的优点，使我们的语音助手能够利用大量信息数据库生成明智和具有上下文相关性的回应。</p>
<h2 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h2><ul>
<li><p>transformers（4.33.0）：提供了各种预构建模型，用于文本翻译和摘要等语言任务，使其成为语言项目的关键工具。</p>
</li>
<li><p>accelerate （0.22.0）：帮助在不需要改变太多代码的情况下，在不同类型的计算机硬件上运行机器学习模型，如 CPU 或 GPU。</p>
</li>
<li><p>einops (0.6.1)：使得在机器学习中使用的数据结构更易于处理和改变形状，有助于构建复杂模型。</p>
</li>
<li><p>langchain (0.0.300)：用于将不同的语言技术结合到一个应用程序中，特别适用于需要多个处理步骤的项目。</p>
</li>
<li><p>xformers（0.0.22.post7）：提供模型的部分，这些部分在学习和使用阶段都能够高效处理数据。</p>
</li>
<li><p>bitsandbytes-windows（windows版本）：有助于更快地训练深度学习模型，并且占用更少的内存，非常适合处理大型数据集。</p>
</li>
<li><p>sentence_transformers (2.2.2)：基于 transformers 库构建详细特征，对于需要理解文本之间相似性的任务非常重要。</p>
</li>
</ul>
<p>让我们首先建立一个虚拟环境并安装库。打开命令行界面 ，然后运行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a virtual environment</span></span><br><span class="line">conda create -n llama3-whisper python=3.11</span><br><span class="line">conda activate llama3-whisper</span><br></pre></td></tr></table></figure>

<p>我的CUDA版本是12.1，如果在线安装torch 持续中断失败，可以去下载 <a target="_blank" rel="noopener" href="https://download.pytorch.org/whl/">wheel</a> 文件，然后本地安装。</p>
<p>在线安装torch：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121</span><br></pre></td></tr></table></figure>

<p>本地安装torch，进入wheel文件所在的目录:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install <span class="string">&quot;torch-2.1.0+cu121-cp311-cp311-win_amd64.whl&quot;</span> </span><br><span class="line">pip install <span class="string">&quot;torchaudio-2.1.0+cu121-cp311-cp311-win_amd64.whl&quot;</span></span><br><span class="line">pip install <span class="string">&quot;torchvision-0.16.0+cu121-cp311-cp311-win_amd64.whl&quot;</span></span><br></pre></td></tr></table></figure>
<p>安装完torch后，安装xformers， 因为xformers和torch的版本对应没有文档，所以要一起安装来约束xformers：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0  xformers  --index-url https://download.pytorch.org/whl/cu121</span><br></pre></td></tr></table></figure>

<p>约束后，可以看到安装的xformers版本，所以可以用以下命令直接安装xformers。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install xformers==0.0.22.post7 --index-url https://download.pytorch.org/whl/cu121</span><br></pre></td></tr></table></figure>

<p>继续安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Install dependencies</span></span><br><span class="line">pip3 install openai</span><br><span class="line">pip3 install transformers==4.33.0 </span><br><span class="line">pip3 install accelerate==0.22.0 </span><br><span class="line">pip3 install einops==0.6.1 </span><br><span class="line">pip3 install langchain==0.0.300 </span><br><span class="line">pip3 install bitsandbytes-windows</span><br><span class="line">pip3 install sentence_transformers==2.2.2</span><br><span class="line">pip3 install arxiv</span><br><span class="line">pip3 install huggingface_hub</span><br><span class="line">pip3 install optimum</span><br><span class="line">pip3 install <span class="string">&quot;git+https://github.com/PanQiWei/AutoGPTQ.git@v0.7.1&quot;</span> </span><br></pre></td></tr></table></figure>

<p>为了准备从 PDF 文件中提取数据、执行 OCR 并创建嵌入，以进行高级数据处理和检索，我们还需要安装一些软件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pip3 install unstructured</span><br><span class="line">pip3 install <span class="string">&quot;unstructured[pdf]&quot;</span></span><br><span class="line">pip3 install poppler-utils</span><br><span class="line">pip3 install pytesseract</span><br><span class="line">pip3 install grpcio-tools==1.60.1</span><br><span class="line">pip3 install qdrant-client==1.7.2</span><br><span class="line">pip3 install WhisperSpeech</span><br><span class="line">pip3 install rich</span><br></pre></td></tr></table></figure>
<p>由于我的win10在线安装tesseract-ocr不成功，改成<a target="_blank" rel="noopener" href="https://github.com/simonflueckiger/tesserocr-windows_build/releases">wheel</a>安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install tesserocr-2.7.0-cp311-cp311-win_amd64.whl</span><br></pre></td></tr></table></figure>

<p>设置系统环境变量，window如下图：<br><img src="/../asset_voicellama3/01.png"></p>
<p>注册Hugging Face，生成token. 登录 Hugging Face Hub, 预先下载模型，并保存到本地。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loging to Huggingface Hub</span></span><br><span class="line">huggingface-cli login</span><br><span class="line">huggingface-cli download --resume-download astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit</span><br></pre></td></tr></table></figure>

<h2 id="导入库"><a href="#导入库" class="headerlink" title="导入库"></a>导入库</h2><p>导入必要的库，包括模型交互、文档处理和嵌入管理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> HTTPError</span><br><span class="line"><span class="keyword">import</span> arxiv</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> cuda, bfloat16</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM, GPTQConfig</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> HuggingFacePipeline</span><br><span class="line"><span class="keyword">from</span> langchain.document_loaders <span class="keyword">import</span> PyPDFLoader,DirectoryLoader,WebBaseLoader</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> RecursiveCharacterTextSplitter,CharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> HuggingFaceEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> RetrievalQA</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> Qdrant</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> whisperspeech.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> rich.console <span class="keyword">import</span> Console</span><br><span class="line"><span class="keyword">from</span> rich.markdown <span class="keyword">import</span> Markdown</span><br></pre></td></tr></table></figure>

<h2 id="处理语音助手的数据"><a href="#处理语音助手的数据" class="headerlink" title="处理语音助手的数据"></a>处理语音助手的数据</h2><p>为 AI 应用构建数据管道, 数据管道对于高效管理和处理应用程序中的数据至关重要，特别是启用了带 RAG 的语音助手等复杂应用程序。</p>
<p>这些管道通常涉及五个关键阶段：</p>
<ul>
<li><p>收集： 在这个阶段，数据从各种来源收集，包括数据存储、数据流和应用程序。对于语音助手来说，这意味着从用户互动、音频输入以及内部和外部数据库收集数据。数据可以来自语音助手需要交互的远程设备、应用程序或业务系统。典型的工具有 Apache Nifi、Apache Flume、Talend 和自定义 API。</p>
</li>
<li><p>摄取：在摄取过程中，收集的数据被加载到系统中，并在事件队列中进行组织。对于语音助手，这涉及捕获音频输入，将其转录为文本，并将其排队等待进一步处理。摄取过程确保所有传入数据都准备好进行实时或批处理。典型工具包括 Apache Kafka、AWS Kinesis、Google Cloud Pub&#x2F;Sub、Apache Airflow。</p>
</li>
<li><p>存储：在摄取后，组织好的数据存储在各种存储解决方案中，如数据仓库、数据湖和数据湖屋。在语音助手的背景下，这包括存储转录、用户查询以及从 RAG 系统检索的文档。存储系统确保数据可供未来处理和分析。典型工具有 Amazon S3、Google Cloud Storage、Azure Data Lake、Snowflake、Apache Hudi、Delta Lake。</p>
</li>
<li><p>处理：在这个阶段，数据经历转换任务，如聚合、清洗和操作，以确保它符合所需的标准。对于语音助手，这意味着将文本数据转换为向量，压缩它，并将其分区以实现高效检索。为了确保数据始终是最新和准确的，使用批处理（一次处理大型数据集）和流处理（实时处理数据）技术。典型的工具有 Apache Spark、Apache Flink、Databricks、AWS Glue、Google Cloud Dataflow。</p>
</li>
<li><p>消费：最后阶段涉及将处理后的数据提供给用户使用。在语音助手的背景下，这意味着使系统能够准确理解和回应用户的查询。它还可以支持决策引擎和面向用户的应用程序，使语音助手能够对用户请求提供相关和及时的响应。典型工具有 Tableau、Power BI、Looker、Elasticsearch、Kibana、Apache Superset、自定义仪表板。</p>
</li>
</ul>
<p>让我们创建一个目录，搜索并下载“LLM”搜索词的Arxiv论文：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">dirpath = <span class="string">&quot;arxiv_papers&quot;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(dirpath):</span><br><span class="line">   os.makedirs(dirpath)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_papers</span>():</span><br><span class="line">    search = arxiv.Search(</span><br><span class="line">    query = <span class="string">&quot;LLM&quot;</span>, <span class="comment"># your query length is limited by ARXIV_MAX_QUERY_LENGTH which is 300 characters</span></span><br><span class="line">    max_results = <span class="number">10</span>,</span><br><span class="line">    sort_by = arxiv.SortCriterion.LastUpdatedDate, <span class="comment"># you can also use SubmittedDate or Relevance</span></span><br><span class="line">    sort_order = arxiv.SortOrder.Descending</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> result <span class="keyword">in</span> search.results():</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                result.download_pdf(dirpath=dirpath)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;-&gt; Paper id <span class="subst">&#123;result.get_short_id()&#125;</span> with title &#x27;<span class="subst">&#123;result.title&#125;</span>&#x27; is downloaded.&quot;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">except</span> FileNotFoundError:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;File not found&quot;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">except</span> HTTPError:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Forbidden&quot;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">except</span> ConnectionResetError <span class="keyword">as</span> e:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Connection reset by peer&quot;</span>)</span><br><span class="line">                time.sleep(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<h2 id="RAG简要概述"><a href="#RAG简要概述" class="headerlink" title="RAG简要概述"></a>RAG简要概述</h2><p>RAG 工作流帮助我们管理和利用来自各种来源的数据，以提供准确和相关的结果。</p>
<p><img src="/../asset_voicellama3/02.png"></p>
<ul>
<li><p>数据加载：从不同来源收集数据，如文本文件、PDF、网站、数据库或 API。例如，Llama Hub 提供许多连接器，使这一步骤更容易。</p>
</li>
<li><p>索引：在索引阶段，系统将原始数据转换为向量嵌入，并对其进行组织。</p>
<ul>
<li><p>矢量化：每个文档或数据片段都被转换为一个高维向量，利用句子转换器等模型捕捉语义含义。</p>
</li>
<li><p>结构化：这些向量然后被组织成一个高效的数据结构，通常是一个 n 维树或哈希映射，从而实现快速的相似性搜索。</p>
</li>
</ul>
</li>
<li><p>存储：保存索引数据和标签，这样您以后就不必再次整理它。</p>
</li>
<li><p>查询: 在查询阶段，系统根据查询向量检索最相关的文档。</p>
<ul>
<li>向量匹配：将查询转换为向量，并使用余弦相似度或其他距离度量标准与索引向量进行比较。</li>
<li>检索：系统检索与查询向量最接近的文档，确保系统能够提供的响应在语境上和用户的请求相关。</li>
</ul>
</li>
<li><p>评估: 评估可能会因其随机性而变得相当具有挑战性。然而，可以用度量标准和工具进行客观评估。</p>
</li>
</ul>
<p>一些示例指标可能包括：忠实度、答案相关性、上下文精度、召回率、相关性和实体召回、答案语义相似度、答案正确性。</p>
<h2 id="文本分割器"><a href="#文本分割器" class="headerlink" title="文本分割器"></a>文本分割器</h2><p>使用 text_splitter 来管理大型文本文档，将它们分成更小、更易管理的块：</p>
<ul>
<li><p>RecursiveCharacterTextSplitter 递归地将文本分割成更小的片段，适用于非常大的文本。它有 2 个主要参数：</p>
<ul>
<li>chunk_size ：每个块的最大字符数（例如，1000 个字符）。</li>
<li>chunk_overlap ：保持上下文的重叠块大小（例如，100 个字符）。</li>
</ul>
<p>  这通常最适合于没有自然分割点的非常大的文本，并通过保持块之间的重叠来防止上下文丢失，确保后续处理具有连续性。</p>
</li>
<li><p>CharacterTextSplitter 根据指定的字符分隔符拆分文本，非常适合具有自然分隔的文本。它有 3 个主要参数</p>
<ul>
<li>separator ：用于分隔的字符（例如， \n 用于换行）。</li>
<li>chunk_size 和 chunk_overlap ：类似于递归分割器，定义块的大小和重叠。</li>
</ul>
<p>  适用于具有明确分界点的文本，如脚本或具有明确定义部分的文档，通过在自然断点处分割文本来确保数据完整性，有助于保持意义和上下文，而无需重叠。</p>
</li>
</ul>
<h2 id="文档加载器"><a href="#文档加载器" class="headerlink" title="文档加载器"></a>文档加载器</h2><p>文档加载器在处理自然语言处理工作流中的不同数据源时至关重要。有以下几种：</p>
<ul>
<li>DirectoryLoader: 从指定目录加载所有文件，通常用于处理多个文本或 PDF 文件。</li>
<li>WebBaseLoader：从指定的 URL 检索文本，从中提取网络内容以进行处理。</li>
<li>PyPDFLoader：专注于从单个 PDF 文件中提取文本以进行进一步分析。</li>
<li>TextLoader：专门设计用于加载纯文本文件，直接读取文本数据以供立即使用。</li>
</ul>
<p>所有的加载器都用于收集数据，然后对数据进行处理，可能用于生成嵌入。</p>
<p>我们将使用 DirectoryLoader 和 RecursiveCharacterTextSplitter 来高效地分块和管理多个文件。</p>
<p>分割器和文档加载器的结合使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">document_loader_spilter</span>():</span><br><span class="line">    papers = []</span><br><span class="line">    loader = DirectoryLoader(dirpath, glob=<span class="string">&quot;./*.pdf&quot;</span>, loader_cls=PyPDFLoader)</span><br><span class="line">    papers = loader.load()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Total number of pages loaded:&quot;</span>, <span class="built_in">len</span>(papers)) <span class="comment"># Total number of pages loaded: 410</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># This merges all papes from all papers into single text block for chunking</span></span><br><span class="line">    full_text = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> paper <span class="keyword">in</span> papers:</span><br><span class="line">        full_text = full_text + paper.page_content</span><br><span class="line">        </span><br><span class="line">    full_text = <span class="string">&quot; &quot;</span>.join(l <span class="keyword">for</span> l <span class="keyword">in</span> full_text.splitlines() <span class="keyword">if</span> l)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(full_text))</span><br><span class="line"></span><br><span class="line">    text_splitter = RecursiveCharacterTextSplitter(</span><br><span class="line">        chunk_size = <span class="number">500</span>,</span><br><span class="line">        chunk_overlap  = <span class="number">50</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    paper_chunks = text_splitter.create_documents([full_text])</span><br><span class="line">    <span class="keyword">return</span> paper_chunks</span><br></pre></td></tr></table></figure>

<h2 id="模型配置"><a href="#模型配置" class="headerlink" title="模型配置"></a>模型配置</h2><p>配置了一个用于语言生成任务的 Meta LLaMA 3 模型,由于我的显卡只有8GB显存，所以只能使用 4-bit 模型。</p>
<p>配置：</p>
<ul>
<li>model_id ：标识具有 80 亿参数的特定 Meta LLaMA 模型，用于高级语言任务。</li>
<li>device ：将模型设置为在 GPU（ “cuda” ）上运行，提高处理速度和效率。</li>
<li>dtype ：使用 torch.float16 来优化内存和计算速度。</li>
</ul>
<p>初始化:</p>
<ul>
<li>tokenizer ：从 Hugging Face 加载一个分词器，将文本预处理为模型可以理解的标记。</li>
<li>model ：使用 AutoModelForCausalLM.from_pretrained 配置初始化模型，用于因果语言建模，模型根据先前文本预测下一个单词。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_llm_model</span>():</span><br><span class="line">    model_id = <span class="string">&quot;astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit&quot;</span></span><br><span class="line">    device = <span class="string">&quot;cuda&quot;</span></span><br><span class="line">    dtype = torch.float16</span><br><span class="line">    </span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(model_id)</span><br><span class="line">    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device, torch_dtype=dtype)</span><br><span class="line">    <span class="keyword">return</span> model, tokenizer</span><br></pre></td></tr></table></figure>

<h2 id="设置查询管道并初始化管道"><a href="#设置查询管道并初始化管道" class="headerlink" title="设置查询管道并初始化管道"></a>设置查询管道并初始化管道</h2><p>现在我们使用 Hugging Face的 transformers 库设置一个 query_pipeline ，旨在简化预训练模型和分词器的使用：</p>
<ul>
<li>model ：指定预训练语言模型。</li>
<li>tokenizer ：将输入文本转换为标记。</li>
<li>torch_dtype ：使用 torch.float16 进行高效计算。</li>
<li>max_length ：将输出限制在 1024 个标记。</li>
<li>device_map ：自动优化模型层的分配到可用硬件。</li>
</ul>
<p>然后使用我们配置的 query_pipeline 初始化一个 HuggingFacePipeline 对象，以便简化文本生成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_query_pipeline</span>(<span class="params">model, tokenizer</span>):</span><br><span class="line">    query_pipeline = transformers.pipeline(</span><br><span class="line">            <span class="string">&quot;text-generation&quot;</span>,</span><br><span class="line">            model=model,</span><br><span class="line">            tokenizer=tokenizer,</span><br><span class="line">            torch_dtype=torch.float16,</span><br><span class="line">            max_length=<span class="number">1024</span>,</span><br><span class="line">            device_map=<span class="string">&quot;auto&quot;</span>,)</span><br><span class="line"></span><br><span class="line">    llm = HuggingFacePipeline(pipeline=query_pipeline)</span><br><span class="line">    <span class="keyword">return</span> llm</span><br></pre></td></tr></table></figure>


<h2 id="处理模型加载并回退到本地资源"><a href="#处理模型加载并回退到本地资源" class="headerlink" title="处理模型加载并回退到本地资源"></a>处理模型加载并回退到本地资源</h2><p>我们现在将从 Hugging Face 的存储库中加载 sentence-transformers&#x2F;all-mpnet-base-v2 嵌入模型，配置为在 CUDA 设备上运行。</p>
<p>如果此过程遇到任何问题，比如连接问题或访问限制，您也可以添加异常以返回使用本地存储的嵌入模型。</p>
<p>通过这种方法，我们的应用程序可以在主要来源不可用时继续使用备用模型进行处理，这有助于我们在不同的运行环境中保持稳健性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_embeddings_model</span>():</span><br><span class="line">    model_name = <span class="string">&quot;sentence-transformers/all-mpnet-base-v2&quot;</span></span><br><span class="line">    model_kwargs = &#123;<span class="string">&quot;device&quot;</span>: <span class="string">&quot;cuda&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># try to access the sentence transformers from HuggingFace: https://huggingface.co/api/models/sentence-transformers/all-mpnet-base-v2</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> ex:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Exception: &quot;</span>, ex)</span><br><span class="line">        <span class="comment"># # alternatively, we will access the embeddings models locally</span></span><br><span class="line">        <span class="comment"># local_model_path = &quot;/kaggle/input/sentence-transformers/minilm-l6-v2/all-MiniLM-L6-v2&quot;</span></span><br><span class="line">        <span class="comment"># print(f&quot;Use alternative (local) model: &#123;local_model_path&#125;\n&quot;)</span></span><br><span class="line">        <span class="comment"># embeddings = HuggingFaceEmbeddings(model_name=local_model_path, model_kwargs=model_kwargs)</span></span><br><span class="line">    <span class="keyword">return</span> embeddings</span><br></pre></td></tr></table></figure>

<h2 id="集成-Qdrant-用于嵌入式存储和检索"><a href="#集成-Qdrant-用于嵌入式存储和检索" class="headerlink" title="集成 Qdrant 用于嵌入式存储和检索"></a>集成 Qdrant 用于嵌入式存储和检索</h2><p>我们将使用 Qdrant 作为我们的向量数据库，因为它在处理向量相似性搜索、可扩展性和灵活的向量数据管理方面具有出色的能力。</p>
<p>此外，Qdrant 支持本地和云存储选项，以便您可以适应各种本地和云环境。</p>
<p>我们已经安装了 Qdrant，并且正在从 LangChain 的向量存储中导入它</p>
<p>Qdrant.from_documents 方法通过将文档及其对应的嵌入作为输入来简化流程。</p>
<p>然后将 vectordb 对象转换为一个具有 vectordb.as_retriever() 的检索器。该检索器被配置为根据向量相似性查询向量数据库，以便检索相关文档，这对于有效的信息检索至关重要。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_retriever</span>():</span><br><span class="line">    documents=document_loader_spilter()</span><br><span class="line">    embeddings=load_embeddings_model()</span><br><span class="line"></span><br><span class="line">    vectordb = Qdrant.from_documents(</span><br><span class="line">        documents,</span><br><span class="line">        embeddings,</span><br><span class="line">        path=<span class="string">&quot;Qdrant_Persist&quot;</span>,</span><br><span class="line">        collection_name=<span class="string">&quot;voice_assistant_documents&quot;</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    retriever = vectordb.as_retriever()</span><br><span class="line">    <span class="keyword">return</span> retriever</span><br></pre></td></tr></table></figure>

<p>使用的参数的详细说明：</p>
<ul>
<li>documents ：生成嵌入的原始文档。</li>
<li>embeddings ：从文档中提取的嵌入已准备好被索引和存储。</li>
<li>path ：指定的本地目录，Qdrant 数据库将在其中持久保存数据，确保嵌入数据安全存储并易于将来检索。</li>
<li>collection_name ：Qdrant 中数据集的标签，有助于组织和检索特定组的嵌入。</li>
</ul>
<p>如果想要在 Qdrant 向量数据库中重用持久化数据, 创建一个 QdrantClient 实例，指向存储我们数据库文件的路径，从而实现对持久化数据的访问。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> qdrant_client <span class="keyword">import</span> QdrantClient</span><br><span class="line"></span><br><span class="line">client = QdrantClient(path = <span class="string">&quot;Qdrant_Persist&quot;</span>)</span><br><span class="line"></span><br><span class="line">vectordb = Qdrant(</span><br><span class="line">    client=client,</span><br><span class="line">    collection_name=<span class="string">&quot;voice_assistant_documents&quot;</span>,</span><br><span class="line">    embeddings=embeddings,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h2 id="设置检索器"><a href="#设置检索器" class="headerlink" title="设置检索器"></a>设置检索器</h2><p>用我们 Qdrant 向量数据库中存储的嵌入来建立一个检索式问答（QA）系统</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model, tokenizer=load_llm_model()</span><br><span class="line">llm=load_query_pipeline(model, tokenizer)</span><br><span class="line">retriever=create_retriever()</span><br><span class="line">qa = RetrievalQA.from_chain_type(</span><br><span class="line">    llm=llm,</span><br><span class="line">    chain_type=<span class="string">&quot;stuff&quot;</span>,</span><br><span class="line">    retriever=retriever,</span><br><span class="line">    verbose=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>然后我们初始化一个 RetrievalQA 实例，它是我们 AI 链中的一部分。该实例使用检索器来获取与查询相关的信息。这里， llm 代表我们的语言模型， chain_type 设置为”stuff”，表示此链将处理的任务或操作类型， verbose&#x3D;True 在操作期间启用详细输出，提供有关检索过程的见解。</p>
<h2 id="测试和可视化-RAG-系统"><a href="#测试和可视化-RAG-系统" class="headerlink" title="测试和可视化 RAG 系统"></a>测试和可视化 RAG 系统</h2><p>我们实现了用于测试和可视化检索增强生成（RAG）系统的功能：</p>
<ul>
<li>colorize_text 功能: 为“Reasoning”、“Question”、“Answer”和“Total time”等关键术语添加颜色，以获得清晰且视觉上吸引人的输出。</li>
<li>test_rag 功能: 接受 QA 系统 ( qa ) 和查询字符串。它测量响应时间，检索答案，并在 Markdown 中显示格式化结果，突出关键元素以便阅读。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">colorize_text</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="keyword">for</span> word, color <span class="keyword">in</span> <span class="built_in">zip</span>([<span class="string">&quot;Reasoning&quot;</span>, <span class="string">&quot;Question&quot;</span>, <span class="string">&quot;Answer&quot;</span>, <span class="string">&quot;Total time&quot;</span>], [<span class="string">&quot;blue&quot;</span>, <span class="string">&quot;red&quot;</span>, <span class="string">&quot;green&quot;</span>, <span class="string">&quot;magenta&quot;</span>]):</span><br><span class="line">        text = text.replace(<span class="string">f&quot;<span class="subst">&#123;word&#125;</span>:&quot;</span>, <span class="string">f&quot;\n\n**&lt;font color=&#x27;<span class="subst">&#123;color&#125;</span>&#x27;&gt;<span class="subst">&#123;word&#125;</span>:&lt;/font&gt;**&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_rag</span>(<span class="params">qa, query</span>):</span><br><span class="line">    console = Console()</span><br><span class="line"></span><br><span class="line">    time_start = time()</span><br><span class="line">    response = qa.run(query)</span><br><span class="line">    time_end = time()</span><br><span class="line">    total_time = <span class="string">f&quot;<span class="subst">&#123;<span class="built_in">round</span>(time_end-time_start, <span class="number">3</span>)&#125;</span> sec.&quot;</span></span><br><span class="line"></span><br><span class="line">    full_response =  <span class="string">f&quot;Question: <span class="subst">&#123;query&#125;</span>\nAnswer: <span class="subst">&#123;response&#125;</span>\nTotal time: <span class="subst">&#123;total_time&#125;</span>&quot;</span></span><br><span class="line">    colored_text = colorize_text(full_response)</span><br><span class="line">    console.<span class="built_in">print</span>(Markdown(colored_text))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure>

<h2 id="Llama-3-和-Whisper-的文本转语音处理"><a href="#Llama-3-和-Whisper-的文本转语音处理" class="headerlink" title="Llama 3 和 Whisper 的文本转语音处理"></a>Llama 3 和 Whisper 的文本转语音处理</h2><p><img src="/../asset_voicellama3/03.png"></p>
<ul>
<li>知识库到向量数据库：最初，知识库中的文档通过嵌入模型进行处理。该模型将文本数据转换为数字向量，然后存储在像 Qdrant 这样的向量数据库中。这种设置通过将文档的语义含义表示为高维空间中的点，实现了高效的检索。</li>
<li>用户查询处理：当用户提交查询时，它首先与嵌入模型交互，将查询转换为其向量表示。</li>
<li>检索：然后使用查询向量从向量数据库中获取与之最相似的前 K 个向量（上下文）。这个过程被称为“检索”，有助于识别与用户查询相关的知识库中最相关的文档或数据片段。</li>
<li>阅读和响应生成：然后将检索到的上下文输入到 Meta Llama 3 LLM中，该系统会阅读和理解这些上下文中与用户查询相关的信息。然后生成一个响应，旨在提供最准确和相关的信息。然后 Whisper 将文本转换为音频响应。</li>
</ul>
<p>首先定义“Whisper”管道。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pipe = Pipeline(s2a_ref=<span class="string">&#x27;WhisperSpeech/WhisperSpeech:s2a-q4-tiny-en+pl.model&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>为windows系统，我们定义一个函数，用于将音频张量转换为音频文件。</p>
<p>然后通过我们的查询来使用 Llama 3 进行文本生成，接着我们可以使用 Whisper 进行音频生成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_audio_for_windows</span>(<span class="params">audio_tensor,wav_file_path</span>):</span><br><span class="line">    <span class="keyword">from</span> pydub <span class="keyword">import</span> AudioSegment</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">    <span class="comment"># generate uses CUDA if available; therefore, it&#x27;s necessary to move to CPU before converting to NumPy array</span></span><br><span class="line">    audio_np = (audio_tensor.cpu().numpy() * <span class="number">32767</span>).astype(np.int16)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(audio_np.shape) == <span class="number">1</span>:</span><br><span class="line">        audio_np = np.expand_dims(audio_np, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        audio_np = audio_np.T</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Array shape:&quot;</span>, audio_np.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Array dtype:&quot;</span>, audio_np.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        audio_segment = AudioSegment(</span><br><span class="line">            audio_np.tobytes(), </span><br><span class="line">            frame_rate=<span class="number">24000</span>, </span><br><span class="line">            sample_width=<span class="number">2</span>, </span><br><span class="line">            channels=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        audio_segment.export(wav_file_path, <span class="built_in">format</span>=<span class="string">&#x27;wav&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Audio file generated: <span class="subst">&#123;wav_file_path&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Error writing audio file: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">query = <span class="string">&quot;How LLMs can be used to understand and interact with the complex 3D world&quot;</span></span><br><span class="line">aud = test_rag(qa, query)</span><br><span class="line"></span><br><span class="line">audio_tensor = pipe.generate(<span class="string">f&quot;<span class="subst">&#123;aud&#125;</span>&quot;</span>)</span><br><span class="line">generate_audio_for_windows(audio_tensor, <span class="string">f&quot;output.wav&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>查询处理：我们从查询“如何LLMs用于理解和与复杂的 3D 世界互动”开始，通过使用模型（ qa ）的检索增强生成（RAG）系统进行处理。该系统的响应已准备用于语音合成。</li>
<li>语音合成：使用 whisper 模型与语音，我们将文本响应转换为音频并保存为 output.wav 。</li>
<li>语音转文本：可以使用 whisper 模型将音频文件转录回文本，以验证语音合成的准确性。</li>
</ul>
<p><img src="/../asset_voicellama3/04.png"></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://medium.com/@datadrifters/llama-3-powered-voice-assistant-integrating-local-rag-withdrant-whisper-and-langchain-b4d075b00ac5">https://medium.com/@datadrifters/llama-3-powered-voice-assistant-integrating-local-rag-withdrant-whisper-and-langchain-b4d075b00ac5</a></p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/05/19/rpa-agent/" rel="prev" title="从 RPA 到企业 AI 代理">
                  <i class="fa fa-angle-left"></i> 从 RPA 到企业 AI 代理
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/05/27/deepQ/" rel="next" title="强化学习：深度 Q 网络">
                  强化学习：深度 Q 网络 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Howard Huang</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">494k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">14:59</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">
    <!--由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动-->
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/fancyapps-ui/5.0.33/fancybox/fancybox.umd.js" integrity="sha256-+2+qOqR8CKoHh/AsVR9k2qaDBKWjYNC2nozhYmv5j9k=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  



  <script src="/js/third-party/fancybox.js"></script>



  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
