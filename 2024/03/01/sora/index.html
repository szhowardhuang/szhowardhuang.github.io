<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>OpenAI Sora视频生成模型技术报告 | 嵌入式老兵博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="报告总结不管是在视频的保真度、长度、稳定性、一致性、分辨率、文字理解等方面，Sora都做到了SOTA（当前最优）。 技术细节写得比较泛（防止别人模仿）大概就是用视觉块编码（visual patch）的方式，把不同格式的视频统一编码成了用transformer架构能够训练的embeding，然后引入类似diffusion的unet的方式做在降维和升维的过程中做加噪和去噪，然后把模型做得足够大，大到能">
<meta property="og:type" content="article">
<meta property="og:title" content="OpenAI Sora视频生成模型技术报告">
<meta property="og:url" content="https://szhowardhuang.github.io/2024/03/01/sora/index.html">
<meta property="og:site_name" content="嵌入式老兵博客">
<meta property="og:description" content="报告总结不管是在视频的保真度、长度、稳定性、一致性、分辨率、文字理解等方面，Sora都做到了SOTA（当前最优）。 技术细节写得比较泛（防止别人模仿）大概就是用视觉块编码（visual patch）的方式，把不同格式的视频统一编码成了用transformer架构能够训练的embeding，然后引入类似diffusion的unet的方式做在降维和升维的过程中做加噪和去噪，然后把模型做得足够大，大到能">
<meta property="og:locale">
<meta property="og:image" content="https://szhowardhuang.github.io/asset_sora/01.png">
<meta property="og:image" content="https://szhowardhuang.github.io/asset_sora/02.png">
<meta property="og:image" content="https://szhowardhuang.github.io/asset_sora/03.png">
<meta property="og:image" content="https://szhowardhuang.github.io/asset_sora/04.png">
<meta property="og:image" content="https://szhowardhuang.github.io/asset_sora/05.png">
<meta property="og:image" content="https://szhowardhuang.github.io/asset_sora/06.png">
<meta property="og:image" content="https://szhowardhuang.github.io/asset_sora/07.png">
<meta property="og:image" content="https://szhowardhuang.github.io/asset_sora/08.png">
<meta property="og:image" content="https://szhowardhuang.github.io/asset_sora/09.png">
<meta property="article:published_time" content="2024-03-01T15:00:16.000Z">
<meta property="article:modified_time" content="2024-03-02T07:31:13.000Z">
<meta property="article:author" content="Howard Huang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://szhowardhuang.github.io/asset_sora/01.png">
  
    <link rel="alternate" href="/atom.xml" title="嵌入式老兵博客" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<meta name="generator" content="Hexo 7.1.1"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">嵌入式老兵博客</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Suche"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Suche"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://szhowardhuang.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-sora" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2024/03/01/sora/" class="article-date">
  <time class="dt-published" datetime="2024-03-01T15:00:16.000Z" itemprop="datePublished">2024-03-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      OpenAI Sora视频生成模型技术报告
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h4 id="报告总结"><a href="#报告总结" class="headerlink" title="报告总结"></a>报告总结</h4><p>不管是在视频的保真度、长度、稳定性、一致性、分辨率、文字理解等方面，Sora都做到了SOTA（当前最优）。</p>
<p>技术细节写得比较泛（防止别人模仿）大概就是用视觉块编码（visual patch）的方式，把不同格式的视频统一编码成了用transformer架构能够训练的embeding，然后引入类似diffusion的unet的方式做在降维和升维的过程中做加噪和去噪，然后把模型做得足够大，大到能够出现涌现能力。</p>
<p>简单来说，在别家做视频模型的时候还是基于“小”模型的思路（基于上一帧预测下一帧，并且用文字或者笔刷遮罩做约束）的时候，OpenAI则是用做“大”模型的思路做视频生成——准备足够大量的视频，用多模态模型给视频做标注，把不同格式的视频编码成统一的视觉块嵌入，然后用足够大的网络架构+足够大的训练批次（batch size）+ 足够强的算力，让模型对足够多的训练集做全局拟合（理解），在模型更好地还原细节的同时让模型出现智能涌现能力——例如在一定程度上理解真实世界的物理影响和因果关系。</p>
<p>最让人期待（不安）的是，这个视频生成模型仿佛只是OpenAI世界模型（理解和模拟真实世界的各种复杂因果关系的通用模型）路上点亮的一个成就，而不是终点。</p>
<h4 id="Sora发布的潜在影响"><a href="#Sora发布的潜在影响" class="headerlink" title="Sora发布的潜在影响"></a>Sora发布的潜在影响</h4><p>C端 &#x2F; 对于普通人</p>
<p>这或许是独立创作者最好的年代，Sora发布之后，文案、音效、视频AI生成的可用工具都已齐备，一个人可以无痛carry一个短片，好故事将价值千金，有才华的人更难被埋没。但是从另一个角度将，创作门槛降低之后故事的竞争将异常激烈。</p>
<p>以vision pro为代表的XR产业将再次获得助力——内容匮乏将不再是问题。</p>
<p>目前当红的短视频推荐的形态可能会发生改变——从系统根据用户喜好推荐短视频，变成针对性生成短视频？或者说，同一个短视频在不同的用户对可以有不同的（实时）微调版本？</p>
<p>B端 &#x2F; 对于商业公司</p>
<p>所有做AI视频生成的公司将面临第一波危机，但是危中有机。因为OpenAI证明了用大模型的思路做视频是可行的，那么他们需要做的只是证明我也可以用大模型做视频。参考chatGPT火了之后做大语言模型的公司反而更多了而不是更少。</p>
<p>AI三维生成的公司将面临第二波冲击，由于多目重建技术的存在，视频生成和3D生成的界限是模糊的。所以3D生成可能要重新考虑当前技术路线的合理性和商业叙事逻辑。</p>
<p>虽然OpenAI没有明说，但是Sora需要的算力不会小，所以显卡公司会迎来新的一波利好，但是不一定利好英伟达。因为现在算力越来越呈现基础设施的特征，而基础设施是各个国家的命脉，即便不考虑禁运，我国不会是唯一一个要求算力自主可控的国家，甚至每个大厂都开始想自己搞显卡或者AI专用算力卡（参考google、特斯拉、openAI、阿里），所以算力领域的竞争者会越来越多。</p>
<h3 id="报告具体内容"><a href="#报告具体内容" class="headerlink" title="报告具体内容"></a>报告具体内容</h3><p>技术报告地址：<a target="_blank" rel="noopener" href="https://openai.com/research/video-generation-models-as-world-simulators">https://openai.com/research/video-generation-models-as-world-simulators</a></p>
<p>OpenAI 探索了视频数据生成模型的大规模训练。具体来说，研究人员在可变持续时间、分辨率和宽高比的视频和图像上联合训练了一个文本条件扩散模型。作者利用对视频和图像潜在代码的时空补丁进行操作的 transformer 架构，其最大的模型 Sora 能够生成长达一分钟的高质量视频。<br>OpenAI 认为，新展示的结果表明，扩展视频生成模型是构建物理世界通用模拟器的一条有前途的途径。</p>
<p>OpenAI 在技术报告中重点展示了：（1）将所有类型的视觉数据转化为统一表示，从而能够大规模训练生成模型的方法；以及（2）对 Sora 的能力和局限性进行定性评估。</p>
<p>令人遗憾的是，OpenAI 的报告不包含模型和训练的细节。<br>最近一段时间，视频生成是 AI 领域的重要方向，先前的许多工作研究了视频数据的生成建模方向，包括循环网络、生成对抗网络、自回归 transformer 和扩散模型。这些工作通常关注一小类视觉数据、较短的视频或固定大小的视频。</p>
<p>与之不同的是，OpenAI 的 Sora 是视觉数据的通用模型，它可以生成不同时长、长宽比和分辨率的视频和图像，而且最多可以输出长达一分钟的高清视频。</p>
<h4 id="视觉数据转为-Patches"><a href="#视觉数据转为-Patches" class="headerlink" title="视觉数据转为 Patches"></a>视觉数据转为 Patches</h4><p>大型语言模型通过在互联网规模的数据上进行训练，获得了出色的通用能力中，OpenAI 从这一点汲取了灵感。LLM 得以确立新范式，部分得益于创新了 token 使用的方法。研究人员们巧妙地将文本的多种模态 —— 代码、数学和各种自然语言统一了起来。</p>
<p>在这项工作中，OpenAI 考虑了生成视觉数据的模型如何继承这种方法的好处。大型语言模型有文本 token，而 Sora 有视觉 patches。此前的研究已经证明 patches 是视觉数据模型的有效表示。OpenAI 发现 patches 是训练生成各种类型视频和图像的模型的可扩展且有效的表示。</p>
<p>在更高层面上，OpenAI 首先将视频压缩到较低维的潜在空间，然后将表示分解为时空 patches，从而将视频转换为 patches。<br><img src="/../asset_sora/01.png"></p>
<h4 id="视频压缩网络"><a href="#视频压缩网络" class="headerlink" title="视频压缩网络"></a>视频压缩网络</h4><p>OpenAI 训练了一个降低视觉数据维度的网络。该网络将原始视频作为输入，并输出在时间和空间上压缩的潜在表示。Sora 在这个压缩的潜在空间中接受训练，而后生成视频。OpenAI 还训练了相应的解码器模型，将生成的潜在表示映射回像素空间。</p>
<h4 id="时空潜在-patches"><a href="#时空潜在-patches" class="headerlink" title="时空潜在 patches"></a>时空潜在 patches</h4><p>给定一个压缩的输入视频，OpenAI 提取一系列时空 patches，充当 Transformer 的 tokens。该方案也适用于图像，因为图像可视为单帧视频。OpenAI 基于 patches 的表示使 Sora 能够对不同分辨率、持续时间和长宽比的视频和图像进行训练。在推理时，OpenAI 可以通过在适当大小的网格中排列随机初始化的 patches 来控制生成视频的大小。</p>
<h4 id="扩展Transformer用于视频生成"><a href="#扩展Transformer用于视频生成" class="headerlink" title="扩展Transformer用于视频生成"></a>扩展Transformer用于视频生成</h4><p>Sora是一个扩散模型；给定输入的噪声块（和像文本提示这样的条件信息），它被训练来预测原始的“干净”块。重要的是，Sora是一个扩散变换器。变换器在包括语言建模、计算机视觉和图像生成等多个领域展现了显著的扩展属性。<br><img src="/../asset_sora/02.png"></p>
<p>在这项工作中，我们发现扩散变换器作为视频模型也能有效地扩展。下面，我们展示了训练进展过程中，使用固定种子和输入的视频样本比较。随着训练计算量的增加，样本质量显著提高。<br><img src="/../asset_sora/03.png"></p>
<h4 id="可变持续时间、分辨率、宽高比"><a href="#可变持续时间、分辨率、宽高比" class="headerlink" title="可变持续时间、分辨率、宽高比"></a>可变持续时间、分辨率、宽高比</h4><p>过去在图像和视频生成中的方法通常会将视频调整大小、裁剪或剪辑到一个标准尺寸——例如，4秒长的视频，分辨率为256x256。我们发现，直接在数据的原始尺寸上进行训练可以带来几个好处。</p>
<h5 id="采样灵活性"><a href="#采样灵活性" class="headerlink" title="采样灵活性"></a>采样灵活性</h5><p>Sora可以采样宽屏1920x1080p视频、竖屏1080x1920视频以及介于两者之间的所有格式。这使得Sora能够直接按照不同设备的原生宽高比创建内容。它还允许我们在使用同一模型生成全分辨率内容之前，快速原型化较小尺寸的内容。</p>
<p><img src="/../asset_sora/04.png"></p>
<h5 id="改进的构图和画面组成"><a href="#改进的构图和画面组成" class="headerlink" title="改进的构图和画面组成"></a>改进的构图和画面组成</h5><p>我们通过实证发现，在视频的原始宽高比上进行训练可以改善构图和取景。我们将Sora与一个版本的模型进行了比较，该模型将所有训练视频裁剪成正方形，这是训练生成模型时的常见做法。在正方形裁剪上训练的模型（左侧）有时会生成主体只部分出现在视野中的视频。相比之下，来自Sora的视频（右侧）具有改善的取景。<br><img src="/../asset_sora/05.png"></p>
<h4 id="语言理解"><a href="#语言理解" class="headerlink" title="语言理解"></a>语言理解</h4><p>训练文本到视频生成系统需要大量带有相应文字标题的视频。我们将在DALL·E 3中引入的重新标注技术应用到视频上。我们首先训练一个高度描述性的标注模型，然后使用它为我们训练集中的所有视频生成文字标题。我们发现，在高度描述性的视频标题上进行训练可以提高文本的准确性以及视频的整体质量。</p>
<p>类似于DALL·E 3，我们也利用GPT将用户的简短提示转换成更长的详细说明，然后发送给视频模型。这使得Sora能够生成高质量的视频，准确地遵循用户的提示。</p>
<p><img src="/../asset_sora/06.png"></p>
<h4 id="使用图片和视频进行提示"><a href="#使用图片和视频进行提示" class="headerlink" title="使用图片和视频进行提示"></a>使用图片和视频进行提示</h4><p>上述结果以及我们的登录页面展示了文本到视频的样本。但是Sora也可以通过其他输入进行提示，例如预先存在的图片或视频。这项能力使得Sora能够执行广泛的图像和视频编辑任务——创建完美循环的视频，为静态图像添加动画，向前或向后延长视频的时间等。</p>
<h4 id="视频到视频编辑"><a href="#视频到视频编辑" class="headerlink" title="视频到视频编辑"></a>视频到视频编辑</h4><p>扩散模型使得从文本提示编辑图像和视频的方法层出不穷。下面我们将其中一种方法，SDEdit，应用于Sora。这项技术使得Sora能够零次学习地转换输入视频的风格和环境。<br><img src="/../asset_sora/07.png"></p>
<h4 id="连接视频"><a href="#连接视频" class="headerlink" title="连接视频"></a>连接视频</h4><p>我们还可以使用Sora在两个输入视频之间逐渐插值，创建在完全不同主题和场景构成的视频之间的无缝过渡。在下面的例子中，中间的视频在左右两边对应视频之间进行插值。<br><img src="/../asset_sora/08.png"></p>
<h4 id="图像生成能力"><a href="#图像生成能力" class="headerlink" title="图像生成能力"></a>图像生成能力</h4><p>Sora也能够生成图像。我们通过在具有一个帧时间范围的空间网格中排列高斯噪声块来实现这一点。该模型可以生成不同大小的图像——分辨率最高可达2048x2048。<br><img src="/../asset_sora/09.png"><br>A snowy mountain village with cozy cabins and a northern lights display, high detail and photorealistic dslr, 50mm f&#x2F;1.2</p>
<p>一个雪山村庄，有着舒适的小木屋和北极光展示，高清晰度和逼真的数码单反相机，50mm f&#x2F;1.2镜头拍摄。</p>
<h4 id="涌现的模拟能力"><a href="#涌现的模拟能力" class="headerlink" title="涌现的模拟能力"></a>涌现的模拟能力</h4><p>我们发现，当在大规模上训练时，视频模型展现出许多有趣的新兴能力。这些能力使得Sora能够模拟现实世界中人类、动物和环境的某些方面。这些属性并没有任何针对3D、物体等的明确归纳偏见——它们纯粹是规模效应的现象。</p>
<p>3D一致性。Sora能够生成具有动态相机运动的视频。随着相机的移动和旋转，人物和场景元素在三维空间中保持一致地移动。</p>
<p>长距离一致性和物体恒存性。对于视频生成系统来说，一个重大挑战是在采样长视频时保持时间上的连贯性。我们发现，尽管不总是如此，Sora通常能够有效地建模短距离和长距离依赖关系。例如，我们的模型即使在人、动物和物体被遮挡或离开画面时，也能持续保持它们的存在。同样，它能在单个样本中生成同一角色的多个镜头，并在整个视频中保持其外观。</p>
<p>与世界互动。Sora有时可以模拟一些简单的动作来影响世界的状态。例如，画家可以在画布上留下随时间持续存在的新笔触，或者一个人可以吃一个汉堡并留下咬痕。</p>
<p>模拟数字世界。Sora也能够模拟人工过程，一个例子是视频游戏。Sora可以在同时控制《我的世界》中的玩家采用基本策略的同时，还能以高保真度渲染世界及其动态。通过用提到“我的世界”的字幕提示Sora，可以零次尝试地引发这些能力。</p>
<p>Sora作为一个模拟器目前展现出许多限制。例如，它并没有准确地模拟许多基本互动的物理效应，比如玻璃破碎。其他互动，比如吃食物，不总是产生正确的物体状态变化。我们在我们的登录页面列举了模型的其他常见故障模式，比如在长时间样本中发展的不连贯性或物体的自发出现。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://szhowardhuang.github.io/2024/03/01/sora/" data-id="clto3ikkb000efgmj4kwx4wqs" data-title="OpenAI Sora视频生成模型技术报告" class="article-share-link"><span class="fa fa-share">Teilen</span></a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2024/03/02/self-RAG/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Neuer</strong>
      <div class="article-nav-title">
        
          self-RAG如何革新工业LLMs
        
      </div>
    </a>
  
  
    <a href="/2024/02/27/digital-human-2/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Älter</strong>
      <div class="article-nav-title">如何设计一个数字人续篇 --- Video Retalking 模型篇</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">March 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">February 2024</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/01/">January 2024</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2024/03/12/ollama/">ollama使用方法</a>
          </li>
        
          <li>
            <a href="/2024/03/11/make-million-llm/">使用Python从头开始构建一个百万参数的LLM</a>
          </li>
        
          <li>
            <a href="/2024/03/08/springboot-music/">用VSCode实践一个Spring Boot项目</a>
          </li>
        
          <li>
            <a href="/2024/03/07/mysql5_7_21-cmd-install/">命令行安装MySQL 5.7.21</a>
          </li>
        
          <li>
            <a href="/2024/03/03/generativeDiffusion/">生成扩散模型综述</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2024 Howard Huang<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>