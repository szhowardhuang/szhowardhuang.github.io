<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"szhowardhuang.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js"></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="嵌入式老兵博客">
<meta property="og:url" content="https://szhowardhuang.github.io/index.html">
<meta property="og:site_name" content="嵌入式老兵博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Howard Huang">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://szhowardhuang.github.io/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>嵌入式老兵博客</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">嵌入式老兵博客</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Howard Huang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">34</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/05/27/deepQ/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/05/27/deepQ/" class="post-title-link" itemprop="url">强化学习：深度 Q 网络</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-05-27 17:29:10" itemprop="dateCreated datePublished" datetime="2024-05-27T17:29:10+08:00">2024-05-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-05-28 15:35:12" itemprop="dateModified" datetime="2024-05-28T15:35:12+08:00">2024-05-28</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>18k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>33 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>在强化学习（RL）中，Q 学习是一种基础算法，帮助代理通过学习最大化累积奖励的策略来导航其环境。它通过更新动作值函数来实现这一点，该函数基于接收的奖励和未来的奖励来估计在给定状态下采取特定动作的预期效用。</p>
<p>然而，传统的 Q 学习也存在挑战。随着状态空间的增长，它在可扩展性方面遇到困难，并且在具有连续状态和动作空间的环境中效果较差。这就是深度 Q 网络（DQNs）发挥作用的地方。DQNs 使用神经网络来近似 Q 值，使代理能够有效处理更大更复杂的环境。</p>
<p>在本文中，我们将深入探讨深度 Q 网络。我们将探讨 DQN 如何克服传统 Q 学习的局限性，并讨论构成 DQN 的关键组件。我们还将介绍如何从头开始实现一个 DQN，并将其应用于更复杂的环境中。</p>
<h2 id="传统的-Q-学习"><a href="#传统的-Q-学习" class="headerlink" title="传统的 Q 学习"></a>传统的 Q 学习</h2><p>Q-learning 指导代理来学习在环境中最大化累积奖励的最佳行动。在深度 Q 网络之前，先简要回顾其前身 Q-learning 背后的机制。</p>
<h3 id="状态和动作"><a href="#状态和动作" class="headerlink" title="状态和动作"></a>状态和动作</h3><p>想象一下，你是一个在迷宫中导航的机器人。迷宫中你所占据的每个位置被称为“状态”。你可以采取的每个可能移动，比如向左、向右、向上或向下，都是一种“动作”。目标是找出在每个状态下应该采取哪种动作，最终找到迷宫中的最佳路径。</p>
<h3 id="Q-值"><a href="#Q-值" class="headerlink" title="Q-值"></a>Q-值</h3><p>Q 学习的核心是 Q 值，表示为𝑄(𝑠, 𝑎)。该值代表在特定状态 s 中采取特定动作 a 后预期的未来奖励，然后沿着最佳路径（策略）继续。</p>
<p>将 Q 值视为指南中的条目，评估每个可能移动的长期收益。例如，如果你在迷宫中的特定位置并考虑向左移动，Q 值会告诉你这一移动在未来奖励方面预计会有多大益处。较高的 Q 值表示更好的移动。</p>
<h3 id="Q-表"><a href="#Q-表" class="headerlink" title="Q 表"></a>Q 表</h3><p>Q 学习使用 Q 表来跟踪这些 Q 值。Q 表本质上是一个大型的电子表格，其中每一行对应一个状态，每一列对应一个动作，每个单元格包含该状态-动作对的 Q 值。</p>
<p>想象 Q 表格就像一个巨大的电子表格，其中每个单元格代表从迷宫中特定位置做出特定移动的潜在未来奖励。随着对环境的了解越来越多，您会用更好的估计值更新这个电子表格中的这些奖励。</p>
<h3 id="学习过程"><a href="#学习过程" class="headerlink" title="学习过程"></a>学习过程</h3><p>Q-Learning 中的学习过程是迭代的。它始于一个初始状态 s。然后，决定采取一个动作 a。这个选择可以基于：</p>
<ul>
<li>探索：尝试新的行动以发现它们的效果。</li>
<li>利用：利用现有知识选择具有已知最高 Q 值的动作。</li>
</ul>
<p>执行选择的动作，观察奖励 r，并移动到下一个状态 s′。使用 Q-Learning 公式更新状态-动作对 (s, a) 的 Q 值：</p>
<p><img src="/../asset_deepq/01.png" alt="Q-值更新公式"></p>
<p>这里：</p>
<ul>
<li>α是学习率，它决定了新信息覆盖旧信息的程度。</li>
<li>γ是折扣因子，它更高地重视即时奖励而不是遥远未来的奖励。</li>
<li>maxa′​Q(s′,a′)代表了在所有可能的动作 a′中，下一个状态 s′的最高 Q 值。</li>
</ul>
<p>想象一下，你不断更新你的指南。每次移动后，你会得到关于这个移动是好是坏的反馈（奖励）。然后，你会调整指南中的评分（Q 值）以反映这些新信息，使你未来的决策更加明智。</p>
<p>继续重复这个过程，直到 Q 值收敛，这意味着代理已经学会了在迷宫中导航的最优策略。随着时间的推移，通过反复探索迷宫并根据经验更新你的指南，你会制定出一套全面的策略，告诉你在任何给定位置做出最佳移动以最大化奖励。</p>
<h2 id="从-Q-学习到深度-Q-网络"><a href="#从-Q-学习到深度-Q-网络" class="headerlink" title="从 Q 学习到深度 Q 网络"></a>从 Q 学习到深度 Q 网络</h2><h3 id="传统-Q-学习的局限性"><a href="#传统-Q-学习的局限性" class="headerlink" title="传统 Q 学习的局限性"></a>传统 Q 学习的局限性</h3><p>虽然 Q 学习是一种强大的强化学习算法，但它有一些限制，阻碍了它在更复杂环境中的有效性：</p>
<ul>
<li><p>可扩展性问题：传统的 Q 学习维护一个 Q 表，其中每个状态-动作对都映射到一个 Q 值。随着状态空间的增长，特别是在高维度或连续环境中，Q 表变得过大，导致内存效率低下和学习过程缓慢。</p>
</li>
<li><p>离散状态和动作空间：Q 学习在状态和动作是离散和有限的环境中表现良好。然而，许多现实世界的问题涉及连续的状态和动作空间，传统的 Q 学习在不对这些空间进行离散化的情况下效率低下，这可能导致信息丢失和次优策略。</p>
</li>
</ul>
<h3 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h3><p>让我们现在介绍神经网络，它在深度网络中扮演着关键角色。受到人类大脑结构和功能的启发，神经网络是强大的函数逼近器，能够从数据中学习复杂模式。它们由相互连接的节点（神经元）层组成，处理输入数据，并通过权重和偏差将其转换为输出。</p>
<p>在强化学习的背景下，神经网络可以用来逼近 Q 函数，它将状态-动作对映射到 Q 值。这使得代理能够更好地在各种状态和动作之间进行泛化，特别是在大型或连续空间中，维护 Q 表是不可行的。</p>
<p>因此，深度 Q 网络（DQNs）将 Q 学习的原理与神经网络的函数逼近能力相结合。通过这样做，它们解决了传统 Q 学习的关键局限性。</p>
<p>DQNs 不是将 Q 值存储在表中，而是使用神经网络来近似 Q 函数。该网络以状态作为输入，并为所有可能的动作输出 Q 值。通过使用来自环境的经验对网络进行训练，代理程序学会预测每个动作的预期奖励，有效地在大量状态和动作之间进行泛化。</p>
<p>想象一下，你正在学习下棋。与其记住每种可能的棋盘布局和每一步最佳走法（这是不可能的），你学习一般性的策略和原则（比如控制棋盘中心和保护国王）。同样，一个 DQN 通过神经网络学习一般模式和策略，使其能够做出明智的决策，而无需记住每种可能的状态。</p>
<p>神经网络的使用使 DQN 能够处理具有大型或连续状态空间的环境。网络可以学习状态空间的表示，捕捉关键特征，使代理能够做出明智的决策，而无需离散化空间。</p>
<p>考虑尝试在一个大城市中导航。与其记住每条街道和建筑物的布局（这就像一个巨大的 Q 表），不如学会识别地标和主要道路，这有助于您找到方向。DQN 中的神经网络工作方式类似，学会识别状态空间的重要特征，帮助代理在复杂环境中导航。</p>
<p>通过对各种经历进行训练，模型学会从过去的经历中归纳。这意味着代理可以将所学应用于新的、未曾见过的状态和行动，使其在不同情况下更具适应性和效率。</p>
<h2 id="深度-Q-网络的解剖"><a href="#深度-Q-网络的解剖" class="headerlink" title="深度 Q 网络的解剖"></a>深度 Q 网络的解剖</h2><h3 id="DQN-的组成部分"><a href="#DQN-的组成部分" class="headerlink" title="DQN 的组成部分"></a>DQN 的组成部分</h3><p>要理解深度 Q 网络（DQNs）的工作原理，必须分解它们的关键组成部分：</p>
<h4 id="神经网络-1"><a href="#神经网络-1" class="headerlink" title="神经网络"></a>神经网络</h4><p><img src="/../asset_deepq/02.png" alt="前馈神经网络 "></p>
<p>DQN 的核心是一个神经网络，它作为 Q 值的函数逼近器。典型的架构通常包括：</p>
<ul>
<li>输入层：将其想象为代理的“眼睛”。它接收来自环境的状态表示，类似于您的眼睛接收周围的视觉信息。这是上图左侧的第一层，有两个节点。</li>
<li>隐藏层：将这些层视为代理的“大脑”。它们通过多个思考阶段处理眼睛接收到的信息，识别复杂的特征和模式，就像你的大脑处理和理解世界一样。这是上图中具有三个节点的中间层。</li>
<li>输出层：这就像代理人的“决策”部分。它根据输入状态产生所有可能动作的 Q 值，类似于你的大脑根据所见和所想决定最佳动作。每个输出对应于采取特定动作的预期奖励。这是上图中最右边的最后一层，有一个节点。</li>
</ul>
<p>上面的图像代表了一个简单的前馈神经网络，是神经网络最基本的形式。虽然这种结构是基础的，但它还不是一个“深度”神经网络。要将其转变为深度神经网络，我们可以添加更多隐藏层，增加网络的深度。此外，我们可以尝试不同的架构和配置来开发更高级的模型。还要注意的是，每个层中的节点数量并不固定；它取决于特定的训练数据集和任务。这种灵活性使我们能够根据特定需求来定制网络。</p>
<h4 id="经历回放"><a href="#经历回放" class="headerlink" title="经历回放"></a>经历回放</h4><p>经历重放, 这是一种用于稳定和改进 DQN 学习过程的技术。它涉及：</p>
<ul>
<li>内存缓冲区：将其想象为代理的“日记”。它会随着时间而存储代理的经历（状态、动作、奖励、下一个状态、完成），就像你可能会每天写下发生在你身上的事情一样。</li>
<li>随机抽样：在训练过程中，代理人翻阅其日记的随机页面，以从过去的经历中学习。这打破了事件序列，帮助代理人通过防止过度拟合到经历顺序来更加稳健地学习。</li>
</ul>
<h4 id="目标网络"><a href="#目标网络" class="headerlink" title="目标网络"></a>目标网络</h4><p>最后，目标网络是一个单独的神经网络，用于计算目标 Q 值进行训练。它在架构上与主网络相同，但具有冻结的权重，这些权重会定期更新以匹配主网络的权重。可以将其视为代理的“稳定指南”。虽然主网络不断学习和更新，但目标网络为训练提供稳定的 Q 值。这就像有一个可靠的、定期更新的手册可供参考，有助于保持学习的稳定性和一致性。</p>
<h3 id="DQN-算法"><a href="#DQN-算法" class="headerlink" title="DQN 算法"></a>DQN 算法</h3><p>有了这些组件，DQN 算法可以在几个关键步骤中概述：</p>
<h4 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h4><p>首先，我们从前向传播开始，这对于预测 Q 值至关重要。这些 Q 值存储了在给定状态下采取某些行动的预期未来奖励。该过程始于状态输入。</p>
<ul>
<li><p>状态输入：<br>代理人从环境中观察当前状态 s。这个状态被表示为描述代理人当前情况的特征向量。将状态视为代理人周围世界的快照，类似于您环顾四周时眼睛捕捉视觉场景。这个快照包含了代理人做出决策所需的所有必要细节。</p>
</li>
<li><p>Q 值预测：<br>接下来，观察到的状态 s 被输入神经网络。神经网络通过多层处理这个输入，并输出一组 Q 值 Q(s, a; θ)。每个 Q 值对应一个可能的动作 a，参数 θ 表示网络的权重和偏差。</p>
</li>
</ul>
<p><img src="/../asset_deepq/03.png" alt="Q 值预测公式"></p>
<p>想象神经网络是代理的大脑中的一个复杂决策机器。当它接收到快照（状态）时，它通过几个阶段（层）处理这些信息，以找出不同行动的潜在结果（Q 值）。就像你的大脑根据所见的情况思考不同可能的行动一样。</p>
<ul>
<li>行动选择：<br>代理然后选择具有最高 Q 值的动作 a∗作为其下一步行动，遵循贪婪动作选择策略：</li>
</ul>
<p><img src="/../asset_deepq/04.png" alt="行动选择公式"></p>
<p>这就好比在深思熟虑所有选项后决定最佳行动。代理人选择它认为会带来最高奖励的行动，就像你根据所见和理解选择似乎最有前途的道路一样。</p>
<h4 id="经历重播"><a href="#经历重播" class="headerlink" title="经历重播"></a>经历重播</h4><p>接下来，我们继续进行经历重演，这有助于稳定和改善学习过程。</p>
<ul>
<li><p>记录经历：<br>代理程序执行动作 a 并获得奖励 r 和新状态 s′ 后，将这一经历存储为一个元组(s, a, r, s′, done) 放入重放缓冲区。变量 done 表示该情节是否已结束。可以将重放缓冲区视为代理程序记录经历的日记，就像记录一天中值得注意的事件一样。</p>
</li>
<li><p>小批量取样:<br>在训练期间，一小批经历从重播缓冲区中随机抽取。这一批数据用于通过计算目标 Q 值和最小化损失来更新网络。当代理进行训练时，它会翻阅其日记的随机页面，以从过去的经历中学习。这种随机抽样有助于打破事件序列，并提供多样化的学习示例，就像查看日记中不同日期以获得更广泛的视角一样。</p>
</li>
</ul>
<p><img src="/../asset_deepq/05.png" alt="深度 Q 网络中的小批量学习"></p>
<h4 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h4><p>最后一步涉及反向传播，它更新网络以改善其预测。</p>
<ul>
<li>计算目标 Q 值：<br>对于小批量中的每个经历，代理计算目标 Q 值 y_。如果下一个状态 s’是终止状态（done即为真），目标 Q 值就是简单的奖励 r。否则，它是奖励加上由目标网络 Qtarget 预测的下一个状态 s’ 的最大 Q 值的折扣：</li>
</ul>
<p><img src="/../asset_deepq/06.png" alt=" "></p>
<p>在这里，γ是折扣因子（0 ≤ γ &lt; 1）。这一步就像是基于过去的经历提前规划。如果经历结束了一段旅程（一个回合），目标就是所获得的奖励。如果继续下去，目标就包括了预期的未来奖励，类似于你考虑即时和未来利益来规划行动的方式。</p>
<ul>
<li>损失计算:<br>接下来，损失被计算为主网络预测的 Q 值 Q(s_i, a_i; θ)与目标 Q 值 y_i 之间的均方误差：</li>
</ul>
<p><img src="/../asset_deepq/07.png" alt="损失公式"></p>
<p>计算损失就像评估你的预测与实际发生情况之间的差距。就像检查你的猜测与实际结果相比有多准确，并注意其中的差异。</p>
<ul>
<li>反向传播和优化:<br>最后，执行反向传播以最小化这种损失。计算得到的损失通过网络进行反向传播，使用优化算法（如随机梯度下降（SGD）或 Adam）来更新权重。这个过程调整网络参数θ 以最小化损失：</li>
</ul>
<p><img src="/../asset_deepq/08.png" alt="反向传播公式 "></p>
<p>这里，α是学习率，∇θ​Loss 代表损失对网络参数的梯度。反向传播就像从错误中学习一样。当你意识到你的预测有多么偏离（损失）时，你会调整你的策略（网络权重）来改善你未来的决策。这就像根据反馈来微调你的方法，以便下次获得更好的结果。</p>
<p>使用这种架构，代理程序会迭代地改进其策略。它学会采取行动，以最大化随时间累积的奖励。神经网络、经历重放和目标网络的结合使得 DQN 能够在复杂的高维环境中有效学习。这个过程会持续下去，直到代理程序能够熟练地在其环境中导航。</p>
<h2 id="从头开始实现深度-Q-网络"><a href="#从头开始实现深度-Q-网络" class="headerlink" title="从头开始实现深度 Q 网络"></a>从头开始实现深度 Q 网络</h2><p>我们从0开始实现一个深度 Q 网络（DQN）。这样可以清楚地了解如何在 Python 中构建和训练一个 DQN。</p>
<p>我们将使用 OpenAI Gym 的LunarLander 环境。在这个环境中，目标是控制月球着陆器并成功降落在指定的着陆垫上。着陆器必须通过环境，使用推进器来控制其运动和方向。</p>
<h3 id="设置环境"><a href="#设置环境" class="headerlink" title="设置环境"></a>设置环境</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">gymnasium             0.29.1</span><br><span class="line">matplotlib            3.8.4</span><br><span class="line">numpy                 1.26.4</span><br><span class="line">optuna                3.6.1</span><br><span class="line">torch                 2.1.0+cu121</span><br><span class="line">torchaudio            2.1.0+cu121</span><br><span class="line">torchvision           0.16.0+cu121</span><br><span class="line">swig                  4.2.1</span><br><span class="line">moviepy               1.0.3</span><br></pre></td></tr></table></figure>
<p>安装box2D：  pip install gymnasium[box2d]</p>
<p>在win10系统会安装失败，显示log如下。安装微软工具，然后继续安装box2d 。</p>
<pre><code>Failed to build box2d-py
ERROR: Could not build wheels for box2d-py, which is required to install pyproject.toml-based projects
</code></pre>
<p>登录该网站<a target="_blank" rel="noopener" href="https://visualstudio.microsoft.com/zh-hans/visual-cpp-build-tools/">build-tools</a> ，点击下载生成工具. 勾选使用C++的桌面开发，点击安装.</p>
<p><img src="/../asset_deepq/11.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> gymnasium <span class="keyword">as</span> gym</span><br><span class="line"><span class="keyword">from</span> gymnasium.wrappers.monitoring.video_recorder <span class="keyword">import</span> VideoRecorder</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> optuna</span><br></pre></td></tr></table></figure>
<p>在这里，我们导入必要的库。 gym用于环境，torch用于构建和训练我们的神经网络，collections和 random、optuna 用于经历重放和超参数优化。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">env = gym.make(<span class="string">&#x27;LunarLander-v2&#x27;</span>, render_mode=<span class="string">&quot;rgb_array&quot;</span>)</span><br><span class="line">state_dim = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line">action_dim = env.action_space.n</span><br></pre></td></tr></table></figure>

<p>我们初始化 LunarLander 环境并获取状态和动作空间的维度。state_dim表示状态中的特征数量，action_dim表示可能动作的数量。</p>
<h3 id="构建深度神经网络"><a href="#构建深度神经网络" class="headerlink" title="构建深度神经网络"></a>构建深度神经网络</h3><p>对于我们的深度神经网络，我们将创建一个名为DQN的类。这个类定义了一个具有三个全连接层的神经网络。输入层接收状态表示，隐藏层通过线性变换和 ReLU 激活函数处理这些信息，输出层为每个可能的动作产生 Q 值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DQN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, state_dim, action_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>(DQN, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(state_dim, <span class="number">128</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">128</span>, <span class="number">128</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">128</span>, action_dim)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(self.fc1(x))</span><br><span class="line">        x = torch.relu(self.fc2(x))</span><br><span class="line">        <span class="keyword">return</span> self.fc3(x)</span><br></pre></td></tr></table></figure>

<p>我们定义一个名为DQN的类，它继承自 nn.Module，这是 PyTorch 中所有神经网络模块的基类。这使我们能够利用 PyTorch 的内置函数和特性来构建神经网络。</p>
<h4 id="类初始化"><a href="#类初始化" class="headerlink" title="类初始化"></a>类初始化</h4><p>__init__方法是一种特殊的方法，用于初始化对象的属性。在我们这个例子，它设置了神经网络的每一层。</p>
<p>我们定义三个全连接（线性）层：</p>
<ul>
<li>self.fc1 &#x3D; nn.Linear(state_dim, 128) ：第一层接受输入状态维度（状态中的特征数量）并将其映射到 128 个神经元。</li>
<li>self.fc2 &#x3D; nn.Linear(128, 128): 第二层将来自第一层的 128 个神经元映射到另外 128 个神经元。</li>
<li>self.fc3 &#x3D; nn.Linear(128, action_dim): 第三层将来自第二层的 128 个神经元映射到动作维度（可能动作数量）。</li>
</ul>
<p>每一层nn.Linear对输入数据执行线性变换：</p>
<p><img src="/../asset_deepq/09.png" alt="线性变换"></p>
<p>其中 x 是输入，W 是权重矩阵，b 是偏置向量。</p>
<h4 id="forward方法"><a href="#forward方法" class="headerlink" title="forward方法"></a>forward方法</h4><p>forward方法定义了数据如何在网络中流动。当您通过网络传递数据时，该方法会自动调用。</p>
<p>在第一层中，输入数据x通过第一个全连接层self.fc1传递。然后使用 ReLU（修正线性单元）激活函数进行转换。</p>
<p>ReLU 激活函数定义为：</p>
<p><img src="/../asset_deepq/10.png" alt="ReLU 激活函数公式"></p>
<p>它将非线性引入模型中，使网络能够学习更复杂的函数。</p>
<p>在第二层中，来自第一层的输出通过第二个全连接层（self.fc2）传递，并再次使用 ReLU 激活函数进行转换.</p>
<p>最后，在输出层，第二层的输出通过第三个全连接层（self.fc3）传递，没有激活函数. 该层为每个动作生成最终的 Q 值。每个值代表在给定状态下采取该动作的预期未来奖励。</p>
<h3 id="实现经历回放"><a href="#实现经历回放" class="headerlink" title="实现经历回放"></a>实现经历回放</h3><p>该ReplayBuffer类提供了一种存储和采样经历的机制，这对于稳定和改进 DQNs 中的学习过程至关重要。因此，它使代理能够从各种过去经历中学习，增强其泛化能力，并在复杂环境中表现良好。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ReplayBuffer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, capacity</span>):</span><br><span class="line">        self.buffer = deque(maxlen=capacity)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">push</span>(<span class="params">self, state, action, reward, next_state, done</span>):</span><br><span class="line">        self.buffer.append((state, action, reward, next_state, done))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self, batch_size</span>):</span><br><span class="line">        state, action, reward, next_state, done = <span class="built_in">zip</span>(*random.sample(self.buffer, batch_size))</span><br><span class="line">        <span class="keyword">return</span> state, action, reward, next_state, done</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br></pre></td></tr></table></figure>

<h4 id="类初始化-1"><a href="#类初始化-1" class="headerlink" title="类初始化"></a>类初始化</h4><p>__init__方法使用固定容量初始化双端队列。双端队列允许您高效地从两端添加和弹出数据。它特别适用于实现需要快速从两端添加和弹出数据的队列和栈。<br>self.buffer &#x3D; deque(maxlen&#x3D;capacity) 实际上创建了一个 deque，可以容纳capacity个的经历。当缓冲区已满时，添加新经历将自动删除最旧的经历。</p>
<h4 id="push方法"><a href="#push方法" class="headerlink" title="push方法"></a>push方法</h4><p>该push 方法向缓冲区添加新经历。每个经历都是一个元组，包括state，action，reward，next_state和done：</p>
<ul>
<li>state ：当前状态。</li>
<li>action ：代理人采取的行动。</li>
<li>reward ：执行动作后获得的奖励。</li>
<li>next_state ：代理采取行动后得到的状态。</li>
<li>done ：一个布尔值，指示该场景是否已结束。</li>
</ul>
<h4 id="采样方法"><a href="#采样方法" class="headerlink" title="采样方法"></a>采样方法</h4><p>该sample方法从缓冲区中随机采样一批经历。<br>random.sample(self.buffer, batch_size) 从缓冲区中随机选择batch_size 经历。 </p>
<p>zip(*random.sample(self.buffer, batch_size)) 将经历列表解压成单独的元组，赋值于state， action， reward， next_state， done</p>
<p>该方法返回这些元组。</p>
<h4 id="length方法"><a href="#length方法" class="headerlink" title="length方法"></a>length方法</h4><p>__len__方法返回缓冲区中存储的当前经历数量。 </p>
<h3 id="实现目标网络"><a href="#实现目标网络" class="headerlink" title="实现目标网络"></a>实现目标网络</h3><p>目标网络，我们为训练提供了一组稳定的 Q 值，这有助于稳定学习过程并提高代理在复杂环境中的性能。目标网络更新频率低于主网络，确保用于更新主网络权重的 Q 值估计更加稳定。</p>
<p>我们将在一个名为DQNTrainer的类中实现目标网络，该类管理 DQN 的训练过程，包括主网络、目标网络、优化器和重放缓冲区。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DQNTrainer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, env, main_network, target_network, optimizer, replay_buffer, model_path=<span class="string">&#x27;model/model.pth&#x27;</span>, gamma=<span class="number">0.99</span>, batch_size=<span class="number">64</span>, target_update_frequency=<span class="number">1000</span></span>):</span><br><span class="line">        self.env = env</span><br><span class="line">        self.main_network = main_network</span><br><span class="line">        self.target_network = target_network</span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self.replay_buffer = replay_buffer</span><br><span class="line">        self.model_path = model_path</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.target_update_frequency = target_update_frequency</span><br><span class="line">        self.step_count = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Load the model if it exists</span></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(os.path.dirname(self.model_path)):</span><br><span class="line">            <span class="keyword">if</span> os.path.isfile(self.model_path):</span><br><span class="line">                self.main_network.load_state_dict(torch.load(self.model_path))</span><br><span class="line">                self.target_network.load_state_dict(torch.load(self.model_path))</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Loaded model from disk&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            os.makedirs(os.path.dirname(self.model_path))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, num_episodes, save_model=SAVE_MODEL, save_video=SAVE_VIDEO</span>):</span><br><span class="line">        total_rewards = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Create a VideoWriter to save the rendering</span></span><br><span class="line">        <span class="keyword">if</span> save_video:</span><br><span class="line">            videoPath = os.path.join(os.path.expanduser(<span class="string">&#x27;~&#x27;</span>), <span class="string">&#x27;deepq/training.mp4&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> os.path.exists(videoPath):</span><br><span class="line">                os.remove(videoPath)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(os.path.dirname(videoPath)):</span><br><span class="line">                    os.makedirs(os.path.dirname(videoPath))</span><br><span class="line">            self.video = VideoRecorder(env, videoPath, enabled=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> episode <span class="keyword">in</span> <span class="built_in">range</span>(num_episodes):</span><br><span class="line">            state, _ = self.env.reset()  <span class="comment"># Extract the state from the returned tuple</span></span><br><span class="line">            done = <span class="literal">False</span></span><br><span class="line">            total_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">                self.env.render()  <span class="comment"># Add this line to render the environment</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> save_video:</span><br><span class="line">                    self.video.capture_frame()</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># Ensure the state is in the correct shape by adding an extra dimension</span></span><br><span class="line">                action = self.main_network(torch.FloatTensor(state).unsqueeze(<span class="number">0</span>)).argmax(dim=<span class="number">1</span>).item()</span><br><span class="line">                next_state, reward, done, _, _ = self.env.step(action)  <span class="comment"># Extract the next_state from the returned tuple</span></span><br><span class="line">                self.replay_buffer.push(state, action, reward, next_state, done)</span><br><span class="line">                state = next_state</span><br><span class="line">                total_reward += reward</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(self.replay_buffer) &gt;= self.batch_size:</span><br><span class="line">                    self.update_network()</span><br><span class="line"></span><br><span class="line">            total_rewards.append(total_reward)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Episode <span class="subst">&#123;episode&#125;</span>, Total Reward: <span class="subst">&#123;total_reward&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Save the model after training</span></span><br><span class="line">        <span class="keyword">if</span> save_model:</span><br><span class="line">            torch.save(self.main_network.state_dict(), self.model_path)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Saved model to disk&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> save_video:</span><br><span class="line">            self.video.close()</span><br><span class="line"></span><br><span class="line">        self.env.close()</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(total_rewards) / <span class="built_in">len</span>(total_rewards)  <span class="comment"># Return average reward</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">update_network</span>(<span class="params">self</span>):</span><br><span class="line">        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.replay_buffer.sample(self.batch_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Convert to tensors</span></span><br><span class="line">        state_batch = torch.FloatTensor(state_batch)</span><br><span class="line">        action_batch = torch.LongTensor(action_batch)</span><br><span class="line">        reward_batch = torch.FloatTensor(reward_batch)</span><br><span class="line">        next_state_batch = torch.FloatTensor(next_state_batch)</span><br><span class="line">        done_batch = torch.FloatTensor(done_batch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the current Q-values</span></span><br><span class="line">        q_values = self.main_network(state_batch).gather(<span class="number">1</span>, action_batch.unsqueeze(<span class="number">1</span>)).squeeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the target Q-values</span></span><br><span class="line">        next_q_values = self.target_network(next_state_batch).<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">        expected_q_values = reward_batch + self.gamma * next_q_values * (<span class="number">1</span> - done_batch)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the loss</span></span><br><span class="line">        loss = nn.MSELoss()(q_values, expected_q_values.detach())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Optimize the model</span></span><br><span class="line">        self.optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        self.optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Periodically update the target network</span></span><br><span class="line">        <span class="keyword">if</span> self.step_count % self.target_update_frequency == <span class="number">0</span>:</span><br><span class="line">            self.target_network.load_state_dict(self.main_network.state_dict())</span><br><span class="line"></span><br><span class="line">        self.step_count += <span class="number">1</span></span><br></pre></td></tr></table></figure>

<h4 id="类定义"><a href="#类定义" class="headerlink" title="类定义"></a>类定义</h4><p>__init__方法初始化了训练所需的各种组件： </p>
<ul>
<li>env ：代理程序运行的环境。</li>
<li>main_network ：正在训练的主要神经网络。</li>
<li>target_network ：用于稳定 Q 值估计的目标神经网络。</li>
<li>optimizer ：用于更新主网络权重的优化器。</li>
<li>replay_buffer ：用于存储和采样经历的缓冲区。</li>
<li>model_path ：保存&#x2F;加载训练模型的路径。</li>
<li>gamma ：未来奖励的折现因子。</li>
<li>batch_size ：每个训练步骤从replay缓冲区中采样的经历数量。</li>
<li>target_update_frequency ：匹配主网络权重的目标网络权重更新频率</li>
<li>step_count ：一个用于跟踪训练过程中步骤数量的计数器。</li>
</ul>
<h4 id="模型加载"><a href="#模型加载" class="headerlink" title="模型加载"></a>模型加载</h4><p>我们用 os.path.exists(os.path.dirname(self.model_path)) 检查模型路径的目录是否存在。如果存在已保存的模型，则加载该模型以继续训练，从离开的地方继续 。</p>
<p>torch.load 使用 load_state_dict 将保存的模型权重加载到主网络和目标网络中。如果模型目录不存在，则使用 os.makedirs 创建它。</p>
<h3 id="训练深度-Q-网络"><a href="#训练深度-Q-网络" class="headerlink" title="训练深度 Q 网络"></a>训练深度 Q 网络</h3><p>接下来，我们将实现训练循环来训练我们的 DQN。这个DQNTrainer方法发生在内部。它运行 DQN 的训练循环，代理与环境交互，收集经验，更新网络，并跟踪性能。<br>train方法运行指定回合数量的训练循环。这个循环对于代理获取经验并提高其决策能力至关重要。</p>
<p>首先将total_rewards 初始化为空列表。 在每个回合开始时，环境会被重置到初始状态。 代理根据当前状态使用主网络选择动作。</p>
<p>torch.FloatTensor(state).unsqueeze(0) 将状态转换为 PyTorch 张量，并添加额外的维度以匹配网络的预期输入形状。</p>
<p>self.main_network(…).argmax(dim&#x3D;1).item() 选择由主网络预测的具有最高 Q 值的动作。</p>
<p>代理程序执行所选动作，观察奖励和下一个状态，并将经历存储在重放缓冲区中。</p>
<p>self.env.step(action) 执行动作并返回下一个状态、奖励以及该情节是否已结束。</p>
<p>self.replay_buffer.push(…) 将经历存储在重放缓冲区中。</p>
<p>state &#x3D; next_state 将当前状态更新为下一个状态。</p>
<p>total_reward +&#x3D; reward 累积当前回合的奖励。</p>
<p>如果重放缓冲区有足够的经历，网络就会被更新。</p>
<p>if len(self.replay_buffer) &gt;&#x3D; self.batch_size 检查回放缓冲区是否至少有batch_size经验。 </p>
<p>self.update_network() 使用来自重放缓冲区的一批经历更新网络。</p>
<p>每个回合结束时记录并打印总奖励。</p>
<p>total_rewards.append(total_reward) 将当前回合的总奖励添加到总奖励列表中。</p>
<p>训练完成后，模型被保存到磁盘。</p>
<p>torch.save(self.main_network.state_dict(), self.model_path) 将主网络的状态字典保存到指定的文件路径。</p>
<p>最后，该方法关闭环境并返回所有剧集的平均奖励。</p>
<p>return sum(total_rewards) &#x2F; len(total_rewards) 计算并返回平均奖励。</p>
<h3 id="调整模型"><a href="#调整模型" class="headerlink" title="调整模型"></a>调整模型</h3><p>最后，我们将看看如何评估和调整训练模型。让我们构建一个Optimizer类，负责优化超参数以提高 DQN 的性能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Optimizer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, env, main_network, target_network, replay_buffer, model_path, params_path=<span class="string">&#x27;params.pkl&#x27;</span></span>):</span><br><span class="line">        self.env = env</span><br><span class="line">        self.main_network = main_network</span><br><span class="line">        self.target_network = target_network</span><br><span class="line">        self.replay_buffer = replay_buffer</span><br><span class="line">        self.model_path = model_path</span><br><span class="line">        self.params_path = params_path</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">objective</span>(<span class="params">self, trial, n_episodes=<span class="number">10</span></span>):</span><br><span class="line">        lr = trial.suggest_loguniform(<span class="string">&#x27;lr&#x27;</span>, <span class="number">1e-5</span>, <span class="number">1e-1</span>)</span><br><span class="line">        gamma = trial.suggest_uniform(<span class="string">&#x27;gamma&#x27;</span>, <span class="number">0.9</span>, <span class="number">0.999</span>)</span><br><span class="line">        batch_size = trial.suggest_categorical(<span class="string">&#x27;batch_size&#x27;</span>, [<span class="number">32</span>, <span class="number">64</span>, <span class="number">128</span>])</span><br><span class="line">        target_update_frequency = trial.suggest_categorical(<span class="string">&#x27;target_update_frequency&#x27;</span>, [<span class="number">500</span>, <span class="number">1000</span>, <span class="number">2000</span>])</span><br><span class="line"></span><br><span class="line">        optimizer = optim.Adam(self.main_network.parameters(), lr=lr)</span><br><span class="line">        trainer = DQNTrainer(self.env, self.main_network, self.target_network, optimizer, self.replay_buffer, self.model_path, gamma=gamma, batch_size=batch_size, target_update_frequency=target_update_frequency)</span><br><span class="line">        reward = trainer.train(n_episodes, save=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> reward</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">optimize</span>(<span class="params">self, n_trials=<span class="number">100</span>, save_params=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> TRAIN <span class="keyword">and</span> os.path.isfile(self.params_path):</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(self.params_path, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                best_params = pickle.load(f)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Loaded parameters from disk&quot;</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> FINETUNE:</span><br><span class="line">            best_params = &#123;</span><br><span class="line">                <span class="string">&#x27;lr&#x27;</span>: LEARNING_RATE, </span><br><span class="line">                <span class="string">&#x27;gamma&#x27;</span>: GAMMA, </span><br><span class="line">                <span class="string">&#x27;batch_size&#x27;</span>: BATCH_SIZE, </span><br><span class="line">                <span class="string">&#x27;target_update_frequency&#x27;</span>: TARGET_UPDATE_FREQUENCY</span><br><span class="line">                &#125;</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Using default parameters: <span class="subst">&#123;best_params&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Optimizing hyperparameters&quot;</span>)</span><br><span class="line">            study = optuna.create_study(direction=<span class="string">&#x27;maximize&#x27;</span>)</span><br><span class="line">            study.optimize(self.objective, n_trials=n_trials)</span><br><span class="line">            best_params = study.best_params</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> save_params:</span><br><span class="line">                <span class="keyword">with</span> <span class="built_in">open</span>(self.params_path, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                    pickle.dump(best_params, f)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Saved parameters to disk&quot;</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> best_params</span><br></pre></td></tr></table></figure>

<h4 id="objective方法建议超参数的值，并使用这些值训练模型"><a href="#objective方法建议超参数的值，并使用这些值训练模型" class="headerlink" title="objective方法建议超参数的值，并使用这些值训练模型"></a>objective方法建议超参数的值，并使用这些值训练模型</h4><ul>
<li>lr &#x3D; trial.suggest_loguniform(‘lr’, 1e-5, 1e-1) ：建议学习率在[1e-5, 1e-1]范围内。</li>
<li>gamma &#x3D; trial.suggest_uniform(‘gamma’, 0.9, 0.999) ：建议在范围[0.9, 0.999]内选择折扣因子。</li>
<li>batch_size &#x3D; trial.suggest_categorical(‘batch_size’, [32, 64, 128]) ：建议从指定列表中选择一个批次大小。</li>
<li>target_update_frequency &#x3D; trial.suggest_categorical(‘target_update_frequency’, [500, 1000, 2000]) ：建议从指定列表中选择目标更新频率。</li>
</ul>
<p>我们使用建议的学习率设置了一个 Adam 优化器。Adam（自适应矩估计的缩写）是一种常用于训练神经网络的优化算法。</p>
<p>对于神经网络中的每个参数，Adam 计算损失函数对该参数的梯度。它跟踪梯度的指数移动平均值（第一时刻，表示为 m）和平方梯度（第二时刻，表示为 v）。</p>
<p>为了考虑移动平均数的初始化偏差，Adam 对第一和第二时刻的估计值应用偏差校正。然后使用校正后的第一和第二时刻来更新参数。更新规则旨在结合学习率和时刻，以一种考虑梯度的大小和方向的方式调整参数。</p>
<p>使用建议的超参数初始化DQNTrainer 实例。 最后，为模型训练指定数量的周期，并返回平均奖励</p>
<h4 id="Optimize方法，该方法运行指定次数的优化过程"><a href="#Optimize方法，该方法运行指定次数的优化过程" class="headerlink" title="Optimize方法，该方法运行指定次数的优化过程"></a>Optimize方法，该方法运行指定次数的优化过程</h4><p>我们使用 Optuna，这是一个 Python 库，它将帮助我们系统地探索超参数空间，高效地找到最大化模型性能的组合。</p>
<p>如果不需要训练(not TRAIN)并且参数文件存在，则从磁盘加载参数。</p>
<p>如果不需要微调（not FINETUNE），则使用默认参数。 </p>
<p>如果需要超参数优化，将使用 Optuna 来找到最佳参数。</p>
<p>study &#x3D; optuna.create_study(direction&#x3D;’maximize’) 创建一个 Optuna study以最大化objective函数。</p>
<p>study.optimize(self.objective, n_trials&#x3D;n_trials) 运行指定次数的优化。</p>
<p>如果save_params是True，则最佳参数将保存到磁盘。</p>
<h3 id="运行模型"><a href="#运行模型" class="headerlink" title="运行模型"></a>运行模型</h3><h4 id="设置训练和微调"><a href="#设置训练和微调" class="headerlink" title="设置训练和微调"></a>设置训练和微调</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">TRAIN = <span class="literal">True</span></span><br><span class="line">FINETUNE = <span class="literal">False</span></span><br><span class="line">SAVE_MODEL = <span class="literal">True</span> <span class="comment"># Save the model after training</span></span><br><span class="line">SAVE_VIDEO = <span class="literal">False</span> <span class="comment"># Save the video of the training process</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Training hyperparameters</span></span><br><span class="line">TRAINING_EPISODES = <span class="number">1000</span> <span class="comment"># valid only if TRAIN is True</span></span><br><span class="line">FINETUNE_TRIALS = <span class="number">100</span> <span class="comment"># valid only if FINETUNE is True</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the following hyperparameters if FINETUNE is False</span></span><br><span class="line">GAMMA = <span class="number">0.99</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line">TARGET_UPDATE_FREQUENCY = <span class="number">1000</span></span><br><span class="line">LEARNING_RATE = <span class="number">1e-3</span></span><br></pre></td></tr></table></figure>

<p>TRAIN &#x3D; True 表示是否训练模型。如果设置为False，训练将被跳过。 </p>
<p>FINETUNE &#x3D; False 表示是否微调模型。如果设置为True，将使用现有参数并进行微调。</p>
<p>如果FINETUNE是False，我们设置以下超参数： </p>
<ul>
<li>GAMMA &#x3D; 0.99 ：未来奖励的折现因子。这决定了未来奖励相对于即时奖励的价值有多大。</li>
<li>BATCH_SIZE &#x3D; 64 ：每个训练步骤从重播缓冲区中采样的经历数量。</li>
<li>TARGET_UPDATE_FREQUENCY &#x3D; 1000 ：匹配主网络权重的目标网络权重更新的频率（以步数计）。</li>
<li>LEARNING_RATE &#x3D; 1e-3 ：优化器的学习率，控制模型在每次更新模型权重时根据估计误差进行的改变大小。</li>
</ul>
<h4 id="初始化网络和重放缓冲区"><a href="#初始化网络和重放缓冲区" class="headerlink" title="初始化网络和重放缓冲区"></a>初始化网络和重放缓冲区</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">main_network = DQN(state_dim, action_dim)</span><br><span class="line">target_network = DQN(state_dim, action_dim)</span><br><span class="line">target_network.load_state_dict(main_network.state_dict())</span><br><span class="line">target_network.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">replay_buffer = ReplayBuffer(<span class="number">10000</span>)</span><br></pre></td></tr></table></figure>


<p>main_network &#x3D; DQN(state_dim, action_dim) 使用指定的状态和动作维度初始化主网络。</p>
<p>target_network &#x3D; DQN(state_dim, action_dim) 使用与主网络相同的架构初始化目标网络。</p>
<p>target_network.load_state_dict(main_network.state_dict()) 将权重从主网络复制到目标网络。</p>
<p>target_network.eval() 将目标网络设置为评估模式。这可以确保在推断期间某些层（如 dropout 和批量归一化）的行为是适当的。</p>
<p>replay_buffer &#x3D; ReplayBuffer(10000) 使用容量为 10,000 的重放缓冲区进行初始化。</p>
<h4 id="设置步数"><a href="#设置步数" class="headerlink" title="设置步数"></a>设置步数</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">STEP_COUNT = <span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>STEP_COUNT &#x3D; 0 初始化一个计数器，用于跟踪训练过程中所采取的步骤数量。</p>
<h4 id="优化器初始化和超参数优化"><a href="#优化器初始化和超参数优化" class="headerlink" title="优化器初始化和超参数优化"></a>优化器初始化和超参数优化</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">home_directory = os.path.expanduser(<span class="string">&#x27;~&#x27;</span>)</span><br><span class="line"></span><br><span class="line">optimizer = Optimizer(env, main_network, target_network, replay_buffer, <span class="string">f&#x27;<span class="subst">&#123;home_directory&#125;</span>/model/model.pth&#x27;</span>, <span class="string">f&#x27;<span class="subst">&#123;home_directory&#125;</span>/model/params.pkl&#x27;</span>)</span><br><span class="line">best_params = optimizer.optimize(n_trials=<span class="number">2</span>, save_params=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>optimizer &#x3D; Optimizer(…) 使用环境、网络、重放缓冲区、模型路径和参数路径初始化Optimizer 类。</p>
<p>best_params &#x3D; optimizer.optimize(n_trials&#x3D;2, save_params&#x3D;True) 运行优化过程以找到最佳超参数。这个函数：</p>
<ul>
<li>运行指定次数的优化（n_trials&#x3D;2）。 </li>
<li>如果save_params是True，则将最佳超参数保存到磁盘。</li>
</ul>
<h4 id="创建-PyTorch-优化器和-DQN-训练器"><a href="#创建-PyTorch-优化器和-DQN-训练器" class="headerlink" title="创建 PyTorch 优化器和 DQN 训练器"></a>创建 PyTorch 优化器和 DQN 训练器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam(main_network.parameters(), lr=best_params[<span class="string">&#x27;lr&#x27;</span>])</span><br><span class="line">trainer = DQNTrainer(env, main_network, target_network, optimizer, replay_buffer, <span class="string">f&#x27;<span class="subst">&#123;home_directory&#125;</span>/model/model.pth&#x27;</span>, gamma=best_params[<span class="string">&#x27;gamma&#x27;</span>], batch_size=best_params[<span class="string">&#x27;batch_size&#x27;</span>], target_update_frequency=best_params[<span class="string">&#x27;target_update_frequency&#x27;</span>])</span><br><span class="line">trainer.train(TRAINING_EPISODES, save_model=SAVE_MODEL, save_video=SAVE_VIDEO)</span><br></pre></td></tr></table></figure>

<p>optimizer &#x3D; optim.Adam(main_network.parameters(), lr&#x3D;best_params[‘lr’]) 使用最佳超参数创建一个 Adam 优化器。</p>
<p>trainer &#x3D; DQNTrainer(…) 使用最佳参数从环境、网络、优化器、重放缓冲区、模型路径和超参数初始化DQNTrainer 类。 </p>
<p>trainer.train(…) 为模型训练了 TRAINING_EPISODES 个周期。</p>
<p>代理在培训的前 10 回合中的表现：</p>
<p><img src="/../asset_deepq/11.gif"></p>
<p>可以看到模型笨拙，做出随机且经常是次优的决策。这是预期的，因为代理仍在探索环境并学习基础知识。它还没有制定出最大化奖励的强大策略。随着更多的训练周期，代理的表现应该会显著提高，因为它不断完善其策略并从经验中学习。</p>
<p>模型训练了 1000 次后的 10 个训练集</p>
<p><img src="/../asset_deepq/12.gif"></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>采用先进技术并探索新架构，以提高您的代理性能。例如，您可以尝试设置不同的超参数，使用不同的优化算法（如 SGD 或 Nadam），使用不同的微调算法等等！</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/reinforcement-learning-from-scratch-deep-q-networks-0a8d33ce165b">https://towardsdatascience.com/reinforcement-learning-from-scratch-deep-q-networks-0a8d33ce165b</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/05/20/voice-with-llama3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/05/20/voice-with-llama3/" class="post-title-link" itemprop="url">用llama3实现语音助手</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-05-20 15:51:55" itemprop="dateCreated datePublished" datetime="2024-05-20T15:51:55+08:00">2024-05-20</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-05-27 17:28:26" itemprop="dateModified" datetime="2024-05-27T17:28:26+08:00">2024-05-27</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>13k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>24 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="技术栈"><a href="#技术栈" class="headerlink" title="技术栈"></a>技术栈</h2><p>Whisper: 由 OpenAI 开发，Whisper 擅长将口语转录为文本。其理解和处理多种语言的能力使其成为任何基于语音的应用程序的必不可少的工具。</p>
<p>LangChain 用于协调组件处理模型和数据库的复杂用户交互。</p>
<p>矢量数据库（Qdrant）：Qdrant 旨在高效处理高维数据，使其非常适用于依赖机器学习和大规模数据检索的应用程序。</p>
<p>检索增强生成（RAG）：RAG 结合了检索和生成模型的优点，使我们的语音助手能够利用大量信息数据库生成明智和具有上下文相关性的回应。</p>
<h2 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h2><ul>
<li><p>transformers（4.33.0）：提供了各种预构建模型，用于文本翻译和摘要等语言任务，使其成为语言项目的关键工具。</p>
</li>
<li><p>accelerate （0.22.0）：帮助在不需要改变太多代码的情况下，在不同类型的计算机硬件上运行机器学习模型，如 CPU 或 GPU。</p>
</li>
<li><p>einops (0.6.1)：使得在机器学习中使用的数据结构更易于处理和改变形状，有助于构建复杂模型。</p>
</li>
<li><p>langchain (0.0.300)：用于将不同的语言技术结合到一个应用程序中，特别适用于需要多个处理步骤的项目。</p>
</li>
<li><p>xformers（0.0.22.post7）：提供模型的部分，这些部分在学习和使用阶段都能够高效处理数据。</p>
</li>
<li><p>bitsandbytes-windows（windows版本）：有助于更快地训练深度学习模型，并且占用更少的内存，非常适合处理大型数据集。</p>
</li>
<li><p>sentence_transformers (2.2.2)：基于 transformers 库构建详细特征，对于需要理解文本之间相似性的任务非常重要。</p>
</li>
</ul>
<p>让我们首先建立一个虚拟环境并安装库。打开命令行界面 ，然后运行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create a virtual environment</span></span><br><span class="line">conda create -n llama3-whisper python=3.11</span><br><span class="line">conda activate llama3-whisper</span><br></pre></td></tr></table></figure>

<p>我的CUDA版本是12.1，如果在线安装torch 持续中断失败，可以去下载 <a target="_blank" rel="noopener" href="https://download.pytorch.org/whl/">wheel</a> 文件，然后本地安装。</p>
<p>在线安装torch：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu121</span><br></pre></td></tr></table></figure>

<p>本地安装torch，进入wheel文件所在的目录:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install <span class="string">&quot;torch-2.1.0+cu121-cp311-cp311-win_amd64.whl&quot;</span> </span><br><span class="line">pip install <span class="string">&quot;torchaudio-2.1.0+cu121-cp311-cp311-win_amd64.whl&quot;</span></span><br><span class="line">pip install <span class="string">&quot;torchvision-0.16.0+cu121-cp311-cp311-win_amd64.whl&quot;</span></span><br></pre></td></tr></table></figure>
<p>安装完torch后，安装xformers， 因为xformers和torch的版本对应没有文档，所以要一起安装来约束xformers：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0  xformers  --index-url https://download.pytorch.org/whl/cu121</span><br></pre></td></tr></table></figure>

<p>约束后，可以看到安装的xformers版本，所以可以用以下命令直接安装xformers。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install xformers==0.0.22.post7 --index-url https://download.pytorch.org/whl/cu121</span><br></pre></td></tr></table></figure>

<p>继续安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Install dependencies</span></span><br><span class="line">pip3 install openai</span><br><span class="line">pip3 install transformers==4.33.0 </span><br><span class="line">pip3 install accelerate==0.22.0 </span><br><span class="line">pip3 install einops==0.6.1 </span><br><span class="line">pip3 install langchain==0.0.300 </span><br><span class="line">pip3 install bitsandbytes-windows</span><br><span class="line">pip3 install sentence_transformers==2.2.2</span><br><span class="line">pip3 install arxiv</span><br><span class="line">pip3 install huggingface_hub</span><br><span class="line">pip3 install optimum</span><br><span class="line">pip3 install <span class="string">&quot;git+https://github.com/PanQiWei/AutoGPTQ.git@v0.7.1&quot;</span> </span><br></pre></td></tr></table></figure>

<p>为了准备从 PDF 文件中提取数据、执行 OCR 并创建嵌入，以进行高级数据处理和检索，我们还需要安装一些软件：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pip3 install unstructured</span><br><span class="line">pip3 install <span class="string">&quot;unstructured[pdf]&quot;</span></span><br><span class="line">pip3 install poppler-utils</span><br><span class="line">pip3 install pytesseract</span><br><span class="line">pip3 install grpcio-tools==1.60.1</span><br><span class="line">pip3 install qdrant-client==1.7.2</span><br><span class="line">pip3 install WhisperSpeech</span><br><span class="line">pip3 install rich</span><br></pre></td></tr></table></figure>
<p>由于我的win10在线安装tesseract-ocr不成功，改成<a target="_blank" rel="noopener" href="https://github.com/simonflueckiger/tesserocr-windows_build/releases">wheel</a>安装：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip3 install tesserocr-2.7.0-cp311-cp311-win_amd64.whl</span><br></pre></td></tr></table></figure>

<p>设置系统环境变量，window如下图：<br><img src="/../asset_voicellama3/01.png"></p>
<p>注册Hugging Face，生成token. 登录 Hugging Face Hub, 预先下载模型，并保存到本地。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loging to Huggingface Hub</span></span><br><span class="line">huggingface-cli login</span><br><span class="line">huggingface-cli download --resume-download astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit</span><br></pre></td></tr></table></figure>

<h2 id="导入库"><a href="#导入库" class="headerlink" title="导入库"></a>导入库</h2><p>导入必要的库，包括模型交互、文档处理和嵌入管理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> HTTPError</span><br><span class="line"><span class="keyword">import</span> arxiv</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> cuda, bfloat16</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> transformers</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForCausalLM, GPTQConfig</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> langchain.llms <span class="keyword">import</span> HuggingFacePipeline</span><br><span class="line"><span class="keyword">from</span> langchain.document_loaders <span class="keyword">import</span> PyPDFLoader,DirectoryLoader,WebBaseLoader</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> RecursiveCharacterTextSplitter,CharacterTextSplitter</span><br><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> HuggingFaceEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> RetrievalQA</span><br><span class="line"><span class="keyword">from</span> langchain.vectorstores <span class="keyword">import</span> Qdrant</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"><span class="keyword">from</span> whisperspeech.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> rich.console <span class="keyword">import</span> Console</span><br><span class="line"><span class="keyword">from</span> rich.markdown <span class="keyword">import</span> Markdown</span><br></pre></td></tr></table></figure>

<h2 id="处理语音助手的数据"><a href="#处理语音助手的数据" class="headerlink" title="处理语音助手的数据"></a>处理语音助手的数据</h2><p>为 AI 应用构建数据管道, 数据管道对于高效管理和处理应用程序中的数据至关重要，特别是启用了带 RAG 的语音助手等复杂应用程序。</p>
<p>这些管道通常涉及五个关键阶段：</p>
<ul>
<li><p>收集： 在这个阶段，数据从各种来源收集，包括数据存储、数据流和应用程序。对于语音助手来说，这意味着从用户互动、音频输入以及内部和外部数据库收集数据。数据可以来自语音助手需要交互的远程设备、应用程序或业务系统。典型的工具有 Apache Nifi、Apache Flume、Talend 和自定义 API。</p>
</li>
<li><p>摄取：在摄取过程中，收集的数据被加载到系统中，并在事件队列中进行组织。对于语音助手，这涉及捕获音频输入，将其转录为文本，并将其排队等待进一步处理。摄取过程确保所有传入数据都准备好进行实时或批处理。典型工具包括 Apache Kafka、AWS Kinesis、Google Cloud Pub&#x2F;Sub、Apache Airflow。</p>
</li>
<li><p>存储：在摄取后，组织好的数据存储在各种存储解决方案中，如数据仓库、数据湖和数据湖屋。在语音助手的背景下，这包括存储转录、用户查询以及从 RAG 系统检索的文档。存储系统确保数据可供未来处理和分析。典型工具有 Amazon S3、Google Cloud Storage、Azure Data Lake、Snowflake、Apache Hudi、Delta Lake。</p>
</li>
<li><p>处理：在这个阶段，数据经历转换任务，如聚合、清洗和操作，以确保它符合所需的标准。对于语音助手，这意味着将文本数据转换为向量，压缩它，并将其分区以实现高效检索。为了确保数据始终是最新和准确的，使用批处理（一次处理大型数据集）和流处理（实时处理数据）技术。典型的工具有 Apache Spark、Apache Flink、Databricks、AWS Glue、Google Cloud Dataflow。</p>
</li>
<li><p>消费：最后阶段涉及将处理后的数据提供给用户使用。在语音助手的背景下，这意味着使系统能够准确理解和回应用户的查询。它还可以支持决策引擎和面向用户的应用程序，使语音助手能够对用户请求提供相关和及时的响应。典型工具有 Tableau、Power BI、Looker、Elasticsearch、Kibana、Apache Superset、自定义仪表板。</p>
</li>
</ul>
<p>让我们创建一个目录，搜索并下载“LLM”搜索词的Arxiv论文：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">dirpath = <span class="string">&quot;arxiv_papers&quot;</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(dirpath):</span><br><span class="line">   os.makedirs(dirpath)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download_papers</span>():</span><br><span class="line">    search = arxiv.Search(</span><br><span class="line">    query = <span class="string">&quot;LLM&quot;</span>, <span class="comment"># your query length is limited by ARXIV_MAX_QUERY_LENGTH which is 300 characters</span></span><br><span class="line">    max_results = <span class="number">10</span>,</span><br><span class="line">    sort_by = arxiv.SortCriterion.LastUpdatedDate, <span class="comment"># you can also use SubmittedDate or Relevance</span></span><br><span class="line">    sort_order = arxiv.SortOrder.Descending</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> result <span class="keyword">in</span> search.results():</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                result.download_pdf(dirpath=dirpath)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;-&gt; Paper id <span class="subst">&#123;result.get_short_id()&#125;</span> with title &#x27;<span class="subst">&#123;result.title&#125;</span>&#x27; is downloaded.&quot;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">except</span> FileNotFoundError:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;File not found&quot;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">except</span> HTTPError:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Forbidden&quot;</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">except</span> ConnectionResetError <span class="keyword">as</span> e:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;Connection reset by peer&quot;</span>)</span><br><span class="line">                time.sleep(<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<h2 id="RAG简要概述"><a href="#RAG简要概述" class="headerlink" title="RAG简要概述"></a>RAG简要概述</h2><p>RAG 工作流帮助我们管理和利用来自各种来源的数据，以提供准确和相关的结果。</p>
<p><img src="/../asset_voicellama3/02.png"></p>
<ul>
<li><p>数据加载：从不同来源收集数据，如文本文件、PDF、网站、数据库或 API。例如，Llama Hub 提供许多连接器，使这一步骤更容易。</p>
</li>
<li><p>索引：在索引阶段，系统将原始数据转换为向量嵌入，并对其进行组织。</p>
<ul>
<li><p>矢量化：每个文档或数据片段都被转换为一个高维向量，利用句子转换器等模型捕捉语义含义。</p>
</li>
<li><p>结构化：这些向量然后被组织成一个高效的数据结构，通常是一个 n 维树或哈希映射，从而实现快速的相似性搜索。</p>
</li>
</ul>
</li>
<li><p>存储：保存索引数据和标签，这样您以后就不必再次整理它。</p>
</li>
<li><p>查询: 在查询阶段，系统根据查询向量检索最相关的文档。</p>
<ul>
<li>向量匹配：将查询转换为向量，并使用余弦相似度或其他距离度量标准与索引向量进行比较。</li>
<li>检索：系统检索与查询向量最接近的文档，确保系统能够提供的响应在语境上和用户的请求相关。</li>
</ul>
</li>
<li><p>评估: 评估可能会因其随机性而变得相当具有挑战性。然而，可以用度量标准和工具进行客观评估。</p>
</li>
</ul>
<p>一些示例指标可能包括：忠实度、答案相关性、上下文精度、召回率、相关性和实体召回、答案语义相似度、答案正确性。</p>
<h2 id="文本分割器"><a href="#文本分割器" class="headerlink" title="文本分割器"></a>文本分割器</h2><p>使用 text_splitter 来管理大型文本文档，将它们分成更小、更易管理的块：</p>
<ul>
<li><p>RecursiveCharacterTextSplitter 递归地将文本分割成更小的片段，适用于非常大的文本。它有 2 个主要参数：</p>
<ul>
<li>chunk_size ：每个块的最大字符数（例如，1000 个字符）。</li>
<li>chunk_overlap ：保持上下文的重叠块大小（例如，100 个字符）。</li>
</ul>
<p>  这通常最适合于没有自然分割点的非常大的文本，并通过保持块之间的重叠来防止上下文丢失，确保后续处理具有连续性。</p>
</li>
<li><p>CharacterTextSplitter 根据指定的字符分隔符拆分文本，非常适合具有自然分隔的文本。它有 3 个主要参数</p>
<ul>
<li>separator ：用于分隔的字符（例如， \n 用于换行）。</li>
<li>chunk_size 和 chunk_overlap ：类似于递归分割器，定义块的大小和重叠。</li>
</ul>
<p>  适用于具有明确分界点的文本，如脚本或具有明确定义部分的文档，通过在自然断点处分割文本来确保数据完整性，有助于保持意义和上下文，而无需重叠。</p>
</li>
</ul>
<h2 id="文档加载器"><a href="#文档加载器" class="headerlink" title="文档加载器"></a>文档加载器</h2><p>文档加载器在处理自然语言处理工作流中的不同数据源时至关重要。有以下几种：</p>
<ul>
<li>DirectoryLoader: 从指定目录加载所有文件，通常用于处理多个文本或 PDF 文件。</li>
<li>WebBaseLoader：从指定的 URL 检索文本，从中提取网络内容以进行处理。</li>
<li>PyPDFLoader：专注于从单个 PDF 文件中提取文本以进行进一步分析。</li>
<li>TextLoader：专门设计用于加载纯文本文件，直接读取文本数据以供立即使用。</li>
</ul>
<p>所有的加载器都用于收集数据，然后对数据进行处理，可能用于生成嵌入。</p>
<p>我们将使用 DirectoryLoader 和 RecursiveCharacterTextSplitter 来高效地分块和管理多个文件。</p>
<p>分割器和文档加载器的结合使用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">document_loader_spilter</span>():</span><br><span class="line">    papers = []</span><br><span class="line">    loader = DirectoryLoader(dirpath, glob=<span class="string">&quot;./*.pdf&quot;</span>, loader_cls=PyPDFLoader)</span><br><span class="line">    papers = loader.load()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Total number of pages loaded:&quot;</span>, <span class="built_in">len</span>(papers)) <span class="comment"># Total number of pages loaded: 410</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># This merges all papes from all papers into single text block for chunking</span></span><br><span class="line">    full_text = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">for</span> paper <span class="keyword">in</span> papers:</span><br><span class="line">        full_text = full_text + paper.page_content</span><br><span class="line">        </span><br><span class="line">    full_text = <span class="string">&quot; &quot;</span>.join(l <span class="keyword">for</span> l <span class="keyword">in</span> full_text.splitlines() <span class="keyword">if</span> l)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(full_text))</span><br><span class="line"></span><br><span class="line">    text_splitter = RecursiveCharacterTextSplitter(</span><br><span class="line">        chunk_size = <span class="number">500</span>,</span><br><span class="line">        chunk_overlap  = <span class="number">50</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    paper_chunks = text_splitter.create_documents([full_text])</span><br><span class="line">    <span class="keyword">return</span> paper_chunks</span><br></pre></td></tr></table></figure>

<h2 id="模型配置"><a href="#模型配置" class="headerlink" title="模型配置"></a>模型配置</h2><p>配置了一个用于语言生成任务的 Meta LLaMA 3 模型,由于我的显卡只有8GB显存，所以只能使用 4-bit 模型。</p>
<p>配置：</p>
<ul>
<li>model_id ：标识具有 80 亿参数的特定 Meta LLaMA 模型，用于高级语言任务。</li>
<li>device ：将模型设置为在 GPU（ “cuda” ）上运行，提高处理速度和效率。</li>
<li>dtype ：使用 torch.float16 来优化内存和计算速度。</li>
</ul>
<p>初始化:</p>
<ul>
<li>tokenizer ：从 Hugging Face 加载一个分词器，将文本预处理为模型可以理解的标记。</li>
<li>model ：使用 AutoModelForCausalLM.from_pretrained 配置初始化模型，用于因果语言建模，模型根据先前文本预测下一个单词。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_llm_model</span>():</span><br><span class="line">    model_id = <span class="string">&quot;astronomer/Llama-3-8B-Instruct-GPTQ-4-Bit&quot;</span></span><br><span class="line">    device = <span class="string">&quot;cuda&quot;</span></span><br><span class="line">    dtype = torch.float16</span><br><span class="line">    </span><br><span class="line">    tokenizer = AutoTokenizer.from_pretrained(model_id)</span><br><span class="line">    model = AutoModelForCausalLM.from_pretrained(model_id, device_map=device, torch_dtype=dtype)</span><br><span class="line">    <span class="keyword">return</span> model, tokenizer</span><br></pre></td></tr></table></figure>

<h2 id="设置查询管道并初始化管道"><a href="#设置查询管道并初始化管道" class="headerlink" title="设置查询管道并初始化管道"></a>设置查询管道并初始化管道</h2><p>现在我们使用 Hugging Face的 transformers 库设置一个 query_pipeline ，旨在简化预训练模型和分词器的使用：</p>
<ul>
<li>model ：指定预训练语言模型。</li>
<li>tokenizer ：将输入文本转换为标记。</li>
<li>torch_dtype ：使用 torch.float16 进行高效计算。</li>
<li>max_length ：将输出限制在 1024 个标记。</li>
<li>device_map ：自动优化模型层的分配到可用硬件。</li>
</ul>
<p>然后使用我们配置的 query_pipeline 初始化一个 HuggingFacePipeline 对象，以便简化文本生成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_query_pipeline</span>(<span class="params">model, tokenizer</span>):</span><br><span class="line">    query_pipeline = transformers.pipeline(</span><br><span class="line">            <span class="string">&quot;text-generation&quot;</span>,</span><br><span class="line">            model=model,</span><br><span class="line">            tokenizer=tokenizer,</span><br><span class="line">            torch_dtype=torch.float16,</span><br><span class="line">            max_length=<span class="number">1024</span>,</span><br><span class="line">            device_map=<span class="string">&quot;auto&quot;</span>,)</span><br><span class="line"></span><br><span class="line">    llm = HuggingFacePipeline(pipeline=query_pipeline)</span><br><span class="line">    <span class="keyword">return</span> llm</span><br></pre></td></tr></table></figure>


<h2 id="处理模型加载并回退到本地资源"><a href="#处理模型加载并回退到本地资源" class="headerlink" title="处理模型加载并回退到本地资源"></a>处理模型加载并回退到本地资源</h2><p>我们现在将从 Hugging Face 的存储库中加载 sentence-transformers&#x2F;all-mpnet-base-v2 嵌入模型，配置为在 CUDA 设备上运行。</p>
<p>如果此过程遇到任何问题，比如连接问题或访问限制，您也可以添加异常以返回使用本地存储的嵌入模型。</p>
<p>通过这种方法，我们的应用程序可以在主要来源不可用时继续使用备用模型进行处理，这有助于我们在不同的运行环境中保持稳健性。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_embeddings_model</span>():</span><br><span class="line">    model_name = <span class="string">&quot;sentence-transformers/all-mpnet-base-v2&quot;</span></span><br><span class="line">    model_kwargs = &#123;<span class="string">&quot;device&quot;</span>: <span class="string">&quot;cuda&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># try to access the sentence transformers from HuggingFace: https://huggingface.co/api/models/sentence-transformers/all-mpnet-base-v2</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> ex:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Exception: &quot;</span>, ex)</span><br><span class="line">        <span class="comment"># # alternatively, we will access the embeddings models locally</span></span><br><span class="line">        <span class="comment"># local_model_path = &quot;/kaggle/input/sentence-transformers/minilm-l6-v2/all-MiniLM-L6-v2&quot;</span></span><br><span class="line">        <span class="comment"># print(f&quot;Use alternative (local) model: &#123;local_model_path&#125;\n&quot;)</span></span><br><span class="line">        <span class="comment"># embeddings = HuggingFaceEmbeddings(model_name=local_model_path, model_kwargs=model_kwargs)</span></span><br><span class="line">    <span class="keyword">return</span> embeddings</span><br></pre></td></tr></table></figure>

<h2 id="集成-Qdrant-用于嵌入式存储和检索"><a href="#集成-Qdrant-用于嵌入式存储和检索" class="headerlink" title="集成 Qdrant 用于嵌入式存储和检索"></a>集成 Qdrant 用于嵌入式存储和检索</h2><p>我们将使用 Qdrant 作为我们的向量数据库，因为它在处理向量相似性搜索、可扩展性和灵活的向量数据管理方面具有出色的能力。</p>
<p>此外，Qdrant 支持本地和云存储选项，以便您可以适应各种本地和云环境。</p>
<p>我们已经安装了 Qdrant，并且正在从 LangChain 的向量存储中导入它</p>
<p>Qdrant.from_documents 方法通过将文档及其对应的嵌入作为输入来简化流程。</p>
<p>然后将 vectordb 对象转换为一个具有 vectordb.as_retriever() 的检索器。该检索器被配置为根据向量相似性查询向量数据库，以便检索相关文档，这对于有效的信息检索至关重要。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_retriever</span>():</span><br><span class="line">    documents=document_loader_spilter()</span><br><span class="line">    embeddings=load_embeddings_model()</span><br><span class="line"></span><br><span class="line">    vectordb = Qdrant.from_documents(</span><br><span class="line">        documents,</span><br><span class="line">        embeddings,</span><br><span class="line">        path=<span class="string">&quot;Qdrant_Persist&quot;</span>,</span><br><span class="line">        collection_name=<span class="string">&quot;voice_assistant_documents&quot;</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    retriever = vectordb.as_retriever()</span><br><span class="line">    <span class="keyword">return</span> retriever</span><br></pre></td></tr></table></figure>

<p>使用的参数的详细说明：</p>
<ul>
<li>documents ：生成嵌入的原始文档。</li>
<li>embeddings ：从文档中提取的嵌入已准备好被索引和存储。</li>
<li>path ：指定的本地目录，Qdrant 数据库将在其中持久保存数据，确保嵌入数据安全存储并易于将来检索。</li>
<li>collection_name ：Qdrant 中数据集的标签，有助于组织和检索特定组的嵌入。</li>
</ul>
<p>如果想要在 Qdrant 向量数据库中重用持久化数据, 创建一个 QdrantClient 实例，指向存储我们数据库文件的路径，从而实现对持久化数据的访问。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> qdrant_client <span class="keyword">import</span> QdrantClient</span><br><span class="line"></span><br><span class="line">client = QdrantClient(path = <span class="string">&quot;Qdrant_Persist&quot;</span>)</span><br><span class="line"></span><br><span class="line">vectordb = Qdrant(</span><br><span class="line">    client=client,</span><br><span class="line">    collection_name=<span class="string">&quot;voice_assistant_documents&quot;</span>,</span><br><span class="line">    embeddings=embeddings,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h2 id="设置检索器"><a href="#设置检索器" class="headerlink" title="设置检索器"></a>设置检索器</h2><p>用我们 Qdrant 向量数据库中存储的嵌入来建立一个检索式问答（QA）系统</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model, tokenizer=load_llm_model()</span><br><span class="line">llm=load_query_pipeline(model, tokenizer)</span><br><span class="line">retriever=create_retriever()</span><br><span class="line">qa = RetrievalQA.from_chain_type(</span><br><span class="line">    llm=llm,</span><br><span class="line">    chain_type=<span class="string">&quot;stuff&quot;</span>,</span><br><span class="line">    retriever=retriever,</span><br><span class="line">    verbose=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>然后我们初始化一个 RetrievalQA 实例，它是我们 AI 链中的一部分。该实例使用检索器来获取与查询相关的信息。这里， llm 代表我们的语言模型， chain_type 设置为”stuff”，表示此链将处理的任务或操作类型， verbose&#x3D;True 在操作期间启用详细输出，提供有关检索过程的见解。</p>
<h2 id="测试和可视化-RAG-系统"><a href="#测试和可视化-RAG-系统" class="headerlink" title="测试和可视化 RAG 系统"></a>测试和可视化 RAG 系统</h2><p>我们实现了用于测试和可视化检索增强生成（RAG）系统的功能：</p>
<ul>
<li>colorize_text 功能: 为“Reasoning”、“Question”、“Answer”和“Total time”等关键术语添加颜色，以获得清晰且视觉上吸引人的输出。</li>
<li>test_rag 功能: 接受 QA 系统 ( qa ) 和查询字符串。它测量响应时间，检索答案，并在 Markdown 中显示格式化结果，突出关键元素以便阅读。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">colorize_text</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="keyword">for</span> word, color <span class="keyword">in</span> <span class="built_in">zip</span>([<span class="string">&quot;Reasoning&quot;</span>, <span class="string">&quot;Question&quot;</span>, <span class="string">&quot;Answer&quot;</span>, <span class="string">&quot;Total time&quot;</span>], [<span class="string">&quot;blue&quot;</span>, <span class="string">&quot;red&quot;</span>, <span class="string">&quot;green&quot;</span>, <span class="string">&quot;magenta&quot;</span>]):</span><br><span class="line">        text = text.replace(<span class="string">f&quot;<span class="subst">&#123;word&#125;</span>:&quot;</span>, <span class="string">f&quot;\n\n**&lt;font color=&#x27;<span class="subst">&#123;color&#125;</span>&#x27;&gt;<span class="subst">&#123;word&#125;</span>:&lt;/font&gt;**&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_rag</span>(<span class="params">qa, query</span>):</span><br><span class="line">    console = Console()</span><br><span class="line"></span><br><span class="line">    time_start = time()</span><br><span class="line">    response = qa.run(query)</span><br><span class="line">    time_end = time()</span><br><span class="line">    total_time = <span class="string">f&quot;<span class="subst">&#123;<span class="built_in">round</span>(time_end-time_start, <span class="number">3</span>)&#125;</span> sec.&quot;</span></span><br><span class="line"></span><br><span class="line">    full_response =  <span class="string">f&quot;Question: <span class="subst">&#123;query&#125;</span>\nAnswer: <span class="subst">&#123;response&#125;</span>\nTotal time: <span class="subst">&#123;total_time&#125;</span>&quot;</span></span><br><span class="line">    colored_text = colorize_text(full_response)</span><br><span class="line">    console.<span class="built_in">print</span>(Markdown(colored_text))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> response</span><br></pre></td></tr></table></figure>

<h2 id="Llama-3-和-Whisper-的文本转语音处理"><a href="#Llama-3-和-Whisper-的文本转语音处理" class="headerlink" title="Llama 3 和 Whisper 的文本转语音处理"></a>Llama 3 和 Whisper 的文本转语音处理</h2><p><img src="/../asset_voicellama3/03.png"></p>
<ul>
<li>知识库到向量数据库：最初，知识库中的文档通过嵌入模型进行处理。该模型将文本数据转换为数字向量，然后存储在像 Qdrant 这样的向量数据库中。这种设置通过将文档的语义含义表示为高维空间中的点，实现了高效的检索。</li>
<li>用户查询处理：当用户提交查询时，它首先与嵌入模型交互，将查询转换为其向量表示。</li>
<li>检索：然后使用查询向量从向量数据库中获取与之最相似的前 K 个向量（上下文）。这个过程被称为“检索”，有助于识别与用户查询相关的知识库中最相关的文档或数据片段。</li>
<li>阅读和响应生成：然后将检索到的上下文输入到 Meta Llama 3 LLM中，该系统会阅读和理解这些上下文中与用户查询相关的信息。然后生成一个响应，旨在提供最准确和相关的信息。然后 Whisper 将文本转换为音频响应。</li>
</ul>
<p>首先定义“Whisper”管道。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pipe = Pipeline(s2a_ref=<span class="string">&#x27;WhisperSpeech/WhisperSpeech:s2a-q4-tiny-en+pl.model&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>为windows系统，我们定义一个函数，用于将音频张量转换为音频文件。</p>
<p>然后通过我们的查询来使用 Llama 3 进行文本生成，接着我们可以使用 Whisper 进行音频生成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generate_audio_for_windows</span>(<span class="params">audio_tensor,wav_file_path</span>):</span><br><span class="line">    <span class="keyword">from</span> pydub <span class="keyword">import</span> AudioSegment</span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">    <span class="comment"># generate uses CUDA if available; therefore, it&#x27;s necessary to move to CPU before converting to NumPy array</span></span><br><span class="line">    audio_np = (audio_tensor.cpu().numpy() * <span class="number">32767</span>).astype(np.int16)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(audio_np.shape) == <span class="number">1</span>:</span><br><span class="line">        audio_np = np.expand_dims(audio_np, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        audio_np = audio_np.T</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Array shape:&quot;</span>, audio_np.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Array dtype:&quot;</span>, audio_np.dtype)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        audio_segment = AudioSegment(</span><br><span class="line">            audio_np.tobytes(), </span><br><span class="line">            frame_rate=<span class="number">24000</span>, </span><br><span class="line">            sample_width=<span class="number">2</span>, </span><br><span class="line">            channels=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">        audio_segment.export(wav_file_path, <span class="built_in">format</span>=<span class="string">&#x27;wav&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Audio file generated: <span class="subst">&#123;wav_file_path&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Error writing audio file: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">query = <span class="string">&quot;How LLMs can be used to understand and interact with the complex 3D world&quot;</span></span><br><span class="line">aud = test_rag(qa, query)</span><br><span class="line"></span><br><span class="line">audio_tensor = pipe.generate(<span class="string">f&quot;<span class="subst">&#123;aud&#125;</span>&quot;</span>)</span><br><span class="line">generate_audio_for_windows(audio_tensor, <span class="string">f&quot;output.wav&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>查询处理：我们从查询“如何LLMs用于理解和与复杂的 3D 世界互动”开始，通过使用模型（ qa ）的检索增强生成（RAG）系统进行处理。该系统的响应已准备用于语音合成。</li>
<li>语音合成：使用 whisper 模型与语音，我们将文本响应转换为音频并保存为 output.wav 。</li>
<li>语音转文本：可以使用 whisper 模型将音频文件转录回文本，以验证语音合成的准确性。</li>
</ul>
<p><img src="/../asset_voicellama3/04.png"></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://medium.com/@datadrifters/llama-3-powered-voice-assistant-integrating-local-rag-withdrant-whisper-and-langchain-b4d075b00ac5">https://medium.com/@datadrifters/llama-3-powered-voice-assistant-integrating-local-rag-withdrant-whisper-and-langchain-b4d075b00ac5</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/05/19/rpa-agent/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/05/19/rpa-agent/" class="post-title-link" itemprop="url">从 RPA 到企业 AI 代理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-05-19 16:18:05" itemprop="dateCreated datePublished" datetime="2024-05-19T16:18:05+08:00">2024-05-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-05-20 10:37:30" itemprop="dateModified" datetime="2024-05-20T10:37:30+08:00">2024-05-20</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>2.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>4 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>RPA(机器人流程自动化) 又称软件机器人，它使用自动化技术模拟人类的后台任务，如提取数据、填写表单和移动文件等等。 它结合了 API 和用户界面 (UI) 互动，整合并执行企业与生产力应用之间的重复性任务。 通过部署用于模拟人工流程的脚本，RPA 工具可以在各个不相关的软件系统中自动执行各项活动和事务。</p>
<p>AI 代理是一种由LLMs实现的新软件范式。代理可以推理、协作和行动，类似于人类的工作方式。代理不仅仅是自动化的一种逐步改进方式，而且是一种完全新的突破性技术，需要掌握新的技能和工具。它们将范围从战术任务级自动化扩展到自动化和增强复杂的知识工作。</p>
<h2 id="RPA-和人工智能"><a href="#RPA-和人工智能" class="headerlink" title="RPA 和人工智能"></a>RPA 和人工智能</h2><p>机器人流程自动化经常被误认为是人工智能 (AI)，但两者截然不同。 AI 结合了认知自动化、机器学习 (ML)、自然语言处理 (NLP) 、推理、假设生成和分析。</p>
<p>关键区别在于 RPA 是流程驱动的，AI 则是数据驱动的。 RPA 机器人只遵循最终用户定义的流程，而 AI 机器人则使用机器学习来识别数据中的模式，特别是非结构化数据，并持续进行学习。 换句话说，AI 旨在模拟人类智能，而 RPA 则仅用于复制人类指导的任务。 虽然人工智能和 RPA 工具的使用能够最大程度地减少人工干预，但它们实现流程自动化的方式是不同的。 </p>
<p>也就是说，RPA 和 AI 彼此还起到很好的互补作用。 AI 可以帮助 RPA 更全面地自动完成任务，并处理更复杂的用例。 RPA 则有助于更迅速地对 AI 洞察采取行动，而不是等待人工实施。</p>
<p>当企业需要加速将AI融入到一线活动和决策中时，许多企业发现RPA可以作为AI的“最后一英里”交付系统。配置的机器人可以将机器学习模型应用到自动化决策流程和分析中，将机器智能深入到日常操作。</p>
<p>企业自动化的演变:</p>
<ul>
<li>自动化任务 — RPA</li>
<li>自动化流程 — 自动化即代码</li>
<li>自动化工作 — 智能人工智能代理</li>
</ul>
<h2 id="超越-RPA-和智能自动化"><a href="#超越-RPA-和智能自动化" class="headerlink" title="超越 RPA 和智能自动化"></a>超越 RPA 和智能自动化</h2><p>企业自动化正在经历一场演变。它始于机器人流程自动化（RPA）, RPA 的崛起得益于录制和播放方式的 UI 自动化，它打造了围绕低代码业务的应用的繁荣。</p>
<p>如今，每家大公司都建立了围绕自动化的中心，并扩展到通常被称为智能自动化（IA）的领域。</p>
<p>IA 并不是一个明确定义的术语，通常是从 RPA 到 API 自动化以及通过 OCR 进行文档处理的技术混合体。它标志着超越点对点自动化，转向使用工具进行流程自动化，这些工具通常将编码与低代码混合，并需要专业技能来部署。然而，基本原则仍然相同 — RPA 和 IA 纯粹基于规则，并适用于明确定义、高度结构化的流程。</p>
<p>AI 代理，使用完全不同的代理规划和执行工作流程。他们可以通过灵活的推理能力来实现理解本地非结构化数据和非结构化流程。这使得代理不仅适用于无法用简单规则捕捉的工作，而且适用于用自然语言描述的手册和工作指南。这也使代理比预先编程的机器人更具弹性 — 代理可以在遇到错误时自行纠正或与人类联系以获得反馈。</p>
<p><img src="/../asset_rpaagent/01.png"></p>
<h2 id="从战术自动化到战略人工智能代理"><a href="#从战术自动化到战略人工智能代理" class="headerlink" title="从战术自动化到战略人工智能代理"></a>从战术自动化到战略人工智能代理</h2><p>尽管人工智能代理采取了一种与企业自动化非常不同的方法，但它们并不标志着 RPA 的终结。如果您的任务需要每天向生产 ERP 系统输入一千条记录，或将十万份电子健康记录迁移到新系统，那么试图使用代理来解决重复的例行任务是没有意义的。</p>
<p>AI 代理扩展了企业自动化的范围，超出了以前可能的范围。由于 AI 代理是一个仍在形成中的新类别，想象代理使用案例的最简单方法是通过代理推理扩展现有的自动化例程。随着时间的推移，AI 代理将扩展到企业核心业务工作流程，这些工作流程是企业的核心。</p>
<p><img src="/../asset_rpaagent/02.png"><br>AI 代理和企业自动化用例的层次结构</p>
<p>上面的插图显示了三种不同类别的自动化使用案例。在底部，有大量不需要复杂决策但在自定义环境下运行的战术性工作。也就是说，这项工作是在与企业高度相关的数据、文件和系统上运行的。</p>
<p>这是 RPA 运作的地方，也是开始实践第一个 AI 代理项目的好地方。寻找在 RPA 例行程序之前和之后出现的工作流中的机会，并通过代理扩大自动化范围。使用 AI 代理来扩大自动化的范围，超越标准 RPA 所能实现的范围。</p>
<p>往上一层，我们有涉及标准决策的工作，这些决策是在标准背景下进行的。这通常是由 ServiceNow 或 Salesforce 等平台和记录系统来做的工作。在这里，您最好通过从这些供应商购买人工智能和自动化解决方案，并依赖于他们创新数据和流程的能力来服务企业。现在每个应用程序都有一个协助机器人，它帮助企业自动化其业务流程。</p>
<p>最后，在金字塔的顶端，我们有最战略性的工作类别，这是企业业务和运营的核心。这项工作涉及复杂的决策，并且在特定的背景下运作。没有人能够从外部进来教你如何更好地做这项工作，因为这就是公司存在的原因。这就是专为企业定制的人工智能代理将产生最大影响的地方。</p>
<p>代理可以与人类一起工作，或完全取代一些人类任务，并使人们专注于更有生产力的工作。由于智能 AI 代理仍处于早期阶段，所以要达到这项最有价值和战略性的工作需要时间，但现在是开始建立所需技能和平台的时候了。</p>
<h2 id="案例：-ERP-低代码平台-RPA-AI"><a href="#案例：-ERP-低代码平台-RPA-AI" class="headerlink" title="案例： ERP+低代码平台+RPA+AI"></a>案例： ERP+低代码平台+RPA+AI</h2><p><img src="/../asset_rpaagent/03.png"></p>
<p>在此架构中，ERP系统可以继续保持不变，低代码平台则通过无代码或低代码快速开发，快速部署上线的特性，以小步快跑，快速迭代的方式，实现企业创新与管理改善。</p>
<p>每个企业的采购申请可能存在比较大的差异，因此很多ERP系统的标准功能都无法满足企业的个性需求,比如定制审批流程.</p>
<h3 id="识别需要与ERP系统交互的数据"><a href="#识别需要与ERP系统交互的数据" class="headerlink" title="识别需要与ERP系统交互的数据:"></a>识别需要与ERP系统交互的数据:</h3><ul>
<li>一是物料数据从ERP同步到低代码平台，以便在低代码平台发起申请时用户可以选择采购物料。</li>
<li>二是将低代码平台审批通过的采购申请返回ERP系统形成PO（采购订单）</li>
</ul>
<h3 id="正常方式"><a href="#正常方式" class="headerlink" title="正常方式:"></a>正常方式:</h3><p>通过低代码平台的JDBC Binder做数据库层面的集成或者通过ERP系统的Web Service接口完成上述的数据交互</p>
<h3 id="另类方式"><a href="#另类方式" class="headerlink" title="另类方式:"></a>另类方式:</h3><p>用RPA完成低代码平台与ERP系统的数据交互. </p>
<ul>
<li>让RPA定时在ERP系统中查询新增的物料和更新的物料，然后将这些值更新到低代码平台中。</li>
<li>低代码平台中采购审批通过的数据，通过低代码平台流程工具触发RPA机器人，在SAP系统中创建新的PO（采购订单）。</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/05/13/gymnasium-introduce/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/05/13/gymnasium-introduce/" class="post-title-link" itemprop="url">Gymnasium 介绍</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-05-13 15:12:51 / 修改时间：15:48:36" itemprop="dateCreated datePublished" datetime="2024-05-13T15:12:51+08:00">2024-05-13</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.8k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>11 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>前面介绍了强化学习的例子，觉得有必要介绍gymnasium环境。 作为强化学习最常用的工具，gym一直在不停地升级和折腾，比较大的变化就是2021年接口从gym库变成了gymnasium库。</p>
<h3 id="step和观察结果"><a href="#step和观察结果" class="headerlink" title="step和观察结果"></a>step和观察结果</h3><p>总体来说，对于gymnasium我们只需要做两件事情：一个是初始化环境，另一个就是通过step函数不停地给环境做输入，然后观察对应的结果。</p>
<p>初始化环境分为两步。<br>第一步是创建gymnasium中所支持的环境，比如我们使用经典的让一个杆子不倒的CartPole环境：</p>
<pre><code>import gymnasium as gym
env = gym.make(&quot;CartPole-v1&quot;)
</code></pre>
<p>第二步，我们就可以通过env的reset函数来进行环境的初始化：</p>
<pre><code>observation, info = env.reset()
</code></pre>
<p>我们可以将observation打印出来，它一个4元组，4个数值分别表示：</p>
<ul>
<li>小车位置</li>
<li>小车速度</li>
<li>棍的倾斜角度</li>
<li>棍的角速度</li>
</ul>
<p>如果角度大于12度，或者小车位置超出了2.4，就意味着失败了，直接结束。</p>
<p>小车的输入就是一个力，要么是向左的力，要么是向右的力。0是向左推小车，1是向右推小车。</p>
<p>下面我们让代码跑起来。</p>
<p>首先我们通过pip来安装gymnasium的包：</p>
<pre><code>pip install gymnasium
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gymnasium <span class="keyword">as</span> gym</span><br><span class="line"></span><br><span class="line">num_eval_episodes = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&quot;CartPole-v1&quot;</span>, render_mode=<span class="string">&quot;rgb_array&quot;</span>)  <span class="comment"># replace with your environment</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(env.action_space)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> episode_num <span class="keyword">in</span> <span class="built_in">range</span>(num_eval_episodes):</span><br><span class="line">    obs, info = env.reset()</span><br><span class="line"></span><br><span class="line">    episode_over = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> episode_over:</span><br><span class="line">        action = env.action_space.sample()  <span class="comment"># replace with actual agent</span></span><br><span class="line">        obs, reward, terminated, truncated, info = env.step(action)</span><br><span class="line"></span><br><span class="line">        episode_over = terminated <span class="keyword">or</span> truncated</span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure>

<p>env.action_space输出是Discrete(2)。也就是两个离散的值0和1。前面我们介绍了，这分别代表向左和向右推动小车。</p>
<p>obs输出的4元组，我们前面也讲过了，像这样：<br>[ 0.0273956 -0.00611216 0.03585979 0.0197368 ]</p>
<p>下面就是关键的step一步：</p>
<pre><code>action = env.action_space.sample()  # replace with actual agent
obs, reward, terminated, truncated, info = env.step(action)
</code></pre>
<p>CartPole的输入只有0和1两个值。我们采用sample()函数只是随机采用左右动的方式来试图让小车不倒。</p>
<p>返回的5元组，obs就是位置4元组，reward是用于强化学习的奖励，在本例中只要是不死就是1. terminated就是是否游戏结束了。<br>Truncated在官方定义中用于处理比如超时等特殊结束的情况。<br>truncated, info对于CartPole来说没有用到。</p>
<p>搭建好了gymnasium环境之后，我们就可以进行策略的升级与迭代了。<br>比如我们写死一个策略，如果位置小于0则向右推，反之则向左推：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">action_pos</span>(<span class="params">obs</span>): </span><br><span class="line">    pos, v, ang, va = obs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> pos &lt;= <span class="number">0</span>: </span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span> </span><br></pre></td></tr></table></figure>
<p>用 action_pos(obs) 替换 env.action_space.sample() 即可。</p>
<h3 id="与老gym的主要区别"><a href="#与老gym的主要区别" class="headerlink" title="与老gym的主要区别"></a>与老gym的主要区别</h3><p>目前版本与之前gym的最主要区别在于step返回值从原来的4元组变成了5元组。<br>原来是observation, reward, done, info，而现在done变成了 terminated增加了truncated。</p>
<p>老版本的：</p>
<pre><code>status, reward, done, info = env.step(0)
</code></pre>
<p>新版的：</p>
<pre><code>observation, reward, terminated, truncated, info = env.step(0)
</code></pre>
<p>因而，原来处理done的地方需要改成terminated或truncated:</p>
<pre><code>if terminated or truncated:
    something()
</code></pre>
<p>另外，env.reset函数目前返回的是两个值，而不是原来的一个值：</p>
<pre><code>obs,info = env.reset()
</code></pre>
<h3 id="gymnasium与强化学习算法库的结合"><a href="#gymnasium与强化学习算法库的结合" class="headerlink" title="gymnasium与强化学习算法库的结合"></a>gymnasium与强化学习算法库的结合</h3><p>stable-baselines3等强化学习库已经对gymnasium进行了支持，所以我们可以在stable-baselines3中直接使用gymnasium的环境。</p>
<p>先安装库：</p>
<pre><code>pip install gymnasium[atari]
pip install gymnasium[accept-rom-license]
pip install stable_baselines3
</code></pre>
<p>我们用DQN算法来训练乒乓球游戏：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gymnasium <span class="keyword">as</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> stable_baselines3 <span class="keyword">import</span> DQN</span><br><span class="line"><span class="keyword">from</span> stable_baselines3.dqn <span class="keyword">import</span> CnnPolicy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">game = <span class="string">&#x27;ALE/Pong-v5&#x27;</span></span><br><span class="line"></span><br><span class="line">env = gym.make(game,render_mode=<span class="string">&quot;rgb_array&quot;</span>)</span><br><span class="line"></span><br><span class="line">save_file = <span class="string">&#x27;dqn_&#x27;</span>+game;</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(env.action_space)</span><br><span class="line">model = DQN(CnnPolicy, env, verbose=<span class="number">1</span>,exploration_final_eps=<span class="number">0.01</span>,exploration_fraction=<span class="number">0.1</span>,gradient_steps=<span class="number">1</span>,learning_rate=<span class="number">0.0001</span>,buffer_size=<span class="number">10000</span>)</span><br><span class="line">model.set_env(env)</span><br><span class="line">model.learn(total_timesteps=<span class="number">1000000</span>, log_interval=<span class="number">10</span>)</span><br><span class="line">model.save(save_file)</span><br><span class="line"></span><br><span class="line">obs,info = env.reset()</span><br><span class="line"></span><br><span class="line">score = <span class="number">0</span></span><br><span class="line">rewards_sum = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    action, _states = model.predict(obs, deterministic=<span class="literal">True</span>)</span><br><span class="line">    obs, reward, terminated, truncated, info = env.step(action)</span><br><span class="line">    score = score + <span class="number">1</span></span><br><span class="line">    rewards_sum += reward</span><br><span class="line">    <span class="keyword">if</span> reward &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;win!!!&#x27;</span>, reward)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> terminated <span class="keyword">or</span> truncated:</span><br><span class="line">        <span class="comment"># obs = env.reset()</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;finished&#x27;</span>, score)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;reward sum=&#x27;</span>, rewards_sum)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p>上面的代码我们还可以做两处改进：</p>
<p>如果存在save_file，我们可以直接加载模型，在原有模型上继续训练。<br>我们可以增加一个测试模式，观看训练后模型打游戏的真实效果。</p>
<p>我们让PPO算法来玩乒乓球游戏了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gymnasium <span class="keyword">as</span> gym</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> stable_baselines3 <span class="keyword">import</span> PPO</span><br><span class="line"><span class="keyword">from</span> stable_baselines3.dqn <span class="keyword">import</span> MlpPolicy</span><br><span class="line"><span class="keyword">from</span> stable_baselines3.dqn <span class="keyword">import</span> CnnPolicy</span><br><span class="line"></span><br><span class="line">game = <span class="string">&#x27;ALE/Pong-v5&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#eval = True</span></span><br><span class="line"><span class="built_in">eval</span> = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">cont = <span class="literal">True</span></span><br><span class="line"><span class="comment">#cont = False</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span> (time.strftime(<span class="string">&quot;%Y-%m-%d %H:%M:%S&quot;</span>, time.localtime()))</span><br><span class="line"></span><br><span class="line">start_time = time.time()</span><br><span class="line">start_date = datetime.now()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">eval</span>:</span><br><span class="line">    env = gym.make(game,render_mode=<span class="string">&quot;human&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    env = gym.make(game,render_mode=<span class="string">&quot;rgb_array&quot;</span>)</span><br><span class="line"></span><br><span class="line">save_file = <span class="string">&#x27;./dqn_&#x27;</span>+game;</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(env.action_space)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">eval</span>:</span><br><span class="line">    model = PPO.load(save_file)</span><br><span class="line">    model.set_env(env) </span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">if</span> cont:</span><br><span class="line">        model = PPO.load(save_file)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model = PPO(<span class="string">&quot;MlpPolicy&quot;</span>, env, verbose=<span class="number">1</span>,learning_rate=<span class="number">2.5e-4</span>,clip_range=<span class="number">0.1</span>,vf_coef=<span class="number">0.5</span>,ent_coef=<span class="number">0.01</span>,n_steps=<span class="number">128</span>)    </span><br><span class="line">    model.set_env(env)</span><br><span class="line">    model.learn(total_timesteps=<span class="number">1000000</span>, log_interval=<span class="number">10</span>)</span><br><span class="line">    model.save(save_file)</span><br><span class="line"></span><br><span class="line">obs,info = env.reset()</span><br><span class="line"></span><br><span class="line">score = <span class="number">0</span></span><br><span class="line">rewards_sum = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    action, _states = model.predict(obs)</span><br><span class="line">    obs, reward, terminated, truncated, info = env.step(action)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">eval</span>:</span><br><span class="line">        env.render()</span><br><span class="line"></span><br><span class="line">    score = score + <span class="number">1</span></span><br><span class="line">    rewards_sum += reward</span><br><span class="line">    <span class="keyword">if</span> reward &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;win!!!&#x27;</span>, reward)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> terminated <span class="keyword">or</span> truncated:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;finished&#x27;</span>, score)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;reward sum=&#x27;</span>, rewards_sum)</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">duration = time.time() - start_time</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;duration=&#x27;</span>, duration)</span><br><span class="line"></span><br><span class="line">time_cost = datetime.now() - start_date</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;time cost=&#x27;</span>, time_cost)</span><br></pre></td></tr></table></figure>
<h3 id="视频输出-从Monitor到RecordVideo"><a href="#视频输出-从Monitor到RecordVideo" class="headerlink" title="视频输出 - 从Monitor到RecordVideo"></a>视频输出 - 从Monitor到RecordVideo</h3><p>有时候我们希望把游戏的视频输出出来，gym曾经使用Monitor来实现。现在gymnasium则改用RecordVideo来实现。</p>
<p>使用RecordVideo需要先安装moviepy库：</p>
<pre><code>pip install moviepy
</code></pre>
<p>然后从gymnasium.wrappers包中引用RecordVideo：</p>
<pre><code>from gymnasium.wrappers import RecordVideo
</code></pre>
<p>human模式是没有办法输出视频的，所以我们需要把human模式改成rgb_array模式。</p>
<pre><code>env = gym.make(&quot;CartPole-v1&quot;, render_mode=&quot;rgb_array&quot;)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> gymnasium <span class="keyword">as</span> gym</span><br><span class="line"><span class="keyword">from</span> gymnasium.wrappers <span class="keyword">import</span> RecordEpisodeStatistics, RecordVideo</span><br><span class="line"></span><br><span class="line">training_period = <span class="number">250</span>  <span class="comment"># record the agent&#x27;s episode every 250</span></span><br><span class="line">num_training_episodes = <span class="number">10_000</span>  <span class="comment"># total number of training episodes</span></span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">&quot;CartPole-v1&quot;</span>, render_mode=<span class="string">&quot;rgb_array&quot;</span>)  <span class="comment"># replace with your environment</span></span><br><span class="line">env = RecordVideo(env, video_folder=<span class="string">&quot;cartpole-agent&quot;</span>, name_prefix=<span class="string">&quot;training&quot;</span>,</span><br><span class="line">                  episode_trigger=<span class="keyword">lambda</span> x: x % training_period == <span class="number">0</span>)</span><br><span class="line">env = RecordEpisodeStatistics(env)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> episode_num <span class="keyword">in</span> <span class="built_in">range</span>(num_training_episodes):</span><br><span class="line">    obs, info = env.reset()</span><br><span class="line"></span><br><span class="line">    episode_over = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> episode_over:</span><br><span class="line">        action = env.action_space.sample()  <span class="comment"># replace with actual agent</span></span><br><span class="line">        obs, reward, terminated, truncated, info = env.step(action)</span><br><span class="line"></span><br><span class="line">        episode_over = terminated <span class="keyword">or</span> truncated</span><br><span class="line"></span><br><span class="line">    logging.info(<span class="string">f&quot;episode-<span class="subst">&#123;episode_num&#125;</span>&quot;</span>, info[<span class="string">&quot;episode&quot;</span>])</span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a target="_blank" rel="noopener" href="https://gymnasium.farama.org/">https://gymnasium.farama.org/</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/05/05/reinforce-learing-game-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/05/05/reinforce-learing-game-2/" class="post-title-link" itemprop="url">通过游戏深入了解强化学习 下篇</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-05-05 16:22:58 / 修改时间：20:33:42" itemprop="dateCreated datePublished" datetime="2024-05-05T16:22:58+08:00">2024-05-05</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>11k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>19 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>回顾一下第一部分中，强化学习中基于价值和基于策略的算法。</p>
<h3 id="基于价值的算法"><a href="#基于价值的算法" class="headerlink" title="基于价值的算法"></a>基于价值的算法</h3><p><img src="/../asset_reinforcelearninggame2/01.png"></p>
<p>专注于优化价值函数，该函数估计状态-动作对（Q 值）的未来奖励。这种方法包括 Q 学习等方法，其目标是通过学习每个状态下每个动作的价值来最大化预期回报。</p>
<h3 id="基于策略的算法"><a href="#基于策略的算法" class="headerlink" title="基于策略的算法"></a>基于策略的算法</h3><p><img src="/../asset_reinforcelearninggame2/02.png"></p>
<p>直接学习决定代理行为的策略，而不需要价值函数。 （通常将 θ 建模为神经网络。）</p>
<p>这些方法通过调整参数来优化策略，以最大化预期回报。示例包括 REINFORCE，其中策略朝着提高性能的方向更新。</p>
<h3 id="Advantage-Actor-Critic-A2C"><a href="#Advantage-Actor-Critic-A2C" class="headerlink" title="Advantage Actor Critic (A2C)"></a>Advantage Actor Critic (A2C)</h3><p>虽然 REINFORCE 方法具有直接更新策略而无需计算所有 Q 值的优点（如基于值的算法所要求的那样），但它引入了新问题：</p>
<ul>
<li>样本效率低下</li>
<li>梯度的高方差</li>
</ul>
<p>在 REINFORCE 中，我们使用trajectory 的返回值来引导我们的梯度。然而，在随机环境中，这些返回值可能变得非常大或非常小，导致梯度不稳定。</p>
<p><img src="/../asset_reinforcelearninggame2/04.webp"></p>
<p>为了减少REINFORCE的方差，常用的方法是用优势函数A(s,a)代替R(τ)。</p>
<p><img src="/../asset_reinforcelearninggame2/05.webp"></p>
<p>优势函数是通过从 V 函数中减去 Q 函数来计算的。它评估如果我们在给定状态下采取特定行动，与该状态下收到的平均奖励相比，我们可以获得多少额外奖励。</p>
<p>但现在问题来了，我们如何获得Q和V的值呢？</p>
<p>我们可以以 TD 误差的形式重新表述这个方程，将 Q 函数表示为即时奖励与下一个状态值的总和。</p>
<p><img src="/../asset_reinforcelearninggame2/06.webp"><br>R：立即奖励，V：状态值</p>
<p>现在，通过构建一个新的模型（critic）来预测价值函数，我们可以轻松获得所采取行动的优势。</p>
<h3 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor &amp; Critic"></a>Actor &amp; Critic</h3><p><img src="/../asset_reinforcelearninggame2/07.webp"></p>
<p>A2C 算法由两个模型组成：Actor 和 Critic。 Actor 负责确定动作 π(a|s)，而 Critic 则评估状态值 V(s)。</p>
<ul>
<li>Critic：通过评估状态值来计算优势 ( &#x3D; Return — V(s_t))。</li>
<li>Actor：功能类似于REINFORCE算法中的代理，负责根据提供的状态预测动作。</li>
</ul>
<p>尽管 Actor 和 Critic 是两个不同的模型，但在实践中，它们通常共享相同的网络架构，只有最后一层产生不同的输出</p>
<h3 id="A2C优化"><a href="#A2C优化" class="headerlink" title="A2C优化"></a>A2C优化</h3><p><img src="/../asset_reinforcelearninggame2/08.webp"></p>
<ul>
<li>N：未来奖励的步数</li>
<li>Rt：&#x3D; rt + γ<em>rt_1 + … + γ^(N-1)</em> rt_N-1 + Vω(s_t+N)</li>
<li>θ: actor</li>
<li>ω: critic</li>
</ul>
<p>对于 Actor：我们根据损失函数 J &#x3D; logπ(a|s) * A(s, a) 更新网络，实现与 REINFORCE 相同。</p>
<p><img src="/../asset_reinforcelearninggame2/09.webp"></p>
<p>对于 Critic：我们可以在 Rt 和 Vω(St) 之间应用 MSE 来改进我们的 Critic。</p>
<p><img src="/../asset_reinforcelearninggame2/10.webp"></p>
<p>函数Vω(s)表示从状态s开始的期望回报。由于回报 Rt​提供了对处于该状态的好坏程度的直接衡量，因此critic的任务是使用价值函数 Vω(s) 尽可能接近地预测这些回报。</p>
<h3 id="A2C实施"><a href="#A2C实施" class="headerlink" title="A2C实施"></a>A2C实施</h3><p>在我们的第一个实现中，我们将玩车杆游戏，这是一个旨在解决车杆问题的基本环境。您还可以尝试其他一些经典的控制环境，例如 Pendulum 和 Acrobot。</p>
<p><img src="/../asset_reinforcelearninggame2/11.webp"></p>
<p><img src="/../asset_reinforcelearninggame2/12.webp"></p>
<p>在开始之前一定要先通读<a target="_blank" rel="noopener" href="https://gymnasium.farama.org/environments/classic_control/cart_pole/">文档</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> gymnasium <span class="keyword">as</span> gym</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> ema_pytorch <span class="keyword">import</span> EMA</span><br><span class="line"><span class="keyword">from</span> torch.distributions.categorical <span class="keyword">import</span> Categorical</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_env</span>(<span class="params">env_id</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapped_env</span>():</span><br><span class="line">        env = gym.make(env_id, render_mode=<span class="string">&#x27;rgb_array&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> env</span><br><span class="line">    <span class="keyword">return</span> wrapped_env</span><br><span class="line"></span><br><span class="line">env_id = <span class="string">&#x27;CartPole-v1&#x27;</span></span><br><span class="line">env = make_env(env_id)()</span><br></pre></td></tr></table></figure>

<p><img src="/../asset_reinforcelearninggame2/13.webp"><br>车杆的动作和观察空间</p>
<p>动作空间很简单，在每个时间步，我们可以选择将小车推到左边或右边。同时，观察空间包含四个连续值，分别代表小车位置、小车速度、极角和极角速度。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Actor</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, obs_dim, n_actions</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Linear(obs_dim, <span class="number">64</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">            nn.Tanh(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, n_actions)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        logits = self.model(x)</span><br><span class="line">        <span class="keyword">return</span> Categorical(logits = logits)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Critic</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, obs_dim</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.model = nn.Sequential(</span><br><span class="line">            nn.Linear(obs_dim, <span class="number">64</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">64</span>, <span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.model(x)</span><br></pre></td></tr></table></figure>
<p>在A2C中，我们将有两个模型，Actor和Critic。 Actor 输出概率分布来确定采取哪个操作，而 Critic 则提供状态值。</p>
<p>在大多数情况下，Actor 和 Critic 将集成到一个模型中</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">gamma = <span class="number">0.99</span></span><br><span class="line">episodes = <span class="number">200</span></span><br><span class="line">env = make_env(env_id)()</span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line"></span><br><span class="line">actor = Actor(<span class="number">4</span>, <span class="number">2</span>).to(device)</span><br><span class="line">critic = Critic(obs_shape[<span class="number">0</span>]).to(device)</span><br><span class="line">actor_optim = torch.optim.Adam(actor.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line">critic_optim = torch.optim.Adam(critic.parameters(), lr=<span class="number">1e-3</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> episode <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">1</span>, episodes + <span class="number">1</span>), desc=<span class="string">&quot;A2C training progress: ~~&quot;</span>):          </span><br><span class="line">    done = <span class="literal">False</span></span><br><span class="line">    obs, _ = env.reset()</span><br><span class="line">    log_probs = []</span><br><span class="line">    values = []</span><br><span class="line">    rewards = []</span><br><span class="line">    frame = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">        obs = torch.tensor(obs).to(device)</span><br><span class="line">        dist = actor(obs)</span><br><span class="line">        value = critic(obs)</span><br><span class="line">        action = dist.sample()</span><br><span class="line">        log_prob = dist.log_prob(action)</span><br><span class="line">        next_obs, reward, done, _, info = env.step(action.detach().cpu().numpy())</span><br><span class="line">      </span><br><span class="line">        log_probs.append(log_prob)</span><br><span class="line">        values.append(value)</span><br><span class="line">        rewards.append(reward)</span><br><span class="line">        obs = next_obs</span><br><span class="line">        </span><br><span class="line">        frame += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> frame &gt;= <span class="number">300</span>:</span><br><span class="line">            <span class="comment"># Give some constraints to the length of the episode</span></span><br><span class="line">            <span class="comment"># otherwise, it will become too large.</span></span><br><span class="line">            <span class="keyword">break</span>   </span><br><span class="line">    R = <span class="number">0</span></span><br><span class="line">    returns = []</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> rewards[::-<span class="number">1</span>]:</span><br><span class="line">        R = r + gamma * R</span><br><span class="line">        returns.insert(<span class="number">0</span>, R)</span><br><span class="line">        </span><br><span class="line">    returns = torch.tensor(returns).to(device)</span><br><span class="line">    values = torch.stack(values).to(device)</span><br><span class="line">    advantage = returns - values</span><br><span class="line">    </span><br><span class="line">    critic_loss = advantage.<span class="built_in">pow</span>(<span class="number">2</span>).mean()</span><br><span class="line">    critic_optim.zero_grad()</span><br><span class="line">    critic_loss.backward()</span><br><span class="line">    critic_optim.step() <span class="comment"># update critic</span></span><br><span class="line">    </span><br><span class="line">    actor_loss = (-torch.stack(log_probs)*advantage.detach()).mean()</span><br><span class="line">    actor_optim.zero_grad()</span><br><span class="line">    actor_loss.backward()</span><br><span class="line">    actor_optim.step() <span class="comment"># update actor</span></span><br></pre></td></tr></table></figure>

<p>训练过程与REINFORCE算法基本相同；唯一的区别是对数概率现在乘以优势而不是回报。</p>
<p><img src="/../asset_reinforcelearninggame2/14.webp"></p>
<h3 id="车杆结果"><a href="#车杆结果" class="headerlink" title="车杆结果"></a>车杆结果</h3><p><img src="/../asset_reinforcelearninggame2/14.gif"></p>
<h3 id="在线策略-离线策略模型"><a href="#在线策略-离线策略模型" class="headerlink" title="在线策略 &amp; 离线策略模型"></a>在线策略 &amp; 离线策略模型</h3><p><img src="/../asset_reinforcelearninggame2/15.webp"></p>
<h4 id="在线策略模型：（例如-REINFORCE，AC2）"><a href="#在线策略模型：（例如-REINFORCE，AC2）" class="headerlink" title="在线策略模型：（例如 REINFORCE，AC2）"></a>在线策略模型：（例如 REINFORCE，AC2）</h4><ul>
<li>基于当前策略，同策略模型通过与环境交互来收集数据。每次更新都需要由更新的策略生成的新数据。</li>
<li>根据收集的数据执行更新。该模型直接从其自身策略生成的数据中学习。</li>
</ul>
<h4 id="离线策略模型：（例如-DQN）"><a href="#离线策略模型：（例如-DQN）" class="headerlink" title="离线策略模型：（例如 DQN）"></a>离线策略模型：（例如 DQN）</h4><ul>
<li><p>离线策略模型可以从任何来源收集数据，包括过去的经验或不同的策略。</p>
</li>
<li><p>这些模型从收集到的数据中学习，无论这些数据是由哪种策略产生，或者源自何处。</p>
</li>
</ul>
<h3 id="近端策略优化-PPO"><a href="#近端策略优化-PPO" class="headerlink" title="近端策略优化 (PPO)"></a>近端策略优化 (PPO)</h3><p>在A2C部分，我们通过从V函数中减去Q函数来解决梯度高方差的问题。然而，另一个有待解决的挑战是样本效率低下。</p>
<p>REINFORCE 和 AC2 等同策略模型中的样本效率低下源于需要根据当前策略 π_θ 收集数据。更新模型参数 θ 会导致策略发生变化，这需要收集新的trajectories。</p>
<p>在之前的实验中，trajectories的最大长度为300。这意味着每次更新参数时，我们都会利用300个数据点，这不是一种有效的数据使用方式。</p>
<p>PPO 修改了严格的在线策略规定，以允许在某些条件下使用离线策略数据。通过将重要性采样应用于我们的策略梯度，我们可以获得以下结果：<br><img src="/../asset_reinforcelearninggame2/16.webp"></p>
<p><img src="/../asset_reinforcelearninggame2/17.webp"></p>
<p>我们可以看到数据不再从当前策略 π_θ 收集，而是从表示为 π_bar 的不同策略收集。这种调整允许我们利用从不同策略采样的数据对原始策略 π_θ 执行多次更新。</p>
<h3 id="Clipped-Surrogate-Objective-Function-（截断替代目标函数）"><a href="#Clipped-Surrogate-Objective-Function-（截断替代目标函数）" class="headerlink" title="Clipped Surrogate Objective Function （截断替代目标函数）"></a>Clipped Surrogate Objective Function （截断替代目标函数）</h3><p><img src="/../asset_reinforcelearninggame2/18.webp"></p>
<p>对于大多数优化方法来说，使用较小的梯度来更新模型的权重通常会带来更好、更稳定的结果。</p>
<p>然而，在策略梯度方法中，梯度log(π_θ)可以很大也可以很小；因此，为了使训练更加稳定，必须限制梯度。</p>
<p>PPO 通过裁剪 [1−ε, 1+ε] 范围之外的梯度来实现这一点。我们将在下表中进一步讨论这个问题~~</p>
<p><img src="/../asset_reinforcelearninggame2/19.webp"><br><img src="/../asset_reinforcelearninggame2/20.webp"></p>
<h4 id="Case-1-2"><a href="#Case-1-2" class="headerlink" title="Case 1, 2:"></a>Case 1, 2:</h4><p>比率 pt 在 [1−ε, 1+ε] 范围内，因此我们对梯度不做任何操作并返回 pt​(θ)At​。</p>
<h4 id="Case-3，6"><a href="#Case-3，6" class="headerlink" title="Case 3，6:"></a>Case 3，6:</h4><p>由于 p𝑡 &lt; 1−𝜖（情况 3），这表明新策略𝜋𝜃​采取行动𝑎_𝑡的概率小于旧策略，并且考虑到优势是正的，我们希望增加 πθ​的值。因此，在这种情况下不需要剪裁。</p>
<h4 id="Case-4，5"><a href="#Case-4，5" class="headerlink" title="Case 4，5:"></a>Case 4，5:</h4><p>以情况 4 为例，我们可以看到概率 πθ​ 小于 𝜋old​（因为 p𝑡&lt;1−𝜖），并且考虑到优势为负，这将倾向于进一步降低 𝜋𝜃 ，这可能会使训练不稳定。因此，我们将梯度设置为 0 以确保策略不会发生变化。</p>
<p>这就是PPO的全部内容。</p>
<h3 id="赛车实施"><a href="#赛车实施" class="headerlink" title="赛车实施"></a>赛车实施</h3><p>完整的实现可以在这里找到：<a target="_blank" rel="noopener" href="https://github.com/Yukino1010/ReinforcementModels.git">链接</a></p>
<p><img src="/../asset_reinforcelearninggame2/21.webp"></p>
<p>在我们的第二个实现中，我们将使用 Box2D 环境来玩赛车游戏。规则很简单：每次访问一个轨道块我们都会得到1000分；然而，每帧我们都会损失 0.1 分。例如，如果您完成了 732 帧，您的奖励将为 1000–0.1*732 &#x3D; 926.8 积分。 （更多详情请参考这里：<a target="_blank" rel="noopener" href="https://arc.net/l/quote/jbfidurc">链接</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#import ...</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_env</span>(<span class="params">env_id</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapped_env</span>():</span><br><span class="line">        env = gym.make(env_id, render_mode=<span class="string">&#x27;rgb_array&#x27;</span>, continuous=<span class="literal">False</span>)</span><br><span class="line">        env = MaxAndSkipEnv(env, skip=<span class="number">4</span>)</span><br><span class="line">        env = ResizeObservation(env, <span class="number">84</span>)</span><br><span class="line">        env = GrayScaleObservation(env)</span><br><span class="line">        env = FrameStack(env, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">return</span> env</span><br><span class="line">    <span class="keyword">return</span> wrapped_env</span><br><span class="line"></span><br><span class="line">env_id = <span class="string">&quot;CarRacing-v2&quot;</span></span><br><span class="line">num_envs = <span class="number">4</span> <span class="comment"># use four parallel env to collect data</span></span><br><span class="line">envs = DummyVecEnv([make_env(env_id) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_envs)])</span><br></pre></td></tr></table></figure>

<p>在赛车游戏中，您可以选择不同类型的动作空间，离散的或连续的。在本实验中，我们将使用离散动作空间，其中包含五个不同的动作。</p>
<ul>
<li>0：什么也不做</li>
<li>1：左转</li>
<li>2：右转</li>
<li>3：加油门</li>
<li>4：刹车</li>
</ul>
<p>观察空间是96x96 RGB图像。这次，我将使用一些<a target="_blank" rel="noopener" href="https://gymnasium.farama.org/api/wrappers/observation_wrappers/">ENV wrappers</a>将其转换为84x84的灰度图像，这更适合模型训练。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ActorCritic</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, num_actions</span>):</span><br><span class="line">        <span class="built_in">super</span>(ActorCritic, self).__init__()</span><br><span class="line">        <span class="comment"># Common layers</span></span><br><span class="line">        self.conv1 = nn.Conv2d(num_inputs, <span class="number">32</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">32</span>, <span class="number">64</span>, kernel_size=<span class="number">4</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">64</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv4 = nn.Conv2d(<span class="number">64</span>, <span class="number">128</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">128</span> * <span class="number">5</span> * <span class="number">5</span>, <span class="number">512</span>)</span><br><span class="line">        self.actor_fc = nn.Linear(<span class="number">512</span>, num_actions)</span><br><span class="line">        self.critic_fc = nn.Linear(<span class="number">512</span>, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = F.relu(self.conv2(x))</span><br><span class="line">        x = F.relu(self.conv3(x))</span><br><span class="line">        x = F.relu(self.conv4(x))</span><br><span class="line">        </span><br><span class="line">        x = x.view(-<span class="number">1</span>, <span class="number">128</span> * <span class="number">5</span> * <span class="number">5</span>)  </span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        logits = self.actor_fc(x) <span class="comment"># Actor</span></span><br><span class="line">        state_values = self.critic_fc(x) <span class="comment"># Critic</span></span><br><span class="line">        <span class="keyword">return</span> Categorical(logits=logits), state_values</span><br></pre></td></tr></table></figure>

<p>由于观察的是图像，我们将使用基于 CNN 的架构来构建我们的代理。它将输出一个 Categorical 对象和当前状态的值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">gamma = <span class="number">0.99</span></span><br><span class="line">gae_lambda = <span class="number">0.95</span></span><br><span class="line">clip_coef = <span class="number">0.2</span></span><br><span class="line">ent_coef = <span class="number">0.01</span></span><br><span class="line">vf_coef = <span class="number">1</span> </span><br><span class="line"></span><br><span class="line">num_envs = <span class="number">4</span></span><br><span class="line">num_steps = <span class="number">512</span></span><br><span class="line">minibatch_size = <span class="number">32</span></span><br><span class="line">total_steps = <span class="number">500000</span></span><br><span class="line">batch_size = num_envs * num_steps</span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">agent = ActorCritic(<span class="number">4</span>, n_action).to(device) <span class="comment"># n_action = 5</span></span><br><span class="line">ema = EMA(agent, beta = <span class="number">0.995</span>, update_every = <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="number">1</span>, num_updates + <span class="number">1</span>), desc=<span class="string">&quot;PPO training progress: ~~&quot;</span>):</span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(num_steps):</span><br><span class="line">        global_step += <span class="number">1</span> * num_envs</span><br><span class="line">        obs[step] = next_obs</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            dist, value = agent(next_obs)</span><br><span class="line">            values[step] = value.view(-<span class="number">1</span>)</span><br><span class="line">            action = dist.sample()</span><br><span class="line">            logprob = dist.log_prob(action)</span><br><span class="line"></span><br><span class="line">        actions[step] = action</span><br><span class="line">        logprobs[step] = logprob</span><br><span class="line">        next_obs, reward, done, info = envs.step(action.cpu().numpy())</span><br><span class="line">        dones[step] = torch.Tensor(done).to(device)</span><br><span class="line">        rewards[step] = torch.tensor(reward).to(device).view(-<span class="number">1</span>)</span><br><span class="line">        next_obs = torch.Tensor(next_obs).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        _, next_value = agent(next_obs)</span><br><span class="line">        advantages = torch.zeros_like(rewards).to(device)</span><br><span class="line">        lastgaelam = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(num_steps)):</span><br><span class="line">            <span class="keyword">if</span> t == num_steps - <span class="number">1</span>:</span><br><span class="line">                is_done = <span class="number">1.0</span> - dones[t]</span><br><span class="line">                nextvalues = next_value</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                is_done = <span class="number">1.0</span> - dones[t]</span><br><span class="line">                nextvalues = values[t+<span class="number">1</span>]</span><br><span class="line">            delta = rewards[t] + gamma * nextvalues * is_done - values[t]</span><br><span class="line">            advantages[t] = lastgaelam = delta + gamma * gae_lambda * is_done * lastgaelam</span><br><span class="line">        returns = advantages + values</span><br><span class="line"></span><br><span class="line">    b_obs = obs.view((-<span class="number">1</span>,) + obs_shape)</span><br><span class="line">    b_actions = actions.view((-<span class="number">1</span>,) + action_shape)</span><br><span class="line">    b_logprobs = logprobs.view(-<span class="number">1</span>)</span><br><span class="line">    b_advantages = advantages.view(-<span class="number">1</span>)</span><br><span class="line">    b_returns = returns.view(-<span class="number">1</span>)</span><br><span class="line">    b_inds = np.arange(batch_size)</span><br><span class="line">   </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">        np.random.shuffle(b_inds)</span><br><span class="line">        <span class="keyword">for</span> start <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, batch_size, minibatch_size):</span><br><span class="line">            end = start + minibatch_size</span><br><span class="line">            mb_inds = b_inds[start:end]</span><br><span class="line">            dist, new_value = agent(b_obs[mb_inds])</span><br><span class="line">            newlogprob = dist.log_prob(b_actions.long()[mb_inds])</span><br><span class="line">            entropy = dist.entropy()</span><br><span class="line">            logratio = newlogprob - b_logprobs[mb_inds]</span><br><span class="line">            ratio = logratio.exp()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Policy loss</span></span><br><span class="line">            mb_advantages = b_advantages[mb_inds]</span><br><span class="line">            pg_loss1 = -mb_advantages * ratio</span><br><span class="line">            pg_loss2 = -mb_advantages * torch.clamp(ratio, <span class="number">1</span> - clip_coef, <span class="number">1</span> + clip_coef)</span><br><span class="line">            pg_loss = torch.<span class="built_in">max</span>(pg_loss1, pg_loss2).mean()</span><br><span class="line">            <span class="comment"># Value loss</span></span><br><span class="line">            newvalue = newvalue.view(-<span class="number">1</span>)</span><br><span class="line">            v_loss = v_loss_fn(newvalue, b_returns[mb_inds])</span><br><span class="line">            entropy_loss = entropy.mean()</span><br><span class="line">            loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef</span><br><span class="line"></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            nn.utils.clip_grad_norm_(agent.parameters(), <span class="number">0.5</span>)</span><br><span class="line">            optimizer.step()</span><br><span class="line">            ema.update()</span><br></pre></td></tr></table></figure>

<p>为了执行 PPO，我们首先从旧策略中收集数据（b_obs、b_actions，…）。然后我们可以比较新旧策略之间的差异（new_logprob - b_logprobs）。最后，我们可以迭代不同的批次并更新 Actor 和 Critic。</p>
<p>我所做的其他一些修改包括合并熵损失以及用 GAE（通用优势估计器）替换优势函数。</p>
<p>熵用于鼓励模型探索更多可能的动作，从而探索更多可能的轨迹。同时，GAE是优势函数的变体，旨在减少奖励的方差（详细信息可以在<a target="_blank" rel="noopener" href="https://towardsdatascience.com/generalized-advantage-estimation-in-reinforcement-learning-bf4a957f7975">这里</a>找到）</p>
<h3 id="赛车结果"><a href="#赛车结果" class="headerlink" title="赛车结果"></a>赛车结果</h3><p><img src="/../asset_reinforcelearninggame2/22.gif"></p>
<h3 id="使用第三方模块"><a href="#使用第三方模块" class="headerlink" title="使用第三方模块"></a>使用第三方模块</h3><p>其实除了我们自己建立模型之外，我们还可以使用第三方模块来帮助我们快速建立模型。在最后一节中，我将使用 Stable Baselines3 来玩 linyiLYi 创建的<a target="_blank" rel="noopener" href="https://github.com/linyiLYi/snake-ai/tree/master">迷你蛇游戏</a>。</p>
<p>环境详细信息可以在<a target="_blank" rel="noopener" href="https://github.com/linyiLYi/snake-ai/tree/master">这里</a>和<a target="_blank" rel="noopener" href="https://youtu.be/jTVMxJBtmFs?si=sdXihKfhT77oLvlK">这里</a>找到</p>
<p>由于OpenAI不再支持旧的gym环境，直接使用snake环境会导致错误。解决办法是降级Stable Baselines的版本或者将snake env转换成Gymnasium使用的格式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> snake_game_custom_wrapper_cnn <span class="keyword">import</span> SnakeEnv</span><br><span class="line"><span class="keyword">from</span> sb3_contrib <span class="keyword">import</span> MaskablePPO</span><br><span class="line"><span class="keyword">from</span> sb3_contrib.common.wrappers <span class="keyword">import</span> ActionMasker</span><br><span class="line"><span class="keyword">from</span> stable_baselines3.common.type_aliases <span class="keyword">import</span> GymEnv</span><br><span class="line"><span class="keyword">from</span> stable_baselines3.common.vec_env <span class="keyword">import</span> VecEnv, DummyVecEnv</span><br><span class="line"></span><br><span class="line"><span class="comment"># From https://github.com/linyiLYi/snake-ai/tree/master</span></span><br><span class="line">EXPECTED_METHOD_NAME = <span class="string">&#x27;get_action_mask&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_action_masks</span>(<span class="params">env: GymEnv</span>) -&gt; np.ndarray:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Checks whether gym env exposes a method returning invalid action masks</span></span><br><span class="line"><span class="string">    :param env: the Gym environment to get masks from</span></span><br><span class="line"><span class="string">    :return: A numpy array of the masks</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(env, VecEnv):</span><br><span class="line">        <span class="keyword">return</span> np.stack(env.env_method(EXPECTED_METHOD_NAME))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">getattr</span>(env, EXPECTED_METHOD_NAME)()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Linear scheduler</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear_schedule</span>(<span class="params">initial_value, final_value=<span class="number">0.0</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(initial_value, <span class="built_in">str</span>):</span><br><span class="line">        initial_value = <span class="built_in">float</span>(initial_value)</span><br><span class="line">        final_value = <span class="built_in">float</span>(final_value)</span><br><span class="line">        <span class="keyword">assert</span> (initial_value &gt; <span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">scheduler</span>(<span class="params">progress</span>):</span><br><span class="line">        <span class="keyword">return</span> final_value + progress * (initial_value - final_value)</span><br><span class="line">    <span class="keyword">return</span> scheduler</span><br><span class="line">lr_schedule = linear_schedule(<span class="number">2.5e-4</span>, <span class="number">2.5e-6</span>)</span><br><span class="line">clip_range_schedule = linear_schedule(<span class="number">0.150</span>, <span class="number">0.025</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">num_envs = <span class="number">16</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_env</span>(<span class="params">seed=<span class="number">0</span></span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_init</span>():</span><br><span class="line">        env = SnakeEnv()</span><br><span class="line">        env = ActionMasker(env, SnakeEnv.get_action_mask)</span><br><span class="line">        <span class="keyword">return</span> env</span><br><span class="line">    <span class="keyword">return</span> _init</span><br><span class="line">envs = DummyVecEnv([make_env() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_envs)])</span><br><span class="line"></span><br><span class="line">model = MaskablePPO(</span><br><span class="line">    <span class="string">&quot;CnnPolicy&quot;</span>,</span><br><span class="line">    envs,</span><br><span class="line">    device=<span class="string">&quot;cuda&quot;</span>,</span><br><span class="line">    verbose=<span class="number">1</span>,</span><br><span class="line">    n_steps=<span class="number">2048</span>,</span><br><span class="line">    batch_size=<span class="number">512</span>,</span><br><span class="line">    n_epochs=<span class="number">4</span>,</span><br><span class="line">    gamma=<span class="number">0.94</span>,</span><br><span class="line">    learning_rate=lr_schedule,</span><br><span class="line">    clip_range=clip_range_schedule,</span><br><span class="line">)</span><br><span class="line">model.learn(total_timesteps=<span class="built_in">int</span>(<span class="number">5000000</span>))</span><br><span class="line">model.save(<span class="string">&quot;./ppo_snake_final.zip&quot;</span>)</span><br><span class="line">envs.close()</span><br></pre></td></tr></table></figure>

<h3 id="贪吃蛇游戏的结果"><a href="#贪吃蛇游戏的结果" class="headerlink" title="贪吃蛇游戏的结果"></a>贪吃蛇游戏的结果</h3><p><img src="/../asset_reinforcelearninggame2/23.gif"></p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a target="_blank" rel="noopener" href="https://levelup.gitconnected.com/mastering-game-worlds-a-deep-dive-into-reinforcement-learning-part-2-574263e11bb1">https://levelup.gitconnected.com/mastering-game-worlds-a-deep-dive-into-reinforcement-learning-part-2-574263e11bb1</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/04/29/ai-agent-everything/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/04/29/ai-agent-everything/" class="post-title-link" itemprop="url">AI 代理无处不在</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-04-29 12:31:29 / 修改时间：16:15:37" itemprop="dateCreated datePublished" datetime="2024-04-29T12:31:29+08:00">2024-04-29</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5.1k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>我现在越来越觉得AI代理是非常重要的技术。 就像linux的kernel，AI代理就是LLM OS的调度核心。</p>
<p>LLMs现在已经存在了几年，它们正迅速发展成为 AI 代理和代理工作流。LLMs很棒，但它们仍然不擅长自动化处理事务。LLMs与其他工具结合起来是利用LLMs所拥有的通用智能的一种非常高效的方式，通过消耗大量的语言数据。LLMs最大的问题在于它们偶尔会迷失方向（幻觉），我们永远不知道何时LLMs或代理可能会失败。围绕这些失败已经有一些防护措施，但我们远未充分利用LLMs普通智能的全部能力。</p>
<h2 id="从-RAG-到-Agents"><a href="#从-RAG-到-Agents" class="headerlink" title="从 RAG 到 Agents"></a>从 RAG 到 Agents</h2><p>RAG 是一种半参数类型的系统，其中参数部分是大型语言模型，其余部分是非参数部分。将所有不同部分组合在一起得到半参数系统。LLMs 将所有信息存储在它们的权重或参数中（以编码形式），而系统的其余部分没有定义该知识的参数。</p>
<ul>
<li>交换索引（特定信息在LLMs中）使我们能够进行定制，这意味着我们不会遭受过时信息，也可以修改索引中的内容。</li>
<li>用这些索引来作为LLMs的基础信息，意味着我们会减少幻觉，我们可以通过指向来源进行引用和归因</li>
</ul>
<p>因此，原则上，RAG 为我们的LLMs创建更好的情境化。</p>
<p><img src="/../asset_aiagenteverything/01.png"></p>
<p>“代理”是一种自动推理和决策引擎。它接收用户输入&#x2F;查询，并可以做出内部决策来执行该查询，以返回正确的结果。关键代理组件可以包括但不限于：</p>
<ul>
<li>将复杂问题分解为更小的问题</li>
<li>选择要使用的外部工具+确定调用该工具的参数</li>
<li>规划一组任务</li>
<li>将先前完成的任务存储在内存模块中</li>
</ul>
<p><img src="/../asset_aiagenteverything/02.webp"></p>
<p>我们有不同类型的代理，从简单到极其复杂的代理。根据任务的复杂性，我们设计这些代理，使它们能够自主决定选择可用的工具。下面的图片显示了不同类型的代理及其复杂程度。</p>
<p><img src="/../asset_aiagenteverything/03.webp"></p>
<h2 id="代理系统概述"><a href="#代理系统概述" class="headerlink" title="代理系统概述"></a>代理系统概述</h2><p>在一个由LLM驱动的自主代理系统中，LLM充当代理的大脑，利用不同的组件在数字世界中执行操作。</p>
<h3 id="工具使用"><a href="#工具使用" class="headerlink" title="工具使用"></a>工具使用</h3><p>代理程序学会调用外部 API 或工具，以获取模型权重中可能缺失的额外信息&#x2F;上下文或功能（通常在预训练后难以更改）。这包括当前信息、数学引擎、代码执行能力、访问专有信息源等等。</p>
<h3 id="内存"><a href="#内存" class="headerlink" title="内存"></a>内存</h3><ul>
<li>短期记忆：在上下文学习中，可以将其视为利用模型的短期记忆来处理给定问题。上下文长度窗口可以被视为短期记忆。</li>
<li>长期记忆：为代理提供在较长时间内保留和召回（无限）信息的能力，通常通过利用外部向量存储和快速检索。RAG 中的检索部分可以被视为长期记忆。</li>
</ul>
<h3 id="计划"><a href="#计划" class="headerlink" title="计划"></a>计划</h3><ul>
<li>子目标和任务分解：代理将较大的任务分解为较小、可管理的子目标，从而有效处理复杂任务。</li>
<li>反思和完善：代理可以进行自我批评（虽然在某些方面存疑），并对过去的行动进行自我反思，从错误中吸取教训，并为未来的步骤进行完善，从而改善最终结果。</li>
</ul>
<p><img src="/../asset_aiagenteverything/04.webp"></p>
<h2 id="人工智能代理的工具使用"><a href="#人工智能代理的工具使用" class="headerlink" title="人工智能代理的工具使用"></a>人工智能代理的工具使用</h2><p>能够使用工具是人类在许多方面与其他生物有所不同的地方。我们创造、修改和利用外部物体来扩展我们的身体和认知能力。同样，为LLMs配备外部工具可以显著扩展它们的能力。</p>
<p>在 AI 代理设置中，工具对应于一组工具，使LLM代理能够与外部环境（如 Google 搜索、代码解释器、数学引擎等）进行交互。工具也可以是某种形式的数据库、知识库和外部模型。当代理与外部工具交互时，它通过工作流执行任务，帮助代理获取观察结果或完成给定子任务和最终任务所需的上下文。</p>
<p>工具如何以不同方式被LLMs利用的几个示例：</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.00445">MRKL</a>（Modular Reasoning, Knowledge, and Language）是一个框架，将LLMs与专家模块结合起来，这些模块可以是LLMs或符号化的（计算器或天气 API）。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.17580">HuggingGPT</a> — 一个由LLM驱动的代理，利用LLMs作为任务规划器，连接各种现有的 AI 模型（基于描述）来解决 AI 任务。</p>
</li>
</ul>
<p><img src="/../asset_aiagenteverything/05.webp"></p>
<p>HuggingGPT 的整个过程如上图所示，可以根据原始论文中提到的内容分为四个阶段：</p>
<p>任务规划：使用 ChatGPT 分析用户的请求以理解他们的意图，并将其拆分为可能可解决的任务。</p>
<p>模型选择：为了解决计划中的任务，ChatGPT 基于模型描述从 Hugging Face 上选择专家模型。</p>
<p>任务执行：调用并执行每个选择的模型，并将结果返回给 ChatGPT。</p>
<p>响应生成：最后，ChatGPT 被用于整合所有模型的预测并为用户生成响应。</p>
<p>HuggingGPT 在一个真实示例中是如何运作的, 如下图。</p>
<p><img src="/../asset_aiagenteverything/06.png"></p>
<h2 id="在代理工作流程中解决内存问题"><a href="#在代理工作流程中解决内存问题" class="headerlink" title="在代理工作流程中解决内存问题"></a>在代理工作流程中解决内存问题</h2><p>内存可以被定义为用于获取、存储、保留和稍后检索信息的资源或存储器。在任何计算系统中都有几种类型的内存。</p>
<h3 id="缓冲内存（感觉内存）"><a href="#缓冲内存（感觉内存）" class="headerlink" title="缓冲内存（感觉内存）:"></a>缓冲内存（感觉内存）:</h3><p>正如感觉记忆作为感觉信息的短暂保留（任务完成后立即消失）的作用，计算机系统中的缓冲存储器可以保存瞬态数据，如指令集。对于LLMs，这可能指的是令牌缓冲区或输入队列。</p>
<h3 id="工作记忆（短期记忆，STM）："><a href="#工作记忆（短期记忆，STM）：" class="headerlink" title="工作记忆（短期记忆，STM）："></a>工作记忆（短期记忆，STM）：</h3><p>LLMs在处理文本时，采用类似于人类工作记忆的机制。它们使用注意力机制来保持对输入的某些部分的“关注”。在基于 Transformer 的模型（如 GPT）中，注意力权重起着类似于 STM 的作用，同时保持和处理多个信息片段。LLM的工作记忆是其上下文长度。</p>
<h3 id="参数内存（长期记忆，LTM）："><a href="#参数内存（长期记忆，LTM）：" class="headerlink" title="参数内存（长期记忆，LTM）："></a>参数内存（长期记忆，LTM）：</h3><p>LLMs 的参数（或权重）可以被视为一种长期记忆形式。一旦训练完成，这些参数会编码从训练数据中获取的大量信息，并可以无限期地保留。</p>
<p>在LLMs中，“记忆”不是作为离散事件存储的，而是作为模式（抽象世界模型，尽管不完全）在一个互连节点网络中表示，并且该模型通过根据其训练状态动态生成响应来“回忆”信息。</p>
<p>外部存储器可以解决有限注意力跨度的问题。标准做法是将嵌入向量（文本转换为密集向量）保存到一个支持快速最大内积搜索（<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Maximum_inner-product_search">MIPS</a>）的向量存储数据库中。为了优化检索速度，常见选择是使用近似最近邻居（ANN）算法，以返回大约前 k 个最近邻居，以换取一点精度损失来获得巨大的加速。</p>
<p>用于此检索的一些算法：<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing">LSH</a>、<a target="_blank" rel="noopener" href="https://github.com/spotify/annoy">ANNOY</a>、<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.09320">HNSW</a>、<a target="_blank" rel="noopener" href="https://github.com/facebookresearch/faiss">FAISS</a> 和 <a target="_blank" rel="noopener" href="https://github.com/google-research/google-research/tree/master/scann">ScaNN</a></p>
<p>所有这些方法背后的一般原则——LSH、ANNOY、HNSW、FAISS 和 ScaNN——是在高维空间中高效地近似最近邻搜索。这些方法旨在通过使用将相似数据点组合在一起的策略，以允许更快的检索来克服精确搜索的计算强度。</p>
<h2 id="计划：当前-AI-代理的最大问题"><a href="#计划：当前-AI-代理的最大问题" class="headerlink" title="计划：当前 AI 代理的最大问题"></a>计划：当前 AI 代理的最大问题</h2><p>有时代理会为了回答一个单一&#x2F;简单的问题而大量调用，为每个查询积累令牌给LLM。这不仅成本高昂，还会引入延迟。令牌生成仍然是一个相对缓慢的过程，大部分（不是全部）基于LLM的应用程序中的延迟来自生成输出令牌。反复调用LLM并要求其提供想法&#x2F;观察，我们最终会生成大量输出令牌（成本），导致高延迟（用户体验下降）。</p>
<p>一个例子如下图所示。为了达到正确答案或行动，LLMs经常发出多个请求，由于代理工作流程的固有结构以树形或图形，这会引入许多延迟。</p>
<p>有很多类似的论文，比如<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2201.11903">思维链</a>，<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.10601.pdf">思维树</a>，<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2308.10379.pdf">思维算法</a>。</p>
<p><img src="/../asset_aiagenteverything/07.webp"></p>
<p>最新的高级提示策略是”思维算法”：这篇思维算法新论文的理念与思维树论文相同，但主要区别在于上下文保存的方式。在思维树中，我们使用基于树的数据结构，但在这里我们使用基于图的结构，因此给我们提供了比使用简单的 BFS 或 DFS 更好的方式来浏览所有的知识图。</p>
<p>另一个优势是思维算法使用更少的提示来实现类似的结果。这些提示策略背后的整个理念是以最少的调用LLM来获得最多的上下文。</p>
<p>AI 代理的另一个问题是LLMs是非确定性的。虽然对于创意生成有益，但在需要可预测性的场景中，这带来了严重挑战。例如，如果我们正在编写一个LLM支持的聊天应用程序来进行 Postgres 查询（Text2SQL），我们希望有很高的可预测性。</p>
<h2 id="自我反馈"><a href="#自我反馈" class="headerlink" title="自我反馈"></a>自我反馈</h2><p>由于计划模块不涉及任何反馈，这使得实现长期规划变得非常困难，尤其是解决复杂任务时尤为必要。为了解决这一挑战，我们可以创建一个过程，根据过去的行动和观察，反复反思和完善执行计划。目标是纠正和改进过去的错误，有助于提高最终结果的质量。这在复杂的现实环境和任务中尤为重要，试错是完成任务的关键。</p>
<h3 id="ReAct"><a href="#ReAct" class="headerlink" title="ReAct"></a>ReAct</h3><p><a target="_blank" rel="noopener" href="https://www.promptingguide.ai/techniques/react">ReAct</a> 结合推理和行动，旨在使LLM能够通过交替执行一系列步骤（重复 N 次）： Thought ， Action 和 Observation 来解决复杂任务。ReAct 促使LLMs生成口头推理痕迹和任务的行动。这使系统能够进行动态推理，为行动创建、维护和调整计划，同时还能与外部环境（例如维基百科）进行交互，将额外信息纳入推理过程。</p>
<p><img src="/../asset_aiagenteverything/08.webp"></p>
<p>下图显示了 ReAct 的示例以及执行问答所涉及的不同步骤。</p>
<p><img src="/../asset_aiagenteverything/09.webp"></p>
<h3 id="Reflexion"><a href="#Reflexion" class="headerlink" title="Reflexion"></a>Reflexion</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2303.11366">Reflexion</a>这是一个旨在为代理人提供动态记忆和自我反思能力以提高推理能力的框架。 Reflexion 具有标准的 RL 设置，其中奖励模型提供简单的二进制奖励，行动空间遵循 ReAct 中的设置，其中特定于任务的行动空间通过语言进行增强，以实现复杂的推理步骤。在每个动作 𝑎𝑡 之后，代理计算启发式 ℎ𝑡，并根据自我反思结果决定是否重置环境以开始新的试验。</p>
<p><img src="/../asset_aiagenteverything/10.webp"></p>
<p>heuristic (启发式)函数确定何时规划低效或包含幻觉,需要停止。低效的规划是指花费太长时间而没有成功。幻觉被定义为遇到一系列连续相同的行动，导致环境中出现相同的观察。</p>
<h3 id="Chain-of-Hindsight-（CoH）"><a href="#Chain-of-Hindsight-（CoH）" class="headerlink" title="Chain of Hindsight （CoH）"></a>Chain of Hindsight （CoH）</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.02676">CoH</a> , 在回顾链中，模型被要求通过查看自己的过去工作来改进自己，评分，并附有如何在下次做得更好的注释。模型通过尝试产生一个能获得更高评分的新输出来进行练习，利用自己尝试和反馈的历史。为了防止模型仅仅记住答案，训练过程中会隐藏部分历史记录。</p>
<h3 id="Algorithm-Distillation-AD"><a href="#Algorithm-Distillation-AD" class="headerlink" title="Algorithm Distillation (AD)"></a>Algorithm Distillation (AD)</h3><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.14215">AD</a> 将类似的想法应用于机器人或代理学习任务（如视频游戏中的 RL 代理）。代理人回顾了自己在过去几次尝试中的表现，并试图找出改进的模式。然后，它预测下一步应该比以前的更好，学习变得更好的策略，而不仅仅是特定问题的解决方案。</p>
<p>AD 专注于从代理人与环境互动的历史中学习的理念。通过查看过去行动序列及其结果，代理人试图将成功的策略提炼到其策略中。</p>
<p>Reflexion 反思则赋予代理人记住过去行动并反思其有效性的能力，特别关注检测和避免重复或无效策略（如幻觉）。</p>
<h2 id="代理类型"><a href="#代理类型" class="headerlink" title="代理类型"></a>代理类型</h2><h3 id="路由代理"><a href="#路由代理" class="headerlink" title="路由代理"></a>路由代理</h3><p>最简单的代理推理形式。根据用户查询和一组选择，输出一个选择子集以路由该查询。</p>
<p><img src="/../asset_aiagenteverything/11.webp"></p>
<p>在上述查询中，“作者在艺术学校期间做了什么？”被传递给路由器。路由器的工作是通过系统确定查询的最佳路径，以检索最相关的信息。</p>
<p>路由器可以选择两条可能的路径：</p>
<p>矢量查询引擎：该引擎根据查询向量与文档向量之间的语义相似性，从更大的集合中检索前 k 个相关文档。它使用向量空间模型来理解和检索与查询在语义上最接近的文档。</p>
<p>摘要查询引擎：该引擎检索所有文档并提供其内容的摘要。它允许系统广泛了解一个主题，而无需逐个查看每个文档。</p>
<p>根据路由器的决定，要么返回来自矢量查询引擎的特定文档子集，要么返回来自摘要查询引擎的所有相关文档的摘要，以回答用户的查询。这从根本上改变了最终返回的输出类型。有些查询可能需要详细答案，有些可能对较广泛的答案满意。这些路由器代理帮助我们修改LLMs的行为。</p>
<h3 id="查询规划代理"><a href="#查询规划代理" class="headerlink" title="查询规划代理"></a>查询规划代理</h3><p>查询规划代理背后的理念是首先将复杂查询分解为每个相关数据源的子问题，然后收集所有中间响应并综合出最终响应。</p>
<p><img src="/../asset_aiagenteverything/12.webp"></p>
<p>查看查询规划代理如何通过子查询获得更好的响应。</p>
<p><img src="/../asset_aiagenteverything/13.webp"></p>
<h3 id="工具使用代理"><a href="#工具使用代理" class="headerlink" title="工具使用代理"></a>工具使用代理</h3><p>这是可以使用工具或 API 的代理类型，推断运行该工具所需的参数，并将其结果返回给LLM上下文，以获得更准确的答案。</p>
<p><img src="/../asset_aiagenteverything/14.webp"></p>
<p>代理如何处理序列化的一堆问题？代理如何随时间保持状态？</p>
<p>这就是更高级代理如 HuggingGPT 和 ChemCrow 进入画面的地方。在这些多代理设置中，系统可以使用多个工具，并且通常还集成了动态内存。这些系统通常使用 ReAct 和 Reflexion 等方法，在代理的环境和行动中具有细致自洽的方法。</p>
<p><img src="/../asset_aiagenteverything/15.webp"></p>
<p>以下是用于构建 LLM 代理的工具和框架的示例：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://python.langchain.com/docs/get_started/introduction">LangChain</a>：一个基于语言模型的应用程序和代理开发框架。</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</a>：提供构建 AI 代理的工具。</li>
<li><a target="_blank" rel="noopener" href="https://microsoft.github.io/autogen/">AutoGen</a> ：一个框架，可以使用多个代理开发LLM应用程序，这些代理可以相互对话以解决任务。</li>
<li><a target="_blank" rel="noopener" href="https://www.llamaindex.ai/">LlamaIndex</a> - 用于将自定义数据源连接到大型语言模型的框架。</li>
<li><a target="_blank" rel="noopener" href="https://github.com/gpt-engineer-org/gpt-engineer">GPT Engineer</a> ：自动化代码生成以完成开发任务。</li>
<li><a target="_blank" rel="noopener" href="https://github.com/melih-unsal/DemoGPT">DemoGPT</a> ：自主 AI 代理以创建交互式 Streamlit 应用程序。</li>
</ul>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>LLM 代理仍处于萌芽阶段，构建它们仍存在许多挑战和限制，比如基于 LLM 的代理需要适应角色以有效地完成领域内的任务，能够进行长期规划，普遍的人类对齐，可靠性，知识限制，以及许多其他问题。</p>
<p>在未来，我们可能会研究更高级的概念，如 KV 缓存和LLM编译器与操作系统。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://medium.com/aiguys/ai-agents-are-all-you-need-0b38e8ee5481">https://medium.com/aiguys/ai-agents-are-all-you-need-0b38e8ee5481</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/04/17/magic-eraser/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/04/17/magic-eraser/" class="post-title-link" itemprop="url">使用 PyTorch 创建迷你神奇橡皮擦</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-04-17 20:16:47 / 修改时间：21:18:24" itemprop="dateCreated datePublished" datetime="2024-04-17T20:16:47+08:00">2024-04-17</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>15k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>27 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>去年，Google 向所有拥有 Google One 的用户提供了“神奇橡皮擦”。有了神奇橡皮擦，我们可以轻松地从照片中删除不需要的部分。它利用人工智能根据周围环境填充适当的对象，使整体图片看起来更加自然！微信也有这个功能。</p>
<p>这项技术可能看起来像魔术，让您可以随心所欲地更改图像。有些人可能认为这需要大量的数学知识和计算机性能。但实际上，魔术橡皮擦，也被称为“图像修复”，并不太复杂，可以在您自己的计算机上完成。</p>
<p>当今流行的方法包括基于扩散、GANs（生成对抗网络）和注意力机制的模型。在本文中，我将重点介绍基于 GAN 的图像修复方法。</p>
<h2 id="条件图像生成"><a href="#条件图像生成" class="headerlink" title="条件图像生成"></a>条件图像生成</h2><p>图像生成本质上是让我们的模型产生一个虚假的数据分布 Pmodel(x)。目标是使这个分布尽可能接近真实分布 Pdata(x)，从而生成新的图像。</p>
<p><img src="/../asset_magiceraser/01.webp"></p>
<p>在图像修复中，我们的目标是填补图像中缺失的部分。这可以被看作是向图像引入一个“条件”，将我们模型的方法从一般分布 Pmodel(x) 转变为条件分布 Pmodel(x | condition)。</p>
<p>这个过程类似于常规图像生成，但多了一个将条件纳入我们模型的额外步骤，我稍后会详细讨论。</p>
<h2 id="图像修复的步骤"><a href="#图像修复的步骤" class="headerlink" title="图像修复的步骤"></a>图像修复的步骤</h2><p><img src="/../asset_magiceraser/02.webp"></p>
<ul>
<li>创建一个随机遮罩</li>
<li>将真实图像与遮罩相结合（遮罩图像）</li>
<li>把遮罩图像和遮罩作为神经网络的输入，生成假图像</li>
<li>计算真实图像和假图像之间的相似性（损失）</li>
</ul>
<h2 id="联合调制-GAN"><a href="#联合调制-GAN" class="headerlink" title="联合调制 GAN"></a>联合调制 GAN</h2><p>联合调制生成对抗网络，或称 CoModGAN，是一种专为图像补全设计的生成模型。该模型在处理图像中大片缺失区域的任务中特别有效。</p>
<p>我们可以看到这是一种 GAN（生成对抗网络）模型，这意味着它主要由两部分组成：生成器负责创建图像，鉴别器评估这些图像以确定它们是真实的还是虚假的。</p>
<p><img src="/../asset_magiceraser/03.webp"><br>有条件图像生成的过程</p>
<p>让我们关注 CoModGAN 的另一个关键方面：’联合调制’。下面的图像展示了调制的不同方法：（D：解码器，M：映射网络, E：有条件编码器）</p>
<p><img src="/../asset_magiceraser/04.webp"></p>
<ul>
<li>模型 (a) 代表了一种用于生成图像的无条件模型，最初在 StyleGAN 中引入。其关键特征包括调制和映射网络。</li>
<li>模型 (b) 和 (c) 是编码器-解码器架构的典型示例，其中编码器将图像转换为潜在空间，而解码器重建原始图像。(例如 Pix2Pix，CycleGAN…)</li>
<li>模型 (d) 基于模型 (b) 和 (c) 的结构，并且还整合了 StyleGAN 的 (a) 映射网络以解开潜在向量 Z。此外，为了更好地控制生成图像的风格，它将 E 和 M 的输出组合作为 D 的额外输入，这就是为什么它被称为共调制。</li>
</ul>
<h2 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h2><p>机器学习中的第一步，也是最重要的一步是数据预处理阶段。在这个实现中，我们将使用 CelebA-HQ 数据集和矩形遮罩来训练模型。此外，您也可以使用自由形式的遮罩或创建自己的遮罩</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def rectangle_mask(shape, min_size=32, max_size=96):</span><br><span class="line">    h, w = shape</span><br><span class="line">    mask = np.zeros([h, w])</span><br><span class="line">    hole_size = random.randint(min_size, max_size)</span><br><span class="line">    hole_size = min(int(w*0.8), int(h*0.8), hole_size)</span><br><span class="line">    x = random.randint(0, w-hole_size-1)</span><br><span class="line">    y = random.randint(0, h-hole_size-1)</span><br><span class="line">    mask[x:x+hole_size, y:y+hole_size] = 1</span><br><span class="line">    return mask.reshape([1, h, w]).astype(np.float32)</span><br><span class="line">    </span><br><span class="line">def random_ff_mask(shape):</span><br><span class="line">    h,w = shape</span><br><span class="line">    mask = np.zeros((h,w))</span><br><span class="line">    num_v = 12+np.random.randint(5)</span><br><span class="line">    </span><br><span class="line">    for i in range(num_v):</span><br><span class="line">        start_x = np.random.randint(w)</span><br><span class="line">        start_y = np.random.randint(h)</span><br><span class="line">        for j in range(1+np.random.randint(5)):</span><br><span class="line">            angle = 0.01+np.random.randint(4)</span><br><span class="line">            if i % 2 == 0:</span><br><span class="line">                angle = 2 * 3.1415926 - angle</span><br><span class="line">            length = 10+np.random.randint(30)</span><br><span class="line">            brush_w = 10+np.random.randint(7)</span><br><span class="line">            end_x = (start_x + length * np.sin(angle)).astype(np.int32)</span><br><span class="line">            end_y = (start_y + length * np.cos(angle)).astype(np.int32)</span><br><span class="line"></span><br><span class="line">            cv2.line(mask, (start_y, start_x), (end_y, end_x), 1.0, brush_w)</span><br><span class="line">            start_x, start_y = end_x, end_y</span><br><span class="line">    return mask.reshape((1,) + mask.shape).astype(np.float32)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">class InpaintData(torch.utils.data.Dataset):</span><br><span class="line">    def __init__(self, path, img_size=[128, 128]):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.file_list = []</span><br><span class="line">        self.img_size = img_size</span><br><span class="line">        self.transforms = transforms.Compose([</span><br><span class="line">            # can put some augmentation here</span><br><span class="line">            transforms.RandomHorizontalFlip(0.5),</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]</span><br><span class="line">        )</span><br><span class="line">        for file_name in glob(f&#x27;&#123;path&#125;/**/*.jpg&#x27;, recursive=True):</span><br><span class="line">            self.file_list.append(file_name)</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.file_list)</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, idx):</span><br><span class="line">        file_name = self.file_list[idx]</span><br><span class="line">        img = Image.open(file_name).convert(&#x27;RGB&#x27;)</span><br><span class="line">        img = transforms.functional.resize(img, self.img_size, Image.BICUBIC)</span><br><span class="line">        img = self.transforms(img)</span><br><span class="line">        mask = rectangle_mask([128, 128])</span><br><span class="line">        return img, mask</span><br><span class="line">      </span><br><span class="line">path = &quot;put your path here!&quot;</span><br><span class="line">dataset = InpaintData(path)</span><br><span class="line">train_data = torch.utils.data.DataLoader(</span><br><span class="line">                    dataset,</span><br><span class="line">                    batch_size=16,</span><br><span class="line">                    shuffle= True,</span><br><span class="line">                    pin_memory=True,</span><br><span class="line">                    drop_last=True)</span><br></pre></td></tr></table></figure>

<p><img src="/../asset_magiceraser/05.webp"></p>
<h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p><img src="/../asset_magiceraser/06.webp"></p>
<p>上面的图表显示了 CoModGAN 中生成器的结构，可以分为两个主要组件：编码器网络和解码器网络。编码器相对简单，我们可以轻松地使用深度卷积网络，如 VGG，来对输入进行编码。</p>
<p>现在，让我们关注解码器部分。正如您所看到的，联合调制结合了来自编码器和映射网络的两个不同潜在向量，并将它们整合到解码器的各个层中，类似于 StyleGAN 中的合成网络。</p>
<h2 id="提高-StyleGAN-的图像质量"><a href="#提高-StyleGAN-的图像质量" class="headerlink" title="提高 StyleGAN 的图像质量"></a>提高 StyleGAN 的图像质量</h2><p><img src="/../asset_magiceraser/07.webp"><br>上面的图像是由 StyleGAN 生成的；我们可以看到图像的角落存在一些水滴。这种现象从分辨率为 64x64 的所有特征图中开始出现，并在更高分辨率下逐渐变得更加明显。<br><img src="/../asset_magiceraser/08.webp"></p>
<p>为了解决水滴问题，作者提出了 StyleGAN2（C 和 D），这是 StyleGAN 的修订版本。 他们认为这个问题是由 AdaIN 层引起的，该层分别对每个特征图的均值和方差进行归一化，可能破坏与特征相对大小相关的信息。</p>
<p><img src="/../asset_magiceraser/09.webp"></p>
<p>因此，通过去除风格均值，我们可以发现水滴伪影完全消失了。（附：AdaIN 现在直接在 CNN 的权重（w2，…）上运行，而不是在图像 Xi 上运行）</p>
<p><img src="/../asset_magiceraser/10.webp"><br>StyleGAN 的 Demod 层</p>
<p>另一个修改涉及将实例归一化（Norm std）替换为 Demod 层。由于归一化的目的是将层的输出恢复到单位标准差，这可以通过直接对层的权重进行归一化来更直接地实现，这被称为“解调”。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">class Conv2DMod(nn.Module):</span><br><span class="line">    def __init__(self, in_channel, out_channel, kernel, demod=True, stride=1):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.kernel = kernel</span><br><span class="line">        self.demod = demod</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.filters = out_channel</span><br><span class="line">        self.weight = nn.Parameter(torch.randn((out_channel, in_channel, kernel, kernel)))</span><br><span class="line">        nn.init.kaiming_normal_(self.weight, a=0, mode=&#x27;fan_in&#x27;, nonlinearity=&#x27;leaky_relu&#x27;)</span><br><span class="line"></span><br><span class="line">    def _get_same_padding(self, size, kernel, stride, dilation=1):</span><br><span class="line">        return ((size - 1) * (stride - 1) + dilation * (kernel - 1)) // 2</span><br><span class="line"></span><br><span class="line">    def forward(self, x, style):</span><br><span class="line">        &#x27;&#x27;&#x27; </span><br><span class="line">        x: [b, c, h, w]</span><br><span class="line">        style: [b, s_dim]</span><br><span class="line">        </span><br><span class="line">        Style modulation:</span><br><span class="line">        w&#x27; = style * w</span><br><span class="line">        Demodulation: </span><br><span class="line">        w&#x27;&#x27; = w&#x27; / sqrt(x&#x27;**2 + eps) &#x27;&#x27;&#x27;</span><br><span class="line">        </span><br><span class="line">        b, c, h, w = x.shape</span><br><span class="line">        style = style[:, None, :, None, None]</span><br><span class="line">        weight = self.weight[None, :, :, :, :]</span><br><span class="line">        weight = weight * (style + 1)</span><br><span class="line">        </span><br><span class="line">        if self.demod: </span><br><span class="line">            d = torch.rsqrt((weight ** 2).sum(dim=(2, 3, 4), keepdim=True) + 1e-8)</span><br><span class="line">            weight = weight * d</span><br><span class="line">            </span><br><span class="line">        x = x.reshape(1, -1, h, w)</span><br><span class="line">        _, _, *ws = weight.shape</span><br><span class="line">        weight = weight.reshape(b * self.filters, *ws)</span><br><span class="line">        padding = self._get_same_padding(h, self.kernel, self.stride)</span><br><span class="line">        x = F.conv2d(x, weight, padding=padding, groups=b)</span><br><span class="line">        x = x.reshape(-1, self.filters, h, w)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>

<h2 id="生成器块"><a href="#生成器块" class="headerlink" title="生成器块"></a>生成器块</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">class EqualLinear(nn.Module):</span><br><span class="line">    def __init__(self, in_dim, out_dim, bias = True, bias_init = 0, lr_mul = 1):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.weight = torch.nn.Parameter(torch.randn([out_dim, in_dim]) / lr_mul)</span><br><span class="line">        self.bias = torch.nn.Parameter(torch.full([out_dim], np.float32(bias_init))) if bias else None</span><br><span class="line">        self.weight_gain = lr_mul / np.sqrt(in_dim)</span><br><span class="line">        self.bias_gain = lr_mul</span><br><span class="line">    </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        w = self.weight * self.weight_gain</span><br><span class="line">        layer = F.linear(x, w, bias = None)</span><br><span class="line">        if self.bias is not None:</span><br><span class="line">            layer = F.linear(x, w, bias = self.bias * self.bias_gain)</span><br><span class="line">        return layer</span><br><span class="line"></span><br><span class="line">class ToRGB(nn.Module):</span><br><span class="line">    def __init__(self, latent_dim, inp_dim):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.style = Linear(latent_dim, inp_dim)</span><br><span class="line">        self.conv = Conv2DMod(inp_dim, 3, 1, demod=False)</span><br><span class="line">        self.upsample = Upsample(scale_factor = 2, mode=&#x27;bilinear&#x27;)</span><br><span class="line">        </span><br><span class="line">    def forward(self, inp, style, prev_rgb = None):</span><br><span class="line">        style = self.style(style)</span><br><span class="line">        out = self.conv(inp, style)</span><br><span class="line">        if prev_rgb is not None:</span><br><span class="line">            prev_rgb = self.upsample(prev_rgb)</span><br><span class="line">            out += prev_rgb</span><br><span class="line">        return out</span><br></pre></td></tr></table></figure>

<p>EqualLinear 用于映射网络中，在训练过程中动态重新调整层的输出。在我的实验中，与常规线性层相比，使用 EqualLinear 在映射网络中并没有显著影响结果。我建议直接使用 torch.nn.Linear。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">class G_encode(nn.Module):</span><br><span class="line">    def __init__(self, in_channel, out_channel, downsample=True):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.conv_res = Conv2d(in_channel, out_channel, kernel_size=1, padding=0)</span><br><span class="line">        self.encode = nn.Sequential(</span><br><span class="line">            Conv2d(in_channel, out_channel, kernel_size=3, padding=1),</span><br><span class="line">            LeakyReLU(0.2, inplace=True),</span><br><span class="line">            Conv2d(out_channel, out_channel, kernel_size=3, padding=1),</span><br><span class="line">            LeakyReLU(0.2, inplace=True)</span><br><span class="line">        )  </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        res = self.conv_res(x)</span><br><span class="line">        x = self.encode(x) + res</span><br><span class="line">        if self.downsample:</span><br><span class="line">            x = AvgPool2d(kernel_size=2, stride=2)(x)</span><br><span class="line">        return x</span><br><span class="line">        </span><br><span class="line">class G_decode(nn.Module):</span><br><span class="line">    def __init__(self, latent_dim, in_channel, out_channel, upsample=True):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.upsample = upsample</span><br><span class="line">        self.style1 = Linear(latent_dim, in_channel)</span><br><span class="line">        self.style2 = Linear(latent_dim, out_channel)</span><br><span class="line">        self.noise1 = Conv2d(1, out_channel, 1, padding=1)</span><br><span class="line">        self.noise2 = Conv2d(1, out_channel, 1, padding=1)</span><br><span class="line">        self.conv1 = Conv2DMod(in_channel, out_channel, 3)</span><br><span class="line">        self.conv2 = Conv2DMod(out_channel, out_channel, 3)</span><br><span class="line">        self.to_rgb = ToRGB(latent_dim, out_channel)</span><br><span class="line">        </span><br><span class="line">    def forward(self, x, style, noise, prev_rgb):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        if self.upsample:</span><br><span class="line">            x = Upsample(scale_factor=2, mode=&#x27;bilinear&#x27;, align_corners=False)(x)</span><br><span class="line">        noise1 = self.noise1(noise)[:, :, :x.shape[2], :x.shape[3]]</span><br><span class="line">        noise2 = self.noise2(noise)[:, :, :x.shape[2], :x.shape[3]]</span><br><span class="line">        style1 = self.style1(style)</span><br><span class="line">        style2 = self.style2(style)</span><br><span class="line">        </span><br><span class="line">        x = self.conv1(x, style1)</span><br><span class="line">        x = LeakyReLU(0.2, inplace=True)(x + noise1)</span><br><span class="line">        x = self.conv2(x, style2)</span><br><span class="line">        x = LeakyReLU(0.2, inplace=True)(x + noise2)</span><br><span class="line">        rgb = self.to_rgb(x, style, prev_rgb)</span><br><span class="line">        return x, rgb</span><br></pre></td></tr></table></figure>

<p><img src="/../asset_magiceraser/11.webp"><br>不同类型的 GAN 架构</p>
<p>这里有一些不同选择的 tRGB 和 fRGB 架构。在我的实验中，我使用 (b) 作为主要结构。</p>
<h2 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h2><p><img src="/../asset_magiceraser/12.webp"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">class Generator(nn.Module):</span><br><span class="line">    def __init__(self, img_size, latent_dim, style_mlp = 8, lr_mul = 0.1, fmap_min = 16, fmap_max = 512):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.num_layers = int(log2(img_size)-2)</span><br><span class="line">        f_map = [fmap_min * (2 ** i) for i in range(self.num_layers + 1)]</span><br><span class="line">        filters = list(map(lambda x: min(fmap_max, x), f_map))</span><br><span class="line">        init_channel = filters[0]</span><br><span class="line">        self.latent_dim = latent_dim</span><br><span class="line">        self.style_dim = latent_dim + filters[-1]</span><br><span class="line">        self.in_out_channel = zip(filters[:-1], filters[1:])</span><br><span class="line">        self.init_conv = Conv2d(init_channel, init_channel, 3, padding=1)</span><br><span class="line">        </span><br><span class="line">        self.style_mapping =  nn.Sequential()</span><br><span class="line">        self.downs =  nn.ModuleList([])</span><br><span class="line">        self.ups = nn.ModuleList([])</span><br><span class="line">        </span><br><span class="line">        # 4 because image (b, 3, h, w) + mask (b, 1, h, w)</span><br><span class="line">        self.from_rgb = nn.Sequential(</span><br><span class="line">            Conv2d(4, filters[0], 3, padding=1),</span><br><span class="line">            LeakyReLU(0.2, inplace=True)</span><br><span class="line">        )</span><br><span class="line">        # x_global: latent vector of the input image</span><br><span class="line">        self.x_global = nn.Sequential(</span><br><span class="line">            Linear(filters[-1] * 4 * 4, filters[-1]),</span><br><span class="line">            LeakyReLU(0.2, inplace=True)</span><br><span class="line">        )</span><br><span class="line">        # style mapping network</span><br><span class="line">        for _ in range(style_mlp):</span><br><span class="line">            self.style_mapping.extend([EqualLinear(latent_dim, latent_dim, lr_mul), </span><br><span class="line">                                       LeakyReLU(0.2, inplace=True)])</span><br><span class="line">        # Encoder</span><br><span class="line">        for in_channel, out_channel in self.in_out_channel:</span><br><span class="line">            g_block = G_encode(in_channel,out_channel)               </span><br><span class="line">            self.downs.append(g_block)</span><br><span class="line">            </span><br><span class="line">        # Decoder (styleGAN)</span><br><span class="line">        reversed_filters = list(reversed(filters))</span><br><span class="line">        self.in_out_channel = zip(reversed_filters[:-1], reversed_filters[1:])</span><br><span class="line">        for in_channel, out_channel in self.in_out_channel:</span><br><span class="line">            g_block = G_decode(self.style_dim, in_channel, out_channel)            </span><br><span class="line">            self.ups.append(g_block)</span><br><span class="line">        </span><br><span class="line">    def forward(self, img, mask, latents, noise, return_latent=False):</span><br><span class="line">        b, c, h, w = img.shape</span><br><span class="line">        x = torch.cat([1 - mask - 0.5, img * (1- mask)], 1)</span><br><span class="line">        x = self.from_rgb(x)</span><br><span class="line">        for blocks in self.downs:</span><br><span class="line">            x = blocks(x)</span><br><span class="line">            </span><br><span class="line">        rgb = None</span><br><span class="line">        x = x.view(b, -1)</span><br><span class="line">        x_global = self.x_global(x) </span><br><span class="line">        styles = self.style_mapping(F.normalize(latents, dim=1))</span><br><span class="line">        styles = styles.view([-1, b, self.latent_dim])</span><br><span class="line">        x = x.view(b, -1, 4, 4)</span><br><span class="line">        </span><br><span class="line">        for block, style in zip(self.ups, styles):</span><br><span class="line">            # co-modulation</span><br><span class="line">            co_style = torch.cat([x_global, style.squeeze()], 1)</span><br><span class="line">            x, rgb = block(x, co_style, noise, rgb)</span><br><span class="line">            </span><br><span class="line">        raw_out = rgb</span><br><span class="line">        images_out = rgb * mask + img * (1-mask)</span><br><span class="line">        if return_latent:</span><br><span class="line">            return images_out, raw_out, styles</span><br><span class="line">        return images_out, raw_out</span><br></pre></td></tr></table></figure>

<h2 id="鉴别器"><a href="#鉴别器" class="headerlink" title="鉴别器"></a>鉴别器</h2><p><img src="/../asset_magiceraser/13.webp"></p>
<p>在讨论了生成器之后，现在是时候转向鉴别器了。由于鉴别器的主要作用是区分图像是真实的还是虚假的，任何图像分类模型都可以用作鉴别器。</p>
<p>在这里，让我们简单地使用类似 VGG 的结构来实现它</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">class D_block(nn.Module):</span><br><span class="line">    def __init__(self, in_channel, out_channel, downsample = True):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.downsample = None</span><br><span class="line">        if downsample:</span><br><span class="line">            self.downsample = Conv2d(out_channel, out_channel, 3, padding = 1, stride = 2)</span><br><span class="line">        self.conv_res = Conv2d(in_channel, out_channel, 1, stride = (2 if downsample else 1))</span><br><span class="line">        self.net = nn.Sequential(</span><br><span class="line">            Conv2d(in_channel, out_channel, 3, padding=1),</span><br><span class="line">            LeakyReLU(0.2, inplace=True),</span><br><span class="line">            Conv2d(out_channel, out_channel, 3, padding=1),</span><br><span class="line">            LeakyReLU(0.2, inplace=True))</span><br><span class="line">        </span><br><span class="line">    def forward(self, x):</span><br><span class="line">        res = self.conv_res(x)</span><br><span class="line">        x = self.net(x)</span><br><span class="line">        if self.downsample is not None:</span><br><span class="line">            x = self.downsample(x)</span><br><span class="line">        x = (x + res) * (1 / math.sqrt(2))</span><br><span class="line">        return x</span><br><span class="line">      </span><br><span class="line">class Discriminator(nn.Module):</span><br><span class="line">    def __init__(self, img_size, fmap_min = 32, fmap_max = 512):</span><br><span class="line">        super().__init__()</span><br><span class="line">        self.num_layers = int(log2(img_size)-1)</span><br><span class="line">        f_map = [4] + [fmap_min * (2 ** i) for i in range(self.num_layers)]</span><br><span class="line">        filters = list(map(lambda x: min(fmap_max, x), f_map))</span><br><span class="line">        out_channel = filters[-1]</span><br><span class="line">        </span><br><span class="line">        self.in_out_channel = zip(filters[:-1], filters[1:])</span><br><span class="line">        self.blocks = nn.ModuleList([])</span><br><span class="line">        self.final_conv = Conv2d(out_channel, out_channel, 3, padding=1)</span><br><span class="line">        self.logit = Linear(out_channel * 4 * 4, 1)</span><br><span class="line">        for i, (in_channel, out_channel) in enumerate(self.in_out_channel):</span><br><span class="line">            down_sample = i != (self.num_layers - 1)</span><br><span class="line">            d_block = D_block(in_channel, out_channel, down_sample)</span><br><span class="line">            self.blocks.append(d_block)</span><br><span class="line">   </span><br><span class="line">    def forward(self, img, mask):</span><br><span class="line">        batch_size = img.shape[0]</span><br><span class="line">        x = torch.cat([1 - mask - 0.5, img], 1)</span><br><span class="line">        for block in self.blocks:</span><br><span class="line">            x = block(x)</span><br><span class="line">        x = self.final_conv(x)</span><br><span class="line">        x = x.view(batch_size, -1)</span><br><span class="line">        x = self.logit(x)</span><br><span class="line">        return x</span><br></pre></td></tr></table></figure>

<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><img src="/../asset_magiceraser/14.webp"><br>条件 GAN 的最小-最大损失</p>
<ul>
<li>G：生成器</li>
<li>D：判别器（输出值介于 0 到 1 之间，假：0，真：1）</li>
<li>X: 输入图像，形状 [B, C, H, W]</li>
<li>Y: 掩模，形状 [B, 1, H, W]</li>
<li>Z: 潜在向量（从高斯分布中随机采样）</li>
</ul>
<p>损失函数与经典 GAN 相同，为生成器和鉴别器设定不同的目标，从而在它们之间创建竞争。</p>
<p>鉴别器试图更好地区分由生成器制作的假图像和真实图像，而生成器的目标是欺骗鉴别器，与鉴别器的做法相反。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def d_loss_fn(real_pred, fake_pred):</span><br><span class="line">    real_loss = F.softplus(-real_pred)</span><br><span class="line">    fake_loss = F.softplus(fake_pred)</span><br><span class="line">    return real_loss.mean() + fake_loss.mean()</span><br><span class="line">    </span><br><span class="line">def g_loss_fn(fake_pred):</span><br><span class="line">    loss = F.softplus(-fake_pred).mean()</span><br><span class="line">    return loss</span><br></pre></td></tr></table></figure>

<h2 id="训练循环和懒惰正则化"><a href="#训练循环和懒惰正则化" class="headerlink" title="训练循环和懒惰正则化"></a>训练循环和懒惰正则化</h2><p>在 StyleGAN 中，作者采用了两种类型的正则化，即鉴别器的 R1 正则化和生成器的路径长度正则化，用于对两个组件施加约束。</p>
<p>另外，作者还发现并不需要在每次权重更新时都应用正则化。相反，对鉴别器的正则化每 16 次更新应用一次，对生成器的正则化每 8 次更新应用一次，这被称为“懒惰正则化”。</p>
<h2 id="R1-正则化"><a href="#R1-正则化" class="headerlink" title="R1 正则化"></a>R1 正则化</h2><p><img src="/../asset_magiceraser/15.webp"><br>R1 正则化仅对真实数据上的梯度进行惩罚，旨在防止鉴别器生成偏离数据流形的非零梯度。</p>
<p>例如，当生成器产生真实数据分布时，鉴别器在数据流形上的输出等于 0（通常意味着鉴别器无法区分真假数据），梯度惩罚确保鉴别器无法生成非零梯度，否则会导致损失增加。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def r1_loss_fn(real_pred, real_img):</span><br><span class="line">    grad_real, = autograd.grad(</span><br><span class="line">        outputs=real_pred.sum(), inputs=real_img, create_graph=True</span><br><span class="line">    )</span><br><span class="line">    grad_penalty = grad_real.pow(2).reshape(grad_real.shape[0], -1).sum(1).mean()</span><br><span class="line">    return grad_penalty</span><br></pre></td></tr></table></figure>

<h2 id="路径长度正则化"><a href="#路径长度正则化" class="headerlink" title="路径长度正则化"></a>路径长度正则化</h2><p><img src="/../asset_magiceraser/16.webp"></p>
<p>路径长度正则化旨在确保当您在潜在空间‘w’（w ~ f(z), f: 映射网络）中进行固定大小的更改时，图像会产生非零且固定幅度的变化。</p>
<p>我们计算生成器输出相对于‘w’的导数，并将其乘以随机噪声‘y’。在对这个结果应用 L2 范数之后，我们再减去其指数移动平均值‘a’。</p>
<p>这种正则化可以确保潜在向量 Z 的变化不会导致生成的图像发生剧烈变化。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def g_path_regularize(fake_img, latents, mean_path_length, decay=0.01):</span><br><span class="line">    noise = torch.randn_like(fake_img) / math.sqrt(</span><br><span class="line">        fake_img.shape[2] * fake_img.shape[3]</span><br><span class="line">    )</span><br><span class="line">    grad, = autograd.grad(</span><br><span class="line">        outputs=(fake_img * noise).sum(), inputs=latents, create_graph=True</span><br><span class="line">    )</span><br><span class="line">    path_lengths = torch.sqrt(grad.pow(2).sum(2).mean(1))</span><br><span class="line">    path_mean = mean_path_length + decay * (path_lengths.mean() - mean_path_length)</span><br><span class="line">    path_penalty = (path_lengths - path_mean).pow(2).mean()</span><br><span class="line">    return path_penalty, path_mean.detach(), path_lengths</span><br></pre></td></tr></table></figure>

<p>最后，这里是 CoModGAN 的训练循环</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def make_noise(n, latent_dim, device):</span><br><span class="line">    return torch.randn(n, 1, latent_dim).cuda(device)</span><br><span class="line"></span><br><span class="line">def get_g_input(batch, img_size, n_layers, latent_dim, device):</span><br><span class="line">    &#x27;&#x27;&#x27;</span><br><span class="line">    ** style mixing **</span><br><span class="line">    tt = int(torch.rand(()).numpy() * n_layers)</span><br><span class="line">    style1 = torch.tile(make_noise(batch, latent_dim, device), [1, tt, 1])</span><br><span class="line">    style2 = torch.tile(make_noise(batch, latent_dim, device), [1, n_layers - tt, 1])</span><br><span class="line">    styles = torch.cat([style1, style2], dim=1)</span><br><span class="line">    &#x27;&#x27;&#x27;   </span><br><span class="line">    styles = torch.tile(make_noise(batch, latent_dim, device), [1, n_layers, 1])</span><br><span class="line">    noise = torch.FloatTensor(batch, 1, img_size, img_size).uniform_(0., 1.).cuda(device)</span><br><span class="line">    return styles, noise</span><br><span class="line"></span><br><span class="line">def requires_grad(model, flag=True):</span><br><span class="line">    for p in model.parameters():</span><br><span class="line">        p.requires_grad = flag</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"># CoModGAN training Loop</span><br><span class="line">latent_dim = 128</span><br><span class="line">img_size = 128</span><br><span class="line">epochs = 100</span><br><span class="line">num_layers = int(log2(img_size)-1)</span><br><span class="line">d_lazy_reg = 16</span><br><span class="line">g_lazy_reg = 4</span><br><span class="line">mean_path = 0</span><br><span class="line"></span><br><span class="line">device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;</span><br><span class="line">g_model = Generator(img_size, latent_dim).to(device)</span><br><span class="line">d_model = Discriminator(img_size).to(device)</span><br><span class="line">g_optimizer = torch.optim.Adam(g_model.parameters(), lr=0.0001, betas=(0.5, 0.9))</span><br><span class="line">d_optimizer = torch.optim.Adam(d_model.parameters(), lr=0.0001, betas=(0.5, 0.9))</span><br><span class="line"></span><br><span class="line">for epoch in range(epochs):</span><br><span class="line">    for step, (real_img, mask) in enumerate(train_data):</span><br><span class="line">        B = real_img.size(0)</span><br><span class="line">        real_img = real_img.to(device)</span><br><span class="line">        mask = mask.to(device)</span><br><span class="line">        requires_grad(g_model, False)</span><br><span class="line">        requires_grad(d_model, True)</span><br><span class="line">        </span><br><span class="line">        style, noise = get_g_input(B, img_size, num_layers, latent_dim, device)</span><br><span class="line">        fake_img, _ = g_model(real_img, mask, style, noise)</span><br><span class="line">        fake_pred = d_model(fake_img, mask)</span><br><span class="line">        real_pred = d_model(real_img, mask)</span><br><span class="line">        d_loss = d_loss_fn(real_pred, fake_pred)</span><br><span class="line">        d_model.zero_grad()</span><br><span class="line">        d_loss.backward()</span><br><span class="line">        d_optimizer.step()</span><br><span class="line"></span><br><span class="line">        d_lazy = step % d_lazy_reg == 0</span><br><span class="line">        if d_lazy:</span><br><span class="line">            real_img.requires_grad = True</span><br><span class="line">            real_pred = d_model(real_img, mask)</span><br><span class="line">            r1_loss = r1_loss_fn(real_pred, real_img)</span><br><span class="line"></span><br><span class="line">            d_model.zero_grad()</span><br><span class="line">            (10 / 2 * r1_loss * d_lazy_reg).backward()</span><br><span class="line">            d_optimizer.step()</span><br><span class="line">            </span><br><span class="line">        requires_grad(g_model, True)</span><br><span class="line">        requires_grad(d_model, False)</span><br><span class="line">        style, noise = get_g_input(B, img_size, num_layers, latent_dim, device)</span><br><span class="line">        fake_img, _ = g_model(real_img, mask, style, noise)</span><br><span class="line">        fake_pred = d_model(fake_img, mask)</span><br><span class="line">        g_loss = g_loss_fn(fake_pred)</span><br><span class="line">        g_model.zero_grad()</span><br><span class="line">        g_loss.backward()</span><br><span class="line">        g_optimizer.step()</span><br><span class="line">        g_lazy = step % g_lazy_reg == 0</span><br><span class="line">        </span><br><span class="line">        if g_lazy:</span><br><span class="line">            style, noise = get_g_input(B, img_size, num_layers, latent_dim, device)</span><br><span class="line">            fake_img, _, latent = g_model(real_img, mask, style.requires_grad_(), </span><br><span class="line">                                          noise, return_latent=True)</span><br><span class="line">            path_loss, mean_path_new, path_lengths = g_path_regularize(fake_img, latent, mean_path)</span><br><span class="line">            g_model.zero_grad()</span><br><span class="line">            mean_path = mean_path_new</span><br><span class="line">            pr_loss = 2 * g_lazy_reg * path_loss</span><br><span class="line">            pr_loss.backward()</span><br><span class="line">            g_optimizer.step()</span><br></pre></td></tr></table></figure>

<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>顶部的图像是由 CoModGAN 生成的，而底部的图像是真实图像数据。</p>
<p>尽管生成的结果与真实数据之间存在一些小差异，但就生成合理图像而言，我认为 CoModGAN 的表现已经可以接受了</p>
<p><img src="/../asset_magiceraser/17.webp"></p>
<p><img src="/../asset_magiceraser/18.webp"></p>
<p><img src="/../asset_magiceraser/19.webp"></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://levelup.gitconnected.com/revealing-the-secrets-of-googles-magic-eraser-fb232c83723b">https://levelup.gitconnected.com/revealing-the-secrets-of-googles-magic-eraser-fb232c83723b</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/04/16/falkordb/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/04/16/falkordb/" class="post-title-link" itemprop="url">FalkorDB 知识图谱 for RAG</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-04-16 19:56:22 / 修改时间：20:09:09" itemprop="dateCreated datePublished" datetime="2024-04-16T19:56:22+08:00">2024-04-16</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>923</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>2 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>在选择适合您的 RAG 应用程序的正确知识图时，需要考虑许多因素。图数据库应该轻量且具有较低的延迟，以实现更高的性能。查询语言应该足够表达性，以支持复杂的图遍历进行模式匹配。图形理想情况下还应设计为具有较低的内存占用量 — 这样可以节省长期资源开销。</p>
<p>考虑到上述因素，我发现知识图谱 FalkorDB 是一个轻量级且高性能的图形框架，适用于构建人工智能应用程序。以下是原因：</p>
<h3 id="低延迟"><a href="#低延迟" class="headerlink" title="低延迟"></a>低延迟</h3><p>FalkorDB 旨在为复杂的图遍历提供快速的查询执行。它构建在其前身 RedisGraph 之上，利用内存处理技术加快查询执行速度。通过将相关数据保留在内存中，可以减少与磁盘 I&#x2F;O 操作相关的延迟。</p>
<h3 id="高效内存利用的分布式架构"><a href="#高效内存利用的分布式架构" class="headerlink" title="高效内存利用的分布式架构"></a>高效内存利用的分布式架构</h3><p>FalkorDB 允许创建<a target="_blank" rel="noopener" href="https://www.falkordb.com/scale-out/">多个读取副本</a>，实现数据集和图形在各个服务器之间的分布。用户可以创建与主数据库同步的数据库副本。这使我们能够将读取查询（在副本上运行）与写入查询（在主数据库上运行）分开。</p>
<p>这种方法优化了读写操作路由到适当副本的过程，确保操作的高吞吐量和较低的延迟。</p>
<h3 id="增强检索的向量索引支持"><a href="#增强检索的向量索引支持" class="headerlink" title="增强检索的向量索引支持"></a>增强检索的向量索引支持</h3><p>这允许将数据存储为高维嵌入，从而使用户能够使用相似性搜索找到实体 — 节点和边。这样，用户的响应可以扩展到还包括与输入查询在语义上相似的数据。</p>
<h3 id="支持-AI-框架"><a href="#支持-AI-框架" class="headerlink" title="支持 AI 框架"></a>支持 AI 框架</h3><p>FalkorDB 提供支持和文档，用于处理 AI 框架，如 LangChain 和 LLamaIndex。通过将其与LLM连接，可以直接使用模块来创建图索引和查询引擎对象。这些集成为用户提供了一个全面的工具包，用于构建像检索增强生成（RAG）管道这样的高级 AI 应用程序。</p>
<h3 id="交互式图浏览器"><a href="#交互式图浏览器" class="headerlink" title="交互式图浏览器"></a>交互式图浏览器</h3><p>FalkorDB 配备了图浏览器，允许交互式查看和查询图形。您可以在<a target="_blank" rel="noopener" href="https://github.com/FalkorDB/falkordb-browser">这里</a>查看 FalkorDB 图浏览器。用户可以通过单击节点和边，展开或折叠子图，并放大或缩小以聚焦特定感兴趣的区域来浏览图形。</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>FalkorDB 的内存处理技术和高效的内存利用率，相较于采用基于磁盘存储方法的图形，有助于更快地执行查询并减少延迟。对各种人工智能框架的支持和文档为构建人工智能应用程序增添了其稳健性。<br>这些特性使得 FalkorDB 成为构建可扩展和高性能 RAG 应用程序的极佳选择。基于知识图的 RAG 模型有望通过生成更准确、信息丰富和连贯的响应来彻底改变自然语言处理领域。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/04/12/autogenstudio-mistral/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/04/12/autogenstudio-mistral/" class="post-title-link" itemprop="url">用AutoGen Studio 与本地 Mistral AI 模型来实践生成式AI代理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-04-12 20:20:13" itemprop="dateCreated datePublished" datetime="2024-04-12T20:20:13+08:00">2024-04-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-04-28 19:55:24" itemprop="dateModified" datetime="2024-04-28T19:55:24+08:00">2024-04-28</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>14k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>25 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>AutoGen 是由 Microsoft 开发的框架，旨在简化多代理应用程序的开发，特别是在编排 LLM 代理方面。</p>
<p>多智能体应用程序涉及多个 LLM 或多模式智能体或实体在整个工作流程中相互交互以实现特定目标或任务的系统。这些代理可以是 LLM 代理、检索代理或其他能够做出独立决策、函数调用或执行操作的代理。</p>
<p>在本文中，我们将重点介绍AutoGen Studio直观的无代码平台与本地集成的Mistral AI模型的融合。这种组合不仅仅是为了让人工智能更容易应用;它还涉及促进我们如何在许多现实生活中的行业工作流程中与不同的生成式 AI 代理进行交互、部署并从中受益。</p>
<p><img src="/../asset_autogenmistral/01.png"></p>
<h2 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h2><h3 id="安装autogen-studio"><a href="#安装autogen-studio" class="headerlink" title="安装autogen studio"></a>安装autogen studio</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install autogenstudio</span><br></pre></td></tr></table></figure>


<h3 id="安装mistral"><a href="#安装mistral" class="headerlink" title="安装mistral"></a>安装mistral</h3><p>先安装ollama</p>
<ul>
<li><p>Ollama 是一个提供对大型语言模型的访问的平台。安装 Ollama 是第一步，因为它是运行 Mistral 模型的环境。它允许您自定义和创建自己的模型或运行现有模型，如 Llama 2、Code Llama 和 Mistral。您可以在此处下载ollama： <a target="_blank" rel="noopener" href="https://ollama.ai/">https://ollama.ai/</a> ， 选择download，选择linux版本，因为mistral 7B需要24GB 显卡，我没有，只能去云端的linux机器上安装ollama。</p>
</li>
<li><p>运行 Ollama Mistral：按照以下命令从 Ollama 平台启动 Mistral 模型，使其准备好处理请求。此命令初始化用于设置模型、加载必要数据以及启动将处理查询处理的服务的进程。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run mistral</span><br></pre></td></tr></table></figure>
<p>如果说没有ollama app，就起一个console，执行 ollama serve，然后在另外一个console执行ollama run mistral。执行完测试一下mistral，就可以把两个进程都杀掉了。</p>
</li>
<li><p>安装 LiteLLM：LiteLLM 库是一个工具，它有助于从 http 端点运行语言模型。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install litellm --upgrade</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装 Gunicorn：Gunicorn 是用于 UNIX 系统的 Python WSGI HTTP 服务器，用于运行 Python Web 应用程序。它是运行 LiteLLM 代理的先决条件，它允许您向语言模型发出本地 HTTP 请求。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install gunicorn</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="运行-Mistral-模型"><a href="#运行-Mistral-模型" class="headerlink" title="运行 Mistral 模型"></a>运行 Mistral 模型</h2><p>使用 Ollama&#x2F;Mistral 模型运行 LiteLLM：此步骤实际上初始化模型并使其准备好运行。此命令告诉 LiteLLM 使用 Ollama 提供的 Mistral 模型。它为交互准备模型，允许您开始进行查询和接收响应。</p>
<p>在此过程结束时，本地 Mistral 模型从 0.0.0.0：4000 开始，有 1 个工作线程。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">litellm --model ollama/mistral</span><br></pre></td></tr></table></figure>


<h2 id="运行-Autogen-Studio"><a href="#运行-Autogen-Studio" class="headerlink" title="运行 Autogen Studio"></a>运行 Autogen Studio</h2><p>通过在终端中输入以下内容来运行 Web UI：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">autogenstudio ui --port 6006</span><br></pre></td></tr></table></figure>

<p>AutoGen Studio 提供了一个简化且用户友好的界面，有助于创建和管理多代理 AI 应用程序。该界面分为几个部分，包括技能、模型、代理和工作流，每个部分在应用程序开发中都发挥着不可或缺的作用。</p>
<h3 id="技能"><a href="#技能" class="headerlink" title="技能"></a>技能</h3><p>在“技能”部分中，用户可以开发和存储代理将用于解决任务的 Python 函数。它是一个编程环境，在其中定义和优化应用程序的功能。<br><img src="/../asset_autogenmistral/06.png"></p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>在模型部分，用户可以配置和管理 AI 模型，包括 GPT-4 和其他本地或自定义模型。</p>
<p>用户可以设置 Mistral AI 等模型，表明工作室支持本地 AI 模型集成和管理。这种灵活性允许使用强大的模型，如 OpenAI 的 GPT-4，以及为特定任务量身定制的专有或专用模型。<br><img src="/../asset_autogenmistral/07.png"></p>
<h3 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h3><p>“代理”部分设计用于配置可重用代理。用户可以创建和定义各种代理的角色，例如本地助理、主要代理和用户代理，协调它们在应用工作流中的交互方式。<br><img src="/../asset_autogenmistral/08.png"></p>
<h3 id="工作流"><a href="#工作流" class="headerlink" title="工作流"></a>工作流</h3><p>“工作流”部分是奇迹发生的地方。用户可以设计复杂的工作流，以定义系统如何处理任务。这是所有组件聚集在一起的地方：技能提供逻辑，模型提供 AI 的智能，代理对任务采取行动，工作流将所有组件连接到一个连贯的系统中。<br><img src="/../asset_autogenmistral/09.png"></p>
<h2 id="使用-AutoGen-和-Mistral-AI-进行工作流编排："><a href="#使用-AutoGen-和-Mistral-AI-进行工作流编排：" class="headerlink" title="使用 AutoGen 和 Mistral AI 进行工作流编排："></a>使用 AutoGen 和 Mistral AI 进行工作流编排：</h2><p>在我们想要编写一个 Python 脚本来绘制正弦波并将其保存为“sine_wave.png”文件的情况下，AutoGen 和 Mistral AI 可以一起使用来创建简化的工作流程。AutoGen Studio 用作构建和管理多智能体应用程序的接口，而 Mistral AI 则通过其本地模型提供处理能力。</p>
<p>下面是使用这两个平台的功能进行编排的工作方式：</p>
<h3 id="模型配置"><a href="#模型配置" class="headerlink" title="模型配置"></a>模型配置</h3><p>在 AutoGen Studio 中设置了一个名为“mistral local model”的模型，用于与 Mistral AI 进行交互。<br>模型的 API 端点将配置为运行 Mistral AI 的本地服务器地址（例如，<a href="http://0.0.0.0:4000）。">http://0.0.0.0:4000）。</a><br>此设置允许 AutoGen Studio 将数据发送到 Mistral AI 模型并接收处理后的结果。</p>
<p><img src="/../asset_autogenmistral/02.png"></p>
<p>因为我们有 LiteLLM 代理服务器的 URL <a href="http://0.0.0.0:4000，可以在">http://0.0.0.0:4000，可以在</a> AutoGen 中使用它，就像使用 OpenAI 或基于云的代理服务器一样。</p>
<p>由于在本地运行此代理服务器，因此不需要 API 密钥。此外，由于模型是在运行 LiteLLM 命令时设置的，因此不需要在 AutoGen 中配置模型名称。但是，model type和api key是 A​​utoGen 中配置的必填字段，因此我们在其中放置虚拟值 NotRequired ，如上图所示。</p>
<h3 id="代理配置"><a href="#代理配置" class="headerlink" title="代理配置"></a>代理配置</h3><p>我们可以新建一个代理，也可以直接修改primary_assistant的Model，设定到之前定义的mistral local model<br><img src="/../asset_autogenmistral/03.png"></p>
<h3 id="创建工作流"><a href="#创建工作流" class="headerlink" title="创建工作流"></a>创建工作流</h3><ul>
<li>在AutoGen Studio中，将启动一个新的工作流，指定用于生成正弦波图像的特定任务。</li>
<li>我们使用名称配置工作流，例如“local mistral Agent Workflow”。</li>
<li>工作流规范包括发送方和接收方代理，其中“userproxy”可以是发起请求的发送方，“primary_assistant”是处理请求并提供输出的接收方。</li>
<li>userproxy代理的角色将与用户交互并执行primary_assistant生成的代码。如果代码中有任何错误，将错误发送给primary_assistant代理。</li>
<li>primary_assistant代理由本地 Mistral AI 模型授权，它将在我们的场景中生成代码。如果错误，根据userproxy代理发送的执行错误信息重新调整代码。</li>
</ul>
<p><img src="/../asset_autogenmistral/04.png"></p>
<h3 id="建立并运行会话"><a href="#建立并运行会话" class="headerlink" title="建立并运行会话"></a>建立并运行会话</h3><p>在playground新建一个session，</p>
<p><img src="/../asset_autogenmistral/05.png"></p>
<ul>
<li>之后在AutoGen Studio的“Playground”界面中，用户通过选择“Local mistral Agent Workflow”来启动一个新会话。</li>
<li>用户输入“write a python script to plot a sine wave and save it to disc as a pngfile sina_wave.png”请求。</li>
<li>请求由userproxy代理发送到primary_assistant代理，该代理利用 Mistral AI 模型。它生成 Python 脚本，userproxy代理执行代码。如果发生错误，则将其发送回primary_assistant代理。primary_assistant代理重新调整代码并将其发送回userproxy代理，直到代码正常工作并创建正弦波图像。</li>
</ul>
<p><img src="/../asset_autogenmistral/10.webp"></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>AutoGen Studio 与本地 Mistral AI 模型一起展示了在编排 GenAI 代理工作流程方面的重大进展。这种架构有利于 AutoGen Studio 的直观设计和 Mistral AI 的出色性能，构建一个多智能体能够做出贡献的环境。</p>
<p>将 AutoGen Studio 与本地 Mistral AI 模型结合使用的优势：</p>
<ul>
<li>易于集成：本地 Mistral AI 模型可以轻松集成到 AutoGen Studio 中，从而简化了将不同 LLM 模型整合到一个部署中的过程，从而简化了开发过程。</li>
<li>定制和灵活性：开发人员可以定制 AI 模型以满足特定要求，提供完全适合单个项目不同需求的定制解决方案。</li>
<li>增强的性能：使用 AutoGen 在本地运行 AI 模型可减少延迟并缩短响应时间，这对于实时应用程序和敏感工作流至关重要。</li>
<li>数据隐私和安全：AI 模型的本地执行可以完全控制数据，这是无法暴露给外部环境的敏感或专有信息的关键因素。</li>
<li>成本效益：通过最大限度地减少对基于云的 AI 服务的依赖，组织可以显著降低与数据传输和 LLM API 使用相关的成本。</li>
<li>离线功能：本地 Mistral AI 模型确保系统即使在没有互联网连接的情况下也能保持运行，从而实现 AI 驱动解决方案的离线功能。</li>
<li>使用 UI 构建复杂的 AI 工作流：用户可以利用AutoGen Studio的无代码界面来可视化和管理整个过程，轻松制作精心设计的人工智能驱动的工作流程。</li>
</ul>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/no-code-genai-agents-workflow-orchestration-autogen-studio-with-local-mistral-ai-model-7566546a16d9">https://towardsdatascience.com/no-code-genai-agents-workflow-orchestration-autogen-studio-with-local-mistral-ai-model-7566546a16d9</a></p>
<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>由于在AutoDL做了一个镜像，装了mistral，就顺便测试一下多文件代码生成。</p>
<h3 id="创建文件-multiple-code-generator-py"><a href="#创建文件-multiple-code-generator-py" class="headerlink" title="创建文件 multiple_code_generator.py"></a>创建文件 multiple_code_generator.py</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br></pre></td><td class="code"><pre><span class="line">from litellm import completion</span><br><span class="line">import colorama</span><br><span class="line">from colorama import Fore, Style</span><br><span class="line">import re</span><br><span class="line">import os</span><br><span class="line">def generate_context_prompt(user_input):</span><br><span class="line">    return f&quot; Understand just the context \&quot;&#123;user_input&#125;\&quot;.&quot;</span><br><span class="line"></span><br><span class="line">def generate_features_prompt(system):</span><br><span class="line">    return f&quot; Understand just the context \&quot;&#123;system&#125;\&quot;. Now identify features by expanding for the context and list them&quot;</span><br><span class="line"></span><br><span class="line">def generate_user_stories_prompt(features, system):</span><br><span class="line">    return f&quot; For the features \&quot;&#123;features&#125; and &#123;system&#125;\&quot;: identify user stories for each feature and list them&quot;</span><br><span class="line"></span><br><span class="line">def generate_solution_architecture_prompt(user_stories, system):</span><br><span class="line">    return f&quot; For user stories in \&quot;&#123;user_stories&#125; and &#123;system&#125;\&quot;: list the solution architecture&quot;</span><br><span class="line"></span><br><span class="line">def generate_project_structure_prompt(solution_architecture, user_stories, system):</span><br><span class="line">    return f&quot;&quot;&quot;  </span><br><span class="line">     For the context \&quot;&#123;solution_architecture&#125; and &#123;user_stories&#125; and &#123;system&#125;\&quot;:</span><br><span class="line">        Think about the whole solution.</span><br><span class="line">        Think about overall coding structure based on user stories.</span><br><span class="line">        Think about overall testing structure based on user stories.</span><br><span class="line">        Think about what file refers to what other file and make it as a dependency in the project structure.</span><br><span class="line">        Think about external or framework library references.</span><br><span class="line">        Think above all the edge cases that can come while coding and testing.</span><br><span class="line">        Think that each file will be completed without any &quot;todo&quot; comments for future.</span><br><span class="line">        Envisage to allow only 200 lines per file.</span><br><span class="line">        If more than 200 lines needed separate them into multiple files.</span><br><span class="line">        Do not allow duplication of functionality and code.</span><br><span class="line">        Do not allow duplication of test code.</span><br><span class="line">        Make sure all top level files like main, index, app etc are included if needed.</span><br><span class="line">        Make sure to have proper test files are envisaged.</span><br><span class="line">        It is also must to think about each reference file as needed, like an addition definition in one file and </span><br><span class="line">        Its reference in other file where it is being called and also in corresponding test files.</span><br><span class="line">        Now based on these understandings create project structure with its solution file structure tree, solution files&#x27; dependency tree and solution deployment script tree of the project.</span><br><span class="line">        In both project structure and dependency tree, filename are a must, if they are available</span><br><span class="line">        Expectation here is to output file names and their oneliner description.</span><br><span class="line">        Expectation also contains each files dependent filenames for each coding and testing files.</span><br><span class="line">        Make sure that the project tree structure hold all the necessary folder structures and files in each.  </span><br><span class="line">        Make sure that the dependency tree structure hold all the necessary file name that are to be generated with its dependency files in the solution.</span><br><span class="line">        Make sure that there are relevant test file names</span><br><span class="line">        Make sure that there are only files to be generated in both trees.</span><br><span class="line">        Make sure that only filename will have &quot;.&quot; in the text here.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">def extract_filenames_prompt(project_structure):</span><br><span class="line">    return f&quot;extract only file names to be generated from: &#123;project_structure&#125;&quot;</span><br><span class="line"></span><br><span class="line">def generate_file_content_prompt(user_stories, project_structure, filenames, taskcount):</span><br><span class="line">    return f&quot;&quot;&quot;</span><br><span class="line">    Understand the context &#x27;&#123;user_stories&#125;&#x27; as a whole.</span><br><span class="line">    Understand the project structure  &#123;project_structure&#125;.</span><br><span class="line">    Then think about no details but just write file content for &#123;filenames[taskcount]&#125;.</span><br><span class="line">    Refer dependency files in the project structure.</span><br><span class="line">    Write bug free well-formatted file.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">def generate_installation_scripts_prompt(features, system):</span><br><span class="line">    return f&quot;&quot;&quot;Based on the features \&quot;&#123;features&#125;\&quot; and system \&quot;&#123;system&#125;\&quot;, generate detailed installation scripts. </span><br><span class="line">           Include environment setup, dependency management, and any necessary system configurations.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">def generate_deployment_scripts_prompt(solution_architecture, system):</span><br><span class="line">    return f&quot;&quot;&quot;Considering the solution architecture \&quot;&#123;solution_architecture&#125;\&quot; and system \&quot;&#123;system&#125;\&quot;,</span><br><span class="line">           generate deployment scripts. Focus on automating the deployment process, including server configurations,</span><br><span class="line">           application deployment, and post-deployment checks.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">def generate_test_files_prompt(user_stories, system):</span><br><span class="line">    return f&quot;&quot;&quot;For each user story in \&quot;&#123;user_stories&#125;\&quot; and the system \&quot;&#123;system&#125;\&quot;,</span><br><span class="line">           generate comprehensive test files. Ensure coverage for unit tests, integration tests, and any relevant</span><br><span class="line">           end-to-end tests. Focus on testing critical functionalities and edge cases.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"># Initialize colorama</span><br><span class="line">colorama.init(autoreset=True)</span><br><span class="line"></span><br><span class="line">roles= &quot;&quot;</span><br><span class="line"></span><br><span class="line">def create_directory(directory_path):</span><br><span class="line">    if not os.path.exists(directory_path):</span><br><span class="line">        os.makedirs(directory_path)</span><br><span class="line">        print(f&quot;Directory &#x27;&#123;directory_path&#125;&#x27; created successfully.&quot;)</span><br><span class="line">    else:</span><br><span class="line">        print(f&quot;Directory &#x27;&#123;directory_path&#125;&#x27; already exists.&quot;)</span><br><span class="line"></span><br><span class="line">def extract_content_between_backticks(text):</span><br><span class="line">        pattern = r&#x27;```(.*?)```&#x27;</span><br><span class="line">        return re.findall(pattern, text, re.DOTALL)</span><br><span class="line"></span><br><span class="line">def complete(prompt_function, *args):</span><br><span class="line">            prompt = prompt_function(*args)</span><br><span class="line">            response = completion(</span><br><span class="line">                model=&quot;ollama/mistral&quot;, </span><br><span class="line">                messages=[&#123;&quot;content&quot;: prompt, &quot;role&quot;: &quot;user&quot;&#125;], </span><br><span class="line">                api_base=&quot;http://localhost:11434&quot;,</span><br><span class="line">                stream=True</span><br><span class="line">            )</span><br><span class="line">            resp=&quot;&quot;</span><br><span class="line">            for chunk in response:</span><br><span class="line">                if chunk[&#x27;choices&#x27;][0][&#x27;delta&#x27;][&#x27;content&#x27;] is not None:</span><br><span class="line">                    resp = resp + chunk[&#x27;choices&#x27;][0][&#x27;delta&#x27;][&#x27;content&#x27;]</span><br><span class="line">                    print(Fore.GREEN + chunk[&#x27;choices&#x27;][0][&#x27;delta&#x27;][&#x27;content&#x27;], end=&quot;&quot;)</span><br><span class="line">            resp=resp+&#x27;\n&#x27;</span><br><span class="line">            return resp</span><br><span class="line"></span><br><span class="line">pattern = r&#x27;(\b\w+\.\w+\b)&#x27;</span><br><span class="line"></span><br><span class="line">def extract_filenames(title):</span><br><span class="line">    filenames = re.findall(pattern, title)</span><br><span class="line">    return filenames</span><br><span class="line"></span><br><span class="line">def create_code():</span><br><span class="line">    user_input = &quot;&quot;</span><br><span class="line">    resp = &quot;&lt;&lt;done&gt;&gt;&quot;</span><br><span class="line">    taskcount = -1</span><br><span class="line">    filenames = []</span><br><span class="line">    text = []</span><br><span class="line">    output_log = []  # List to collect all output strings</span><br><span class="line"></span><br><span class="line">    while taskcount &lt; len(filenames):</span><br><span class="line">        input_lines = []</span><br><span class="line">        if &quot;&lt;&lt;done&gt;&gt;&quot; in resp.strip().lower():</span><br><span class="line">            prompt = Fore.YELLOW + &quot;Describe the problem for which coding to be done:&quot;</span><br><span class="line">            print(prompt)</span><br><span class="line">            output_log.append(prompt)  # Collect prompt for output log</span><br><span class="line">            while True:</span><br><span class="line">                line = input()</span><br><span class="line">                if line == &quot;exit&quot;:</span><br><span class="line">                    exit_message = Fore.RED + &quot;Exiting chat.&quot;</span><br><span class="line">                    print(exit_message)</span><br><span class="line">                    output_log.append(&quot;Exiting chat.&quot;)  # Collect exit message without color codes</span><br><span class="line">                    return</span><br><span class="line">                if line == &quot;&quot;:</span><br><span class="line">                    break</span><br><span class="line">                input_lines.append(line)</span><br><span class="line">            user_input = &quot;&quot;.join(input_lines)</span><br><span class="line">            taskcount = 0</span><br><span class="line">            system = complete(generate_context_prompt, user_input)</span><br><span class="line">            features = complete(generate_features_prompt, system)</span><br><span class="line">            user_stories = complete(generate_user_stories_prompt, features, system)</span><br><span class="line">            solution_architecture = complete(generate_solution_architecture_prompt, user_stories, system)</span><br><span class="line">            project_structure = complete(generate_project_structure_prompt, solution_architecture, user_stories, system)</span><br><span class="line">            </span><br><span class="line">            # Generate Installation Scripts</span><br><span class="line">            installation_scripts = complete(generate_installation_scripts_prompt, features, system)</span><br><span class="line">            output_log.append(installation_scripts)  # Collect installation scripts</span><br><span class="line"></span><br><span class="line">            # Generate Deployment Scripts</span><br><span class="line">            deployment_scripts = complete(generate_deployment_scripts_prompt, solution_architecture, system)</span><br><span class="line">            output_log.append(deployment_scripts)  # Collect deployment scripts</span><br><span class="line">            </span><br><span class="line">            # Generate Test Files</span><br><span class="line">            test_files = complete(generate_test_files_prompt, user_stories, system)</span><br><span class="line">            output_log.append(test_files)  # Collect test files</span><br><span class="line"></span><br><span class="line">            filenames_response = complete(extract_filenames_prompt, project_structure)</span><br><span class="line">            filenames = extract_filenames(filenames_response)</span><br><span class="line">            output_log.extend([system, features, user_stories, solution_architecture, project_structure, filenames_response])  # Collect all outputs</span><br><span class="line"></span><br><span class="line">        for filename in filenames:</span><br><span class="line">            prompt = f&quot;\n-------------------------------------------------------\n------------------&#123;filename&#125;----------------\n-------------------------------------------------------\n&quot;</span><br><span class="line">            print(prompt)</span><br><span class="line">            output_log.append(prompt)  # Collect file processing prompt</span><br><span class="line">            resp = complete(generate_file_content_prompt, user_stories, project_structure, filenames, taskcount)</span><br><span class="line">            text.append(resp)</span><br><span class="line">            output_log.append(resp)  # Collect file content</span><br><span class="line">            taskcount += 1</span><br><span class="line"></span><br><span class="line">    out_dir = &quot;generated_output&quot;</span><br><span class="line">    create_directory(out_dir)</span><br><span class="line">    for i, filename in enumerate(filenames):</span><br><span class="line">        with open(f&quot;&#123;out_dir&#125;/&#123;filename&#125;&quot;, &quot;w&quot;) as file:</span><br><span class="line">            extracted_content = extract_content_between_backticks(text[i])</span><br><span class="line">            if extracted_content:</span><br><span class="line">                file_content = extracted_content[0]</span><br><span class="line">                file.write(file_content)</span><br><span class="line">            else:</span><br><span class="line">                error_message = f&quot;No content extracted for &#123;filename&#125;&quot;</span><br><span class="line">                print(error_message)</span><br><span class="line">                output_log.append(error_message)  # Collect error message</span><br><span class="line"></span><br><span class="line">    # Write the collected outputs and logs to a README file</span><br><span class="line">    with open(f&quot;&#123;out_dir&#125;/README.md&quot;, &quot;w&quot;) as readme_file:</span><br><span class="line">        readme_file.write(&quot;\n&quot;.join(output_log))</span><br><span class="line"></span><br><span class="line"># Note: Ensure the complete function is defined to work with the new prompt function names and logic.</span><br><span class="line"></span><br><span class="line">create_code()</span><br></pre></td></tr></table></figure>

<p>该脚本旨在自动化软件项目的脚手架搭建，利用 Ollama 进行动态代码生成和 Colorama 来增强终端输出的可视化效果。它系统地涵盖了开发生命周期的各个阶段，从收集初始上下文到构建整个项目并生成诸如代码文件、安装脚本、部署脚本和测试文件等必要组件。</p>
<ul>
<li><p>初始化：在开始时调用 colorama.init(autoreset&#x3D;True) 确保终端中的每个打印语句在视觉上都经过样式处理，而不会将样式传递给后续打印，使输出清晰易读。</p>
</li>
<li><p>动态提示: 重命名和更具描述性的定义功能（例如， generate_context_prompt ， generate_features_prompt 等）用于构建特定上下文的提示。这些提示被发送到 Ollama 代码生成模型，指导其根据当前开发阶段生成项目的相关部分。这种动态提示机制对于定制生成的代码和文档以满足项目特定需求和结构至关重要。</p>
</li>
<li><p>目录和文件管理: create_directory 函数确保为项目建立必要的目录结构，有效地组织生成的代码、脚本和文件。 extract_filenames 函数在识别和管理生成内容中的文件名方面发挥关键作用，有助于系统化组织项目的组件。</p>
</li>
<li><p>complete 函数: 作为脚本的核心，此函数通过向 Ollama 发送动态构建的提示并将模型的响应集成到项目中来与 Ollama 进行接口。通过这个过程，该函数协调生成项目的架构，包括代码库、安装要求、部署程序和测试框架。</p>
</li>
</ul>
<p>python multiple_code_generator.py</p>
<p>运行脚本会启动一个引导过程,开发人员被提示描述问题</p>
<p>输入了以下内容：</p>
<p>Write a react based chatboat with flask based backend to communicate to LLMs with openai API framework. I want typescript to be used in react and also flask should have python typings.</p>
<p>它生成以下内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Project Structure:</span><br><span class="line">1. frontend/</span><br><span class="line">   - node_modules/</span><br><span class="line">   - public/</span><br><span class="line">      - index.html</span><br><span class="line">   - src/</span><br><span class="line">      - components/</span><br><span class="line">         - ChatInterface.tsx</span><br><span class="line">         - MessageDisplay.tsx</span><br><span class="line">         - UserInput.tsx</span><br><span class="line">      - hooks/</span><br><span class="line">         - useChatBotApi.ts</span><br><span class="line">      - App.tsx</span><br><span class="line">      - index.tsx</span><br><span class="line">      - setupTests.ts</span><br><span class="line">2. backend/</span><br><span class="line">   - app.py</span><br><span class="line">   - venv/</span><br><span class="line">3. .gitignore</span><br><span class="line">4. package.json</span><br><span class="line">5. tsconfig.json</span><br><span class="line">6. README.md</span><br></pre></td></tr></table></figure>

<p>Readme.md 显示整个执行过程的log</p>
<p>以后可以将此脚本集成到开发工作流程中，根据项目需求进行定制，并考虑扩展其功能，如与版本控制系统或自动化测试框架集成。</p>
<p>结束。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://szhowardhuang.github.io/2024/04/11/pytorch-file-info/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Howard Huang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="undefined | 嵌入式老兵博客">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/04/11/pytorch-file-info/" class="post-title-link" itemprop="url">Pytorch格式 .pt .pth .bin .onnx 解释</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2024-04-11 14:11:27 / 修改时间：18:58:05" itemprop="dateCreated datePublished" datetime="2024-04-11T14:11:27+08:00">2024-04-11</time>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>5k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>9 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Pytorch是深度学习领域中非常流行的框架之一，支持的模型保存格式包括.pt和.pth .bin .onnx。这几种格式的文件都可以保存Pytorch训练出的模型，但是它们的区别是什么呢？</p>
<h3 id="格式汇总"><a href="#格式汇总" class="headerlink" title="格式汇总"></a>格式汇总</h3><p>下面是一个整理了 .pt、.pth、.bin、ONNX 和 TorchScript 等 PyTorch 模型文件格式的表格：</p>
<table>
<thead>
<tr>
<th>格式</th>
<th>解释</th>
<th>适用场景</th>
<th>可对应的后缀</th>
</tr>
</thead>
<tbody><tr>
<td>.pt 或 .pth</td>
<td>PyTorch 的默认模型文件格式，用于保存和加载完整的 PyTorch 模型，包含模型的结构和参数等信息。</td>
<td>需要保存和加载完整的 PyTorch 模型的场景，例如在训练中保存最佳的模型或在部署中加载训练好的模型。</td>
<td>.pt 或 .pth</td>
</tr>
<tr>
<td>.bin</td>
<td>一种通用的二进制格式，可以用于保存和加载各种类型的模型和数据。</td>
<td>需要将 PyTorch 模型转换为通用的二进制格式的场景。</td>
<td>.bin</td>
</tr>
<tr>
<td>ONNX</td>
<td>一种通用的模型交换格式，可以用于将模型从一个深度学习框架转换到另一个深度学习框架或硬件平台。在 PyTorch 中，可以使用 torch.onnx.export 函数将 PyTorch 模型转换为 ONNX 格式。</td>
<td>需要将 PyTorch 模型转换为其他深度学习框架或硬件平台可用的格式的场景。</td>
<td>.onnx</td>
</tr>
<tr>
<td>TorchScript</td>
<td>PyTorch 提供的一种序列化和优化模型的方法，可以将 PyTorch 模型转换为一个序列化的程序，并使用 JIT 编译器对模型进行优化。在 PyTorch 中，可以使用 torch.jit.trace 或 torch.jit.script 函数将 PyTorch 模型转换为 TorchScript 格式。</td>
<td>需要将 PyTorch 模型序列化和优化，并在没有 Python 环境的情况下运行模型的场景。</td>
<td>.pt 或 .pth</td>
</tr>
</tbody></table>
<p>不同的后缀只是用于提示我们文件可能包含的内容，但是具体的内容需要看模型提供者编写的README.md才知道。而在使用torch.load()方法加载模型信息的时候，并不是根据文件的后缀进行的读取，而是根据文件的实际内容自动识别的，因此对于torch.load()方法而言，不管你把后缀改成是什么，只要文件是对的都可以读取。</p>
<h3 id="模型的保存与加载到底在做什么？"><a href="#模型的保存与加载到底在做什么？" class="headerlink" title="模型的保存与加载到底在做什么？"></a>模型的保存与加载到底在做什么？</h3><p>我们在使用pytorch构建模型并且训练完成后，下一步要做的就是把这个模型放到实际场景中应用，或者是分享给其他人学习、研究、使用。因此，我们开始思考一个问题，提供哪些模型信息，能够让对方能够完全复现我们的模型？</p>
<p>模型代码：</p>
<ul>
<li>包含了我们如何定义模型的结构，包括模型有多少层&#x2F;每层有多少神经元等等信息；</li>
<li>包含了我们如何定义的训练过程，包括epoch batch_size等参数；</li>
<li>包含了我们如何加载数据和使用；</li>
<li>包含了我们如何测试评估模型。</li>
</ul>
<p>模型参数：提供了模型代码之后，对方确实能够复现模型，但是运行的参数需要重新训练才能得到，而没有办法在我们的模型参数基础上继续训练，因此对方还希望我们能够把模型的参数也保存下来给对方。</p>
<ul>
<li>包含model.state_dict()，这是模型每一层可学习的节点的参数，比如weight&#x2F;bias；</li>
<li>包含optimizer.state_dict()，这是模型的优化器中的参数；</li>
<li>包含我们其他参数信息，如epoch&#x2F;batch_size&#x2F;loss等。</li>
</ul>
<p>数据集：</p>
<ul>
<li>包含了我们训练模型使用的所有数据；</li>
<li>可以提示对方如何去准备同样格式的数据来训练模型。</li>
</ul>
<p>使用文档：</p>
<ul>
<li>根据使用文档的步骤，每个人都可以重现模型；</li>
<li>包含了模型的使用细节和我们相关参数的设置依据等信息。</li>
</ul>
<p>可以看到，根据我们提供的模型代码&#x2F;模型参数&#x2F;数据集&#x2F;使用文档，我们就可以有理由相信对方是有手就会了，那么目的就达到了。</p>
<h3 id="pt-pth格式"><a href="#pt-pth格式" class="headerlink" title=".pt .pth格式"></a>.pt .pth格式</h3><p>一个完整的Pytorch模型文件，包含了如下参数：</p>
<ul>
<li>model_state_dict：模型参数</li>
<li>optimizer_state_dict：优化器的状态</li>
<li>epoch：当前的训练轮数</li>
<li>loss：当前的损失值</li>
</ul>
<p>下面是一个.pt文件的保存和加载示例（注意，后缀也可以是 .pth ）：</p>
<ul>
<li>.state_dict()：包含所有的参数和持久化缓存的字典，model和optimizer都有这个方法</li>
<li>torch.save()：将所有的组件保存到文件中</li>
</ul>
<h4 id="模型保存"><a href="#模型保存" class="headerlink" title="模型保存"></a>模型保存</h4><pre><code>import torch
import torch.nn as nn

# 定义一个简单的模型
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(10, 5)
        self.fc2 = nn.Linear(5, 1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.fc2(x)
        return x

model = Net()

# 保存模型
torch.save(&#123;
            &#39;epoch&#39;: 10,
            &#39;model_state_dict&#39;: model.state_dict(),
            &#39;optimizer_state_dict&#39;: optimizer.state_dict(),
            &#39;loss&#39;: loss,
            &#125;, PATH)
</code></pre>
<h4 id="模型加载"><a href="#模型加载" class="headerlink" title="模型加载"></a>模型加载</h4><pre><code>import torch
import torch.nn as nn

# 定义同样的模型结构
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(10, 5)
        self.fc2 = nn.Linear(5, 1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.fc2(x)
        return x

# 加载模型
model = Net()
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
checkpoint = torch.load(PATH)
model.load_state_dict(checkpoint[&#39;model_state_dict&#39;])
optimizer.load_state_dict(checkpoint[&#39;optimizer_state_dict&#39;])
epoch = checkpoint[&#39;epoch&#39;]
loss = checkpoint[&#39;loss&#39;]
model.eval()
</code></pre>
<h3 id="bin格式"><a href="#bin格式" class="headerlink" title=".bin格式"></a>.bin格式</h3><p>.bin文件是一个二进制文件，可以保存Pytorch模型的参数和持久化缓存。.bin文件的大小较小，加载速度较快，因此在生产环境中使用较多。</p>
<p>下面是一个.bin文件的保存和加载示例（注意：也可以使用 .pt .pth 后缀）：</p>
<h4 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h4><pre><code>import torch
import torch.nn as nn

# 定义一个简单的模型
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(10, 5)
        self.fc2 = nn.Linear(5, 1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.fc2(x)
        return x

model = Net()
# 保存参数到.bin文件
torch.save(model.state_dict(), PATH)
</code></pre>
<h4 id="加载模型"><a href="#加载模型" class="headerlink" title="加载模型"></a>加载模型</h4><pre><code>import torch
import torch.nn as nn

# 定义相同的模型结构
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(10, 5)
        self.fc2 = nn.Linear(5, 1)

    def forward(self, x):
        x = self.fc1(x)
        x = self.fc2(x)
        return x

# 加载.bin文件
model = Net()
model.load_state_dict(torch.load(PATH))
model.eval()
</code></pre>
<h3 id="onnx格式"><a href="#onnx格式" class="headerlink" title=".onnx格式"></a>.onnx格式</h3><p>上述保存的文件可以通过PyTorch提供的torch.onnx.export函数转化为ONNX格式，这样可以在其他深度学习框架中使用PyTorch训练的模型。转化方法如下：</p>
<pre><code>import torch
import torch.onnx

# 将模型保存为.bin文件
model = torch.nn.Linear(3, 1)
torch.save(model.state_dict(), &quot;model.bin&quot;)
# torch.save(model.state_dict(), &quot;model.pt&quot;)
# torch.save(model.state_dict(), &quot;model.pth&quot;)

# 将.bin文件转化为ONNX格式
model = torch.nn.Linear(3, 1)
model.load_state_dict(torch.load(&quot;model.bin&quot;))
# model.load_state_dict(torch.load(&quot;model.pt&quot;))
# model.load_state_dict(torch.load(&quot;model.pth&quot;))
example_input = torch.randn(1, 3)
torch.onnx.export(model, example_input, &quot;model.onnx&quot;, input_names=[&quot;input&quot;], output_names=[&quot;output&quot;])
</code></pre>
<p>加载ONNX格式的代码可以参考以下示例代码：</p>
<pre><code>import onnx
import onnxruntime

# 加载ONNX文件
onnx_model = onnx.load(&quot;model.onnx&quot;)

# 将ONNX文件转化为ORT格式
ort_session = onnxruntime.InferenceSession(&quot;model.onnx&quot;)

# 输入数据
input_data = np.random.random(size=(1, 3)).astype(np.float32)

# 运行模型
outputs = ort_session.run(None, &#123;&quot;input&quot;: input_data&#125;)

# 输出结果
print(outputs)
</code></pre>
<p>注意，需要安装onnx和onnxruntime两个Python包。此外，还需要使用numpy等其他常用的科学计算库。</p>
<h3 id="直接保存完整模型"><a href="#直接保存完整模型" class="headerlink" title="直接保存完整模型"></a>直接保存完整模型</h3><p>可以看出来，我们在之前的保存方式中，都是保存了.state_dict()，但是没有保存模型的结构，在其他地方使用的时候，必须先重新定义相同结构的模型（或兼容模型），才能够加载模型参数进行使用，如果我们想直接把整个模型都保存下来，避免重新定义模型，可以按如下操作：</p>
<h4 id="保存模型-1"><a href="#保存模型-1" class="headerlink" title="保存模型"></a>保存模型</h4><pre><code>PATH = &quot;entire_model.pt&quot;
# PATH = &quot;entire_model.pth&quot;
# PATH = &quot;entire_model.bin&quot;
model = Net()
torch.save(model, PATH)
</code></pre>
<h4 id="加载模型-1"><a href="#加载模型-1" class="headerlink" title="加载模型"></a>加载模型</h4><pre><code>model = torch.load(&quot;entire_model.pt&quot;)
model.eval()
</code></pre>
<h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>本文介绍了pytorch可以导出的模型的几种后缀格式，但是模型导出的关键并不是后缀，而是到处时候提供的信息到底是什么，只要知道了模型的model.state_dict()和optimizer.state_dict()，以及相应的epoch batch_size loss等信息，我们就能够重建出模型，至于要导出哪些信息，就取决于你了，务必在readme.md中写清楚，你导出了哪些信息。</p>
<p>之所以写了本文，是因为上一篇“通过游戏深入了解强化学习” 训练完模型后，怎么把模型应用到实际场景中，就写了这篇，理解如何加载模型。</p>
<h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/620688513">https://zhuanlan.zhihu.com/p/620688513</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Howard Huang</span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">255k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">7:44</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">
    <!--由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动-->
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





</body>
</html>
